{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-22: Intro to scikit-learn\n",
    "\n",
    "<img src=\"https://www.cityofberkeley.info/uploadedImages/Public_Works/Level_3_-_Transportation/DSC_0637.JPG\" style=\"width: 500px; height: 275px;\" />\n",
    "---\n",
    "\n",
    "** Regression** is useful for predicting a value that varies on a continuous scale from a bunch of features. This lab will introduce the regression methods available in the scikit-learn extension to scipy, focusing on ordinary least squares linear regression, LASSO, and Ridge regression.\n",
    "\n",
    "*Estimated Time: 45 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "\n",
    "1 - [The Test-Train Split](#section 1)<br>\n",
    "\n",
    "2 - [Linear Regression](#section 2)<br>\n",
    "\n",
    "3 - [LASSO Regression](#section 3)<br>\n",
    "\n",
    "3 - [Ridge Regression](#section 4)<br>\n",
    "\n",
    "\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datascience import *\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Bike Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your time at Cal, you've probably passed by one of the many bike sharing station around campus. Bike sharing systems have become more and more popular as traffic and concerns about global warming rise. This lab's data describes one such bike sharing system in Washington D.C., from [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike=Table().read_table(('data/Bike-Sharing-Dataset/day.csv'))\n",
    "\n",
    "# reformat the date column to integers representing the day of the year, 001-366\n",
    "bike['dteday'] = pd.to_datetime(bike['dteday']).strftime('%j')\n",
    "\n",
    "# get rid of the index column\n",
    "bike = bike.drop(0)\n",
    "\n",
    "bike.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to get familiar with the data set. In data science, you'll often hear rows referred to as **records** and columns as **features**. Before you continue, make sure you can answer the following:\n",
    "\n",
    "- How many records are in this data set?\n",
    "- What does each record represent?\n",
    "- What are the different features?\n",
    "- How is each feature represented? What values does it take, and what are the data types of each value?\n",
    "\n",
    "Use Table methods and check the UC Irvine link for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore the data set here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Test-Train Split  <a id='section 1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a model on a data set, we run the risk of [**over-fitting**](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html). Over-fitting happens when a model becomes so complex that it makes very accurate predictions for the data it was trained on, but it can't generalize to make good predictions on new data.\n",
    "\n",
    "We can reduce the risk of overfitting by using a **test-train split**. \n",
    "\n",
    "1. Randomly divide our data set into two smaller sets: one for training and one for testing\n",
    "2. Train the data on the training set, changing our model along the way to increase accuracy\n",
    "3. Test the data's predictions using the test set.\n",
    "\n",
    "Scikit-learn's [`test_train_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function will help here. First, separate your data into two parts: a Table containing the features used to make our prediction, and an array of the true values. To start, let's predict the *total number of riders* using every feature that isn't a rider count.\n",
    "\n",
    "Note: for the function to work, X can't be a Table. Save X as a pandas DataFrame by calling `.to_df()` on the feature Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the features used to predict riders\n",
    "features_only = ...\n",
    "X = ...\n",
    "\n",
    "# the number of riders\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the random seed using `np.random.seed(...)`. Any seed number is fine- the important thing is to document the number you used in case we need to recreate this pseudorandom split in the future.\n",
    "\n",
    "Then, call `train_test_split` on your X and y. Also set the parameters `train_size=` and `test_size=` to set aside 75% of the data for training and 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression (Ordinary Least Squares) <a id='section 2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to start training models and making predictions. We'll start with a **linear regression** model.\n",
    "\n",
    "[Scikit-learn's linear regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) is built around scipy's ordinary least squares, which you used in the last lab. The syntax for each scikit-learn model is very similar:\n",
    "1. Create a model by calling its constructor function. For example, `LinearRegression()` makes a linear regression model.\n",
    "2. Train the model on your training data by calling `.fit(train_X, train_y)` on the model\n",
    "\n",
    "Create a linear regression model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a model\n",
    "lin_reg = ...\n",
    "\n",
    "# fit the model\n",
    "lin_model = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model fit, you can look at the best-fit slope for each feature using `.coef_`, and you can get the intercept of the regression line with `.intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients and intercept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get a sense of how good our model is. We can do this by looking at the **residuals**: the difference between the predicted values and the actual values, also called the error.\n",
    "\n",
    "- Call `.predict(X, Y)` on your linear regression model, using your training X and training y, to return a list of predicted number of riders per hour. Save it to a variable `lin_pred`.\n",
    "- Calculate the residuals by subtracting the predicted riders from the actual riders. Save the results to a variable `lin_error`.\n",
    "- Plot the residuals on a scatter plot (use `plt.scatter(...)` with `X_train['day_of_yr'`] on the x-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the number of riders\n",
    "...\n",
    "\n",
    "# calculate residuals\n",
    "...\n",
    "\n",
    "# plot the residuals on a scatter plot\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what should our scatter plot look like if our model was 100% accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** All points (i.e. errors) should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a sense of how well our model is doing with `.score(X, y)`, which returns the [**Coefficient of Determination**](https://en.wikipedia.org/wiki/Coefficient_of_determination), also called $R^2$. Generally, scores closer to 1 indicate a better fit, while a score close to 0 indicates nearly no correlation at all.\n",
    "\n",
    "Print the scores for the train data and the test data. Which one do you expect to be higher, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge Regression <a id='section 3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've gone through the process for OLS linear regression, it's easy to do the same for [**Ridge Regression**](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). In this case, the constructor function that makes the model is `Ridge()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make and fit a Ridge regression model\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "...\n",
    "\n",
    "# calculate the residuals for the training data\n",
    "...\n",
    "\n",
    "# plot the residuals\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model for the test and train data\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the documentation for Ridge regression shows it has lots of **hyperparameters**: values we can choose when the model is made. Now that we've tried it using the defaults, look at the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) and try changing some parameters to see if you can get a higher score (`alpha` might be a good one to try)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LASSO Regression <a id='section 4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll try using [LASSO regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html). The constructor function to make the model is `Lasso()`. \n",
    "\n",
    "You may get a warning message saying the objective did not converge. The model will still work, but to get convergence try increasing the number of iterations (`max_iter=`) when you construct the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "...\n",
    "\n",
    "# calculate the residuals for the training data\n",
    "...\n",
    "\n",
    "# plot the residuals\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model for the test and train data\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: LASSO regression also has many tweakable hyperparameters. See how changing them affects the score!\n",
    "\n",
    "Question: How do these three models compare on performance? What sorts of things could we do to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** All three models have very similar accuracy ($R^2 \\approx .8$ for training data and $\\approx .78$ for test data).\n",
    "\n",
    "We could try changing which features we use or adjust the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming up this semester: how to select your models, model parameters, and features to get the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Keeley Takimoto\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
