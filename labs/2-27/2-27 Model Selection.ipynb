{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-27: Intro to Model Selection Techniques\n",
    "\n",
    "\n",
    "---\n",
    "<img src = \"http://izquotes.com/quotes-pictures/quote-essentially-all-models-are-wrong-but-some-are-useful-george-e-p-box-212711.jpg\" style = \"width: 500px; height: 275px;\" />\n",
    "\n",
    "**Model Selection** is a key step in the machine learning process. Generally, you will have several candidate models that you fit to your data, and must select the one that you will use on out-of-sample data. These techniques help you determine which model is the \"best.\"\n",
    "\n",
    "*Estimated Time: 30 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from datascience import *\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Bike Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the same bike sharing data as last week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike=Table().read_table(('data/Bike-Sharing-Dataset/day.csv'))\n",
    "\n",
    "# reformat the date column to integers representing the day of the year, 001-366\n",
    "bike['dteday'] = pd.to_datetime(bike['dteday']).strftime('%j')\n",
    "\n",
    "# get rid of the index column\n",
    "bike = bike.drop(0)\n",
    "\n",
    "bike.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test-Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we typically want to split our data into training, validation, and test sets for the purposes of developing and tweaking our Machine Learning models. Below we reproduce the code from last lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the features used to predict riders\n",
    "X = bike.drop('casual', 'registered', 'cnt')\n",
    "X = X.to_df()\n",
    "\n",
    "# the number of riders\n",
    "y = bike['cnt']\n",
    "\n",
    "# set the random seed\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.80, test_size=0.20)\n",
    "\n",
    "# split the data\n",
    "# Returns 4 values: X_train, X_validate, y_train, y_validate\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specify Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the linear regression, ridge, and lasso models that we explored in the last labs on the training data. This time you don't need to report the coefficients or plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Create lin_reg method and fit model\n",
    "\n",
    "\n",
    "# Ridge\n",
    "\n",
    "## Create ridge_reg method and fit model\n",
    "\n",
    "# LASSO\n",
    "\n",
    "## Create lasso_reg and fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information Criterion Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have three candidate models! First, let's implement the AIC and BIC approaches. Write code that calculates the AIC and BIC for each of the models. The (simplified) formulas for each are: \n",
    "\n",
    "$AIC = 2k - 2ln(sse)$\n",
    "\n",
    "$BIC = n*ln(sse/n) + k*ln(n)$\n",
    "\n",
    "Where $k$ is the number of features, $n$ is the number of observations, $ln$ is the natural log (hint: use np.log), and sse is the \"sum of squared errors\" or the squared residuals between the predicted and actual responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns\n",
    "k = \n",
    "# Rows\n",
    "n = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_hat = \n",
    "lin_reg_resid = \n",
    "lin_reg_sse = \n",
    "\n",
    "lin_AIC = \n",
    "lin_AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg_hat = \n",
    "ridge_reg_resid = \n",
    "ridge_reg_sse = \n",
    "\n",
    "ridge_AIC = \n",
    "ridge_AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg_hat = \n",
    "lasso_reg_resid = \n",
    "lasso_reg_sse = \n",
    "\n",
    "lasso_AIC = \n",
    "lasso_AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model should we prefer based on the AIC? How confident are you about this choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the BIC for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_BIC = \n",
    "lin_BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_BIC = \n",
    "ridge_BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_BIC = \n",
    "lasso_BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model should you prefer based on the BIC? Again, how confident are you about this choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try a cross-validation approach. The basic logic of cross-validation is as follows:\n",
    "\n",
    "1. Randomly split the data into k-folds\n",
    "2. Build the model on k-1 folds, then test on the last fold\n",
    "3. Record prediction error\n",
    "4. Cycle until each fold has served as the test set\n",
    "5. The average of the errors is the cv-error\n",
    "\n",
    "Luckily a lot of this functionality is already packaged up for us in sklearn's [cross-validation methods](http://scikit-learn.org/stable/modules/cross_validation.html). First, generate predictions for each of the models by using \"cross_val_predict.\" Use \"cv = 3\" and return the r^2 score, and plot the predicted vs. actual values. For this section, disregard the train-validation-split you did earlier, and instead use the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_predicted = cross_val_predict(...)\n",
    "r2_score(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals on a scatter plot\n",
    "plt.scatter(...)\n",
    "plt.title('Linear Model (OLS)')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_predicted = cross_val_predict(...)\n",
    "r2_score(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals on a scatter plot\n",
    "plt.scatter(...)\n",
    "plt.title('Linear Model (OLS)')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_predicted = cross_val_predict(...)\n",
    "r2_score(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals on a scatter plot\n",
    "plt.scatter(...)\n",
    "plt.title('Linear Model (OLS)')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does cross-validation fit the data? In general, explicitly calculating $r^2$ like this tends to overfit the data. The preferred method is to use \"cross_val_score.\" Use \"cross_val_score\" to return the mean prediction from a 3-fold cross validation for each of the models. How do the $r^2$ metrics compare?\n",
    "\n",
    "Experiment with different [metrics](http://scikit-learn.org/stable/modules/model_evaluation.html), and select your preferred model. Keep in mind that higher return values are better than lower return values in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're feeling ambitious, experiment with the full range of cross-validation options (including Leave-One-Out)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
