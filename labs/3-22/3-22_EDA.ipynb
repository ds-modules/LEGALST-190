{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3-22] Exploratory Data Analysis\n",
    "---\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/ba/Data_visualization_process_v1.png\" style=\"width: 500px; height: 400px;\" />\n",
    "\n",
    "\n",
    "This lab covers the Tukey approach to exploratory data analysis, bringing together key concepts and skills from earlier in the course like data visualization and the train-test-validate split. We will also introduce dimensionality reduction in the form of Principal Components Analysis.\n",
    "\n",
    "*Estimated Time: 45 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "\n",
    "1 - [Intro to Exploratory Data Analysis](#section 1)<br>\n",
    "\n",
    "2 - [EDA and Visualization](#section 2)<br>\n",
    "\n",
    "3 - [Dimensionality Reduction](#section 3)<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "!pip install plotly\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction to Exploratory Data Analysis <a id='section 1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"It is important to understand what you CAN DO before you learn to measure how WELL you seem to have done it.\" -John W. Tukey, *Exploratory Data Analysis*\n",
    "\n",
    "Previous labs and many introductory data analysis classes like Data-8 focus on **Confirmatory Data Analysis (CDA)**: the use of statistical methods to see whether the data supports a given hypothesis or model. But, where does the hypothesis come from in the first place?\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is the process of 'looking at data to see what it seems to say'. Through this process, we hope to accomplish several things:\n",
    "- learn about the overall 'shape' of the data: structure, organization, ranges of values\n",
    "- assess what assumptions we can make about the data as a basis for later statistical inference\n",
    "- figure out the appropriate tools and techniques for analysis \n",
    "- tentatively create testable, appropriate hypotheses or models\n",
    "\n",
    "EDA is complementary to CDA- in fact, CDA is often only possible after having done EDA. This is clear when compared to the methods of natural sciences: a scientist's research starts with making observations about the world, which leads to questions about what they see. Only after observing and wondering can we begin to systematically test our hypotheses. Similarly, every data analysis project you do should begin with EDA.\n",
    "\n",
    "Today, we'll go through the EDA process for speech transcripts from [2016 US Presidential campaigns](http://www.presidency.ucsb.edu/2016_election.php). You'll use many coding skills from previous labs. And because EDA is an exploratory process, there are many ways to approach the code (we'll give some tips and guidance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "campaign = pd.read_csv(\"data/campaign_2016.csv\", index_col=0)\n",
    "campaign.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy way to get basic summary statistics about your data is using the `.describe()` method on your table. Other useful methods are:\n",
    "- `.unique()` to look at a column's unique values\n",
    "- `.value_counts()` to see how often a column's values occur.\n",
    "- `.isnull()` to screen for null values\n",
    "\n",
    "From these few methods, we can immediately see things that will affect future analysis options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas methods to broadly examine the scope and values of the data\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**:\n",
    "Name at least two issues seen using `describe`, `unique`, `value_counts`, and `isnull`. Why would each issue affect future analysis?\n",
    "Hint for analyzing `describe`: Which statistics would we expect to be the same for all columns, and are they actually the same? Which column would we expect 'count' to be different from 'unique', and are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a handful of null values in the 'Text' column. Null values can mess up calculations and need to be dealt with by filling them with a dummy value or deleting the row. Because the null values are for text, and because we're most interested in the text content, we'll drop the offending rows. How you deal with null values will vary on a project-by-project basis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the null-valued rows with dropna\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "Many of numpy and pandas' summary and aggregation methods are built for numerical data, not text. To better understand our data, we use **feature extraction** to create features from transformations of the input features. Some useful features for text would be:\n",
    "- the number of characters per text\n",
    "- the number of words per text\n",
    "- the number of sentences per text\n",
    "- the average length of words per text\n",
    "- the average length of sentences per text\n",
    "\n",
    "Since we're not focusing on data cleaning today (remember the 3/1 lab?), we've provided the code to add columns with lowercase text, word tokens, and sentence tokens that you might find useful. You may also want to revisit the pandas [str methods](https://pandas.pydata.org/pandas-docs/stable/text.html#method-summary).\n",
    "\n",
    "Note: During EDA, it is always best practice to clearly document what transformations you did, and to add transformed data in new columns rather than transforming the data in place. This will save you a lot of trouble if you need to refer to the original data in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with lowercase, word tokens, sentence tokens\n",
    "campaign['lower_text'] = campaign['Text'].str.lower()\n",
    "campaign['words'] = campaign['lower_text'].apply(nltk.word_tokenize)\n",
    "campaign['sentences'] = campaign['lower_text'].apply(nltk.sent_tokenize)\n",
    "\n",
    "campaign.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from the text data\n",
    "campaign['char_count'] = ...\n",
    "campaign['word_count'] = ...\n",
    "campaign['sentence_count'] = ...\n",
    "campaign['avg_word_length'] = ...\n",
    "campaign['avg_sentence_length'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Date' column currently stores dates as strings, which we can't easily compare on a graph. Fortunately, pandas has a **datetime** object that can be easily ordered. Add a column containing string dates that have been transformed into datetime objects. [There's a function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html) that might be helpful here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the 'Date' column to type datetime\n",
    "campaign['datetime'] = ...\n",
    "campaign.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also might be interested in seeing how candidates use particular words. Using a for loop, create a new column for each topic word containing the number of times the word is used in each text.\n",
    "\n",
    "Hint: take a look at pandas str methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of possible topics\n",
    "topics = ['nuclear', 'peace', ' war',\n",
    "          'north korea', 'russia', 'terror', 'fake news', 'email',\n",
    "          ' poverty', ' human rights', ' abortion', ' refugee', ' immigration',\n",
    "          'equality', ' democracy', ' freedom', 'vote', 'energy', 'oil',  'coal',  ' income',\n",
    "          'economy', 'growth', 'inflation', 'climate change', 'security',\n",
    "          'cyber', 'trade', 'inequality', 'pollution', 'global warming',\n",
    "          'education', 'health', 'infrastructure', 'regulation', 'nutrition', 'transportation',\n",
    "          'violence', 'agriculture', 'crime', 'drugs', 'obesity',\n",
    "          'islam', 'housing', 'sustainable']\n",
    "\n",
    "\n",
    "for i in topics:    \n",
    "    campaign[i] = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have numerical data, try using `describe()` again. What's changed now that there's numerical data? What else can you say about this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA and Visualization <a id='section 2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"The greatest value of a picture is when it *forces* us to notice what we never expected to see.\" - John W. Tukey, *Exploratory Data Analysis*\n",
    "\n",
    "Our data set contains almost 7500 records, which is difficult for our brains to parse as a whole. And, this would be considered a fairly small data set by most data science standards. Visualizations are an important tool for summarizing and understanding a data set.\n",
    "\n",
    "In EDA, we're often interested in a few characteristics:\n",
    "- where the data is centered (the mean or median)\n",
    "- how much the data varies from the mean (the spread/variance/standard deviation)\n",
    "- outliers (the min and max)\n",
    "\n",
    "The **box plot** is a useful way to visualize all of these things.\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Elements_of_a_boxplot_en.svg/640px-Elements_of_a_boxplot_en.svg.png\" />\n",
    "The rectangular box of the plot holds 50% of the data points. The bottom of the box represents the point that is higher in value than 25% of the data (the lower quartile), and the top of the box represents the point that is higher than 75% of the data (the upper quartile). The whiskers extend on each side to the lowest data point that is within 1.5 times the length of the box (the inter-quartile range, or IQR). All points farther out are graphed as circles (the outliers).\n",
    "\n",
    "We can make box plots with `plt.boxplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a box plot of the word counts\n",
    "plt.boxplot(campaign['word_count'])\n",
    "plt.ylabel('number words')\n",
    "plt.title('Word Counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an awfully scrunched-up box plot! We can see that over half of our texts contain less than a thousand words, but many texts contain several times that. \n",
    "\n",
    "It may be helpful to sort out the different types of texts. Create a dataframe that only has type 'speech', and another dataframe that only has type 'statement'. Then, make a box plot for each. Compare the box plots for word counts to those for word length, sentence length, or character count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out everything except speeches\n",
    "speeches = ...\n",
    "\n",
    "# make a box plot\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out everything but statements\n",
    "statements = ...\n",
    "\n",
    "# make a box plot\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUESTION__: what can you conclude about the text data from these box plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots pack a lot of information, but the plots can get muddy if many data points overlap. A **Violin plot** is similar to a box plot, but it also shows the distribution of the data at different values (you can think of it like a smoothed histogram).\n",
    "\n",
    "Look at the word counts, word lengths, and other features using `plt.violinplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a violin plot\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another violin plot\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: What can you see with the violin plot that you couldn't with the box plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unexpected or interesting information may also be found by looking at multiple features together, or by looking at subgroups. \n",
    "\n",
    "If you need a refresher on grouping in pandas:\n",
    "\n",
    "1. select the features you need using `.loc[<rows>, <columns>]`. The rows and columns can be a single label, a list of labels, or a range (e.g. `'word_count':` would give you all columns to the right of and including `word_count`).\n",
    "2. call `.groupby` with the label of the feature you want to group by\n",
    "3. call the function you want to use to aggregate the values in each group (e.g. `sum()`, `min()`, `max()`, `mean()`...)\n",
    "\n",
    "For example, here's a bar plot of the average word count in each candidate's speeches using groupby:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_candidate = (speeches.loc[:, ['Candidate', 'inequality', 'russia', 'nuclear']] # select all rows, only the columns needed\n",
    "            .groupby('Candidate') # make groups of word counts for each candidate\n",
    "            .mean()) # take the average of each group of candidate's word counts\n",
    "\n",
    "by_candidate.plot.bar(figsize=(12, 5)); # make a bar plot; figsize embiggens it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below, try creating:\n",
    "- a line plot showing the average number of times the word 'email' was used on each date (use groupby on the `datetime` column, not `Date`)\n",
    "- histograms showing the distribution of `word_length` for Democrats and Republicans\n",
    "- a scatter plot plotting the length of words against the length of sentences for speeches and statements\n",
    "\n",
    "Refer to the [pyplot](https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py) and [pandas dataframe plot](https://pandas.pydata.org/pandas-docs/stable/api.html#api-dataframe-plotting) docs as needed. Feel free to experiment with other groups or features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: What kinds of interesting/suspicious patterns or outliers did you see?\n",
    "Given what you now know about the data set, what kinds of hypotheses could we test on the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principal Components Analysis <a id='section 3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that with text analysis, feature matrices get very large, very fast (remember the tf-idf matrix?). To help us work with high-dimensional data, we can use **Principal Component Analysis (PCA)**.\n",
    "\n",
    "The goal of PCA is to look for patterns in the data (i.e. correlations between variables). If two variables are strongly correlated, including just one of them in the model might represent the data about as good as if we had them both. PCA looks for variables that account for as much of the variance in the data as possible. This makes it great for **dimensionality reduction**: getting rid of features (dimensions) that don't explain much of the variance in the data so we can more easily manipulate, visualize, and understand the data.\n",
    "\n",
    "Note: the details of PCA are hard to follow without some knowledge of linear algebra. If you're interested, check out a more in-depth example [here](https://plot.ly/ipython-notebooks/principal-component-analysis/).\n",
    "\n",
    "Let's explore reducing the dimensions for our text data. We'll start by using the features we extracted. First, create a table `X` that only has the columns with the extracted numerical features (no string or datetime values) and an array `y` that has the names of the candidates (this will help us visualize our data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns with int or float values\n",
    "X = campaign.iloc[:, 9:].drop('datetime', axis=1)\n",
    "\n",
    "# select the column with the candidate names\n",
    "y = campaign['Candidate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to standardize X so that all the features have the same scale (i.e. have an average value of 0 and a standard deviation of 1). Do this by creating a `StandardScaler()`, then run its `fit_transform` method on `X`. You should recognize the syntax from other scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "X_std = ...\n",
    "\n",
    "# look at the covariance matrix\n",
    "np.cov(X_std.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, initialize a `PCA` object (set `n_components=2` so we can graph it in two dimensions) and use its `fit_transform` method on your standardized X to get `Y_pca`: the top n (in this case 2) principal components that explain variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PCA\n",
    "...\n",
    "\n",
    "# fit the standardized data\n",
    "Y_pca = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how much variance is being explained by these components by using `.explained_variance_ratio`. This gives the proportions of the total variance in documents that are explained by each component in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the explained variance ratios\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: if our PCA was fit using ALL possible components (that is, n_components was set to the original number of features in the data set), what should the explained variance ratios add up to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: The ratios should add up to 1: including all the features means that all the variance can be explained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tell the candidates apart on our scatter plot, we'll assign each a different color: blues for Democrats, and reds for Republicans. Run the next cell to create the custom palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate candidates by party and pair each candidate with a shade of their party color\n",
    "dems = zip(campaign[campaign.Party=='D'].Candidate.unique(), sns.color_palette('Blues', 5).as_hex())\n",
    "gops = zip(campaign[campaign.Party=='R'].Candidate.unique(), sns.color_palette('Reds', 16).as_hex())\n",
    "\n",
    "# collect the candidate-color pairs in a dictionary\n",
    "candidate_colors = {}\n",
    "\n",
    "for dem, color in dems:\n",
    "    candidate_colors[dem] = color\n",
    "\n",
    "for gop, color in gops:\n",
    "    candidate_colors[gop] = color\n",
    "\n",
    "# create the ordered color palette\n",
    "colors = candidate_colors.values()\n",
    "\n",
    "# show the candidates and palette\n",
    "print(candidate_colors.keys())\n",
    "sns.palplot(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot the data by running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the principal components and the classes\n",
    "components = pd.DataFrame({'component 1': Y_pca[:, 0],\n",
    "                          'component 2': Y_pca[:, 1],\n",
    "                          'Candidate': y})\n",
    "\n",
    "colors = candidate_colors.values()\n",
    "\n",
    "# plot\n",
    "sns.lmplot(x = 'component 1', y = 'component 2', data = components, hue='Candidate', legend=True, fit_reg=False, \n",
    "            hue_order=candidate_colors.keys(),palette=colors, scatter_kws={'s':5}, size=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA on our derived features (e.g. word counts), we can see some separation between different people. However most of the data points are clustered in the same area, and there's a lot of overlap. Would we see something different if we had done dimensionality reduction on a different numerical representation of the data?\n",
    "\n",
    "Let's revisit the tf-idf. As we saw Tuesday, a tf-idf is very high-dimensional, giving information about the frequency of a term in a document and across all documents for every possible term in every document. We can do dimensionality reduction on a tf-idf matrix, too, using **Latent Semantic Analysis (LSA)**. \n",
    "\n",
    "Like PCA, LSA uses linear algebra to reduce dimensionality while still representing the variation between documents. The code is very similar too, except in this case we don't have to standardize the feature data. First, create a `TfidfVectorizer()` and fit the data in the 'clean_text' column of `campaign` to it to create the sparse tf-idf matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TfidfVectorizer\n",
    "...\n",
    "\n",
    "# Fit vectorizer\n",
    "X = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, do the LSA by created a `TruncatedSVD` object (set `n_components=2`) and fit the sparse tf-idf matrix to it using `fit_transform` (see [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)). You should get a matrix with two values for every text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TruncatedSVD\n",
    "...\n",
    "\n",
    "# fit model\n",
    "Y_lsa = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the explained variance ratio in the cell below. The ratios are very different from those found in PCA above. Why? (Hint: think about what kind and how many features were originally in the data given to the PCA model versus the LSA model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the explained variance ratios\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the next cell to plot the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put components in a data frame\n",
    "lsa_components = pd.DataFrame({'component1': Y_lsa[:, 0],\n",
    "                          'component2': Y_lsa[:, 1],\n",
    "                          'Type': y})\n",
    "# plot\n",
    "sns.lmplot(x = 'component1', y = 'component2', data = lsa_components, hue='Type', legend=True, fit_reg=False, \n",
    "           hue_order=['press release', 'speech', 'statement'], palette=colors, scatter_kws={'s':5}, size=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, take a look at the exact same data, but with the document type marked in colors rather than the candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put components in a data frame\n",
    "lsa_components = pd.DataFrame({'component1': Y_lsa[:, 0],\n",
    "                          'component2': Y_lsa[:, 1],\n",
    "                          'Type': campaign.Type})\n",
    "# plot\n",
    "sns.lmplot(x = 'component1', y = 'component2', data = lsa_components, hue='Type', legend=True, fit_reg=False, \n",
    "            scatter_kws={'s':8}, size=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: How does this plot compare to the one using PCA and extracted numerical features? What does it tell you about the variance of the data? Keep in mind that in this case, the data was fed into LSA as a 'bag of words' for every document- there was no input that explicitly gave the document's type or candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important notes on PCA:\n",
    "1. PCA can also be used while training a model for prediction or classification tasks. In that case, like during any model training, you would set aside your test data before doing PCA to avoid overfitting your model.\n",
    "2. The components output from PCA can't really be described as any specific input features like 'character length'. They're a set of linearly uncorrelated vectors; we can't break it down much more than than, unfortunately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for fun: 3 dimensional graphing\n",
    "Can we see anything new if we ask for 3 principal components instead of 2? Run the following cells to see. Does adding an extra dimension capture anything new about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "lsa3 = TruncatedSVD(n_components=3)\n",
    "\n",
    "# Fit models\n",
    "Y_lsa3 = lsa3.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the components and some info about each data point\n",
    "lsa_3components = pd.DataFrame({'component1': Y_lsa3[:, 0],\n",
    "                          'component2': Y_lsa3[:, 1],\n",
    "                            'component3': Y_lsa3[:, 2],\n",
    "                          'Type': campaign.Type,\n",
    "                          'Candidate': campaign.Candidate, \n",
    "                            'Date': campaign.Date})\n",
    "\n",
    "lsa3.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to show a 3-dimensional graph of the transformed data points. Try zooming in and out or hiding certain candidates data by clicking on their name on the list at the right side. Do you notice any party or candidate patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp = []\n",
    "\n",
    "# plots the texts for each candidate, labeling each point with the date and candidate's color and name\n",
    "for cand in candidate_colors.keys():\n",
    "    points = lsa_3components[lsa_3components.Candidate==cand]\n",
    "    data_comp.append(go.Scatter3d(\n",
    "        x=points.component1,\n",
    "        y=points.component2,\n",
    "        z=points.component3,\n",
    "        mode='markers',\n",
    "        marker=dict(size=3,\n",
    "                line=dict(width=1),\n",
    "                color=candidate_colors[cand]\n",
    "               ),\n",
    "        name=cand, \n",
    "        text=points.Date))\n",
    "    \n",
    "layout_comp = go.Layout(\n",
    "    title='LSA',\n",
    "    hovermode='closest',\n",
    "\n",
    ")\n",
    "fig_comp = go.Figure(data=data_comp, layout=layout_comp)\n",
    "py.iplot(fig_comp, filename='LSA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tukey, J. W. (1977). Exploratory data analysis\n",
    "- Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. Psychological Methods, 2(2), 131."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Keeley Takimoto\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
