{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-190] 4-19: Decision Trees and Ensemble Methods\n",
    "---\n",
    "<img src=\"https://c.pxhere.com/photos/df/5e/uganda_signs_outdoor_wooden_sign_direction_this_wy_that_way-341892.jpg!d\" style=\"width: 600px; height: 400px;\" />\n",
    "\n",
    "\n",
    "This will wrap up the regression methods labs by introducing ensemble methods.\n",
    "\n",
    "*Estimated Time: 40 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "\n",
    "1 - [Base Model: Decision Trees](#section 1)<br>\n",
    "\n",
    "2 - [Averaging Methods](#section 2)<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 - [Random Forest](#subsection 1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 - [Grid Search](#subsection 2)\n",
    "\n",
    "3 - [Boosting Methods](#section 3)<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 - [Ada Boost](#subsection 1)\n",
    "\n",
    "4 - [Feedback Survey](#section 4)<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Bike Sharing<a id='section data'></a>\n",
    "\n",
    "Today's lab will use the now-familiar bike-sharing data set. Run the cell below to load the data.\n",
    "\n",
    "Information about the dataset: http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike = pd.read_csv('data/day.csv', index_col=0)\n",
    "# reformat the date column to integers that represent the day of the year, 001-366\n",
    "bike['dteday'] = pd.to_datetime(bike['dteday'].unique()).strftime('%j')\n",
    "\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Over the course of this class, we've seen that a single model may have significant trouble making accurate predictions. **Ensemble methods** seek to improve on the single-model method by combining the predictions from multiple base models.\n",
    "\n",
    "This lab will cover the two types of ensemble methods- averaging and boosting- using decision trees as our base model. But, one of the strengths of ensemble methods is their ability to solve many kinds of problems using many different kinds of base models.\n",
    "\n",
    "As always, we start by dividing our data into training, validation, and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into features and values to predict\n",
    "X= bike.drop(['cnt', 'casual', 'registered'], axis=1)\n",
    "y= bike.cnt\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data with 0.20 proportion for test size\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test \n",
    "X, X_test, y, y_test = train_test_split(X, y,\n",
    "                                            train_size=0.80, test_size=0.20)\n",
    "# split the remaining data with 0.75 proportion for train size and 0.25 for validation size\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Base Model: Decision Tree <a id='section 1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, you used an **ExtraTree** (EXTremely RAndomized TREE) to identify low-importance features. Extra trees are a subclass of a model called the **Decision Tree**, which we'll explore in more detail today.\n",
    "\n",
    "Decision trees predict target values by creating a set of decision rules. The tree is made up of *nodes*, which constitute decision points, and *branches*, which represent the outcome of the decision. Here's an example using the [Titanic](https://www.kaggle.com/c/titanic/data) data set to predict whether or not a passenger survived the sinking of the ship. Nodes are represented by the text, and branches by lines (left branch = 'yes', right branch='no').\n",
    "\n",
    "Starting at the *root node* (which in computer science, somewhat counterintuitively, is at the top), the data is split into different subgroups at each decision node going top to bottom. The very bottom nodes in the tree (the *leaves*) assign prediction values to the data. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\" style=\"width: 400px; height: 400px;\" />\n",
    "\n",
    "> *'sibsp' gives the number of siblings or spouses a passanger had on board. The left number under a leaf is the chance of survival for that subgroup; the right number is the percentage of passengers in that subgroup. *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Based on this decision tree, what would the model predict would happen to an 8-year-old boy with 2 sisters and a brother? What would the chance of survival be for a 28-year-old married man?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Titanic data uses decision trees for categorization, they can also be used for regression. The process is very similar to the other Scikit-Learn models you've used.\n",
    "1. Create the `DecisionTreeRegressor()`. Set `max_depth` equal to 3.\n",
    "2. Fit `X_train` and `y_train` to the regressor to create the model\n",
    "\n",
    "Note: The `max_depth=` parameter of DecisionTreeRegression constrains how many times a data set can be split. For example, the Titanic tree had a max depth of 3 (i.e. you could pass through at most 3 branches when going from the root to a leaf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the DecisionTreeRegressor\n",
    "dt_reg = DecisionTreeRegressor(criterion='mse',  # how to measure fit\n",
    "                                    splitter='best',  # or 'random' for random best split\n",
    "                                    max_depth=3,  # how deep tree nodes can go\n",
    "                                    min_samples_split=2,  # samples needed to split node\n",
    "                                    min_samples_leaf=1,  # samples needed for a leaf\n",
    "                                    min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                                    max_features=None,  # number of features to look for when splitting\n",
    "                                    max_leaf_nodes=None,  # max nodes\n",
    "                                    min_impurity_decrease=1e-07)  # early stopping\n",
    "\n",
    "# fit the model\n",
    "model = ...\n",
    "\n",
    "# score the model\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `feature_importances_` show how much weight is given to each feature in the model. Higher numbers correspond to more important features. The importances correspond to features by their index: the importance weight in position 1 goes with feature 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the feature importance, in a data frame for convenience\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "             'importance': model.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the final steps:\n",
    "3. Check the model's accuracy on the training and validation data using `.score`\n",
    "4. Plot the predictions of the model against the actual values in a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values for the training and validation sets\n",
    "pred_y_train = ...\n",
    "pred_y_val =...\n",
    "\n",
    "# plot the predicted values against the true values\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** The scatter plot looks kind of funky. Based on what you know about how decision trees make predictions, why would that be the case? \n",
    "\n",
    "Hint: look at the one parameter we changed from its default value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the great thing about decision trees is that, unlike many other models, it's relatively easy to visualize what is going on inside the model. The graphviz library can show the structure of the tree, as well as what decision is being made at each node.\n",
    "\n",
    "Due to some datahub limitations we can't use graphviz directly through our notebook. However, we can use the [Webgraphviz site](http://webgraphviz.com/) as a workaround. Run the cell below to generate the graphviz data for the model you just trained. Then, copy the *entire* output of the cell, click the link to Webgraphviz, replace the sample text in the text box with your copied data, and hit the button to generate the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the graphviz data\n",
    "print(export_graphviz(model, out_file = None, feature_names = X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on the visualization:\n",
    "- the top line of every node shows the decision that splits the data at that node\n",
    "- `samples` is the number of samples (rows) that are going through that node on the way down the tree\n",
    "- `value` is the value that would be predicted for all samples that stop at that node\n",
    "- `mse` is the mean squared error that would result from all of that node's samples being given `value` as their prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Averaging Methods <a id='section 2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the decision tree quickly maxed out its training set accuracy, but that accuracy didn't necessarily carry over to the validation set. We can try to address this overfitting issue using **averaging** ensemble methods. The intuition behind averaging is to build multiple estimators, then use the average of all their predictions as the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** accomplishes this by creating multiple decision trees (a 'forest' of them, if you will), each trained on sample of data drawn at random with replacement from the given set. Additionally, when each tree is constructed, not every feature is considered as a candidate on which to split the tree for each decision point.\n",
    "\n",
    "By adding some randomization into the subsets and features that are considered by each model, then averaging the predictions across models, Random Forest can typically produce a model that is better at generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an out-of-the-box `RandomForestRegessor()` below (i.e. use all the default settings), then fit it to the data and get the model's scores on the training and validation data. How does it compare to the Decision Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=10,  # number of trees\n",
    "                                        criterion='mse',  # how to measure fit\n",
    "                                        max_depth=None,  # how deep tree nodes can go\n",
    "                                        min_samples_split=2,  # samples needed to split node\n",
    "                                        min_samples_leaf=1,  # samples needed for a leaf\n",
    "                                        min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                                        max_features='auto',  # max feats\n",
    "                                        max_leaf_nodes=None,  # max nodes\n",
    "                                        n_jobs=1, # how many to run parallel\n",
    "                                        random_state=10)\n",
    "# fit the data \n",
    "model = rf_reg.fit(X_train, y_train)\n",
    "\n",
    "#score the model on the training and validation data\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with most models, we can get better results by tuning the hyperparameters of the model. Let's try changing three: `max_depth`, `n_estimators`, and `min_samples_split`.\n",
    "\n",
    "The code in the cell below allows you to compare the scatter plots and scores for different values of these parameters by changing the values in the different sliders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_by_param(depth, estimators, min_samples_split):\n",
    "    \"\"\"Train a DecisionTreeRegressor with max_depth = DEPTH, n_estimators=ESTIMATORS,\n",
    "    min_samples_split=MIN_SAMPLES_SPLIT and plot the scatter plot of predicted values \n",
    "    for training and validation data.\"\"\"\n",
    "    \n",
    "    # make and train the model\n",
    "    dt_reg = RandomForestRegressor(max_depth = depth, n_estimators=estimators, \n",
    "                                   min_samples_split = min_samples_split)\n",
    "    model = dt_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # predict values for training and validation sets\n",
    "    pred_y_val = model.predict(X_val)\n",
    "    pred_y_train = model.predict(X_train)\n",
    "    \n",
    "    # plot predicted against actual values for training and validation sets\n",
    "    plt.scatter(pred_y_train, y_train, label='training')\n",
    "    plt.scatter(pred_y_val, y_val, label='validation')\n",
    "    plt.xlabel('predicted count')\n",
    "    plt.ylabel('actual count')\n",
    "    plt.title('Random Forest Regression')\n",
    "    \n",
    "    # get scores for training and validation sets\n",
    "    print('training score: ', model.score(X_train, y_train))\n",
    "    print('validation score: ', model.score(X_val, y_val))\n",
    "\n",
    "# create the slider sfor the widget\n",
    "slider = widgets.IntSlider(min=1, max=21,step=1,value=0)\n",
    "slider2 = widgets.IntSlider(min=1, max=10,step=1,value=0)\n",
    "slider3 = widgets.IntSlider(min=2, max=50,step=5,value=0)\n",
    "\n",
    "# create the widget to view plots for different parameter values\n",
    "display(widgets.interactive(scatter_by_param, depth=slider, estimators=slider2, min_samples_split=slider3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** How does each parameter change the accuracy of predictions? Do any parameters affect training accuracy differently from validation accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of choosing good hyperparameters can be tedious, involving a lot of trial and error. Fortunately, Scikit-Learn has a tool to help.\n",
    "\n",
    "A **grid search** tests different possible parameter combinations to see which combination yields the best results. The grid is formatted as a dictionary, where the keys are the parameter names and the values are the different values you want to try for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameter grid to look for optimal values for n_estimators, max_depth, and _min_samples_split\n",
    "param_grid = {'n_estimators': range(450, 551, 50),\n",
    "              'max_depth': range(1, 12, 5),\n",
    "              'min_samples_split': [2]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the grid is made, it gets fed into a `GridSearchCV` along with the corresponding model. This may take several seconds to run- the computer is calculating the score for every possible combination of parameter values in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fit the model to the data, information about the search process and results is stored in `.cv_results_`. Here's what you can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keys stored in the grid search process results dictionary\n",
    "sorted(grid_search.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"params\" contains the different combinations of parameters that were tried. \"mean_test_score\" gives the average score for models using each set of parameters. Items are matched by index- the ith score is for the ith set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Find the set of parameters that got the best average score (`np.argmax` might help). Create a new random forest regressor using those parameters, then fit the model and print the scores for the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = ...\n",
    "best_params = ...\n",
    "\n",
    "# create the regressor\n",
    "rf_reg2 = ...\n",
    "\n",
    "# fit the data \n",
    "model = ...\n",
    "\n",
    "#score the model on the training and validation data\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as always, there is a shortcut- if you call `.predict` or `.score` on the grid search object you originally used to find the best parameters, it will do so using the best set of parameters automatically. Note that the score might be slightly different from the one in the model you just calculated due to the 'random' part of 'random forest'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting Methods <a id='section 3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting** algorithms work roughly like so:\n",
    "1. Make a weak predictor (one that makes predictions with slightly better-than-chance accuracy)\n",
    "2. Train and evaluate the weak predictor\n",
    "3. Make a new weak predictor that takes into account the errors made in the last model and improves on them.\n",
    "4. Repeat steps 2 and 3 many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada Boost (for ADAptive BOOSTing) is one of the most popular boosting algorithms. The adaptive part of the algorithm comes from how it updates the data for each weak model in the sequence.\n",
    "\n",
    "Each sample $i$ in the training set is weighted by some number $w_i$, and the input to the model is the samples multiplied by the weights. At first, all the $w_i$s are the same. After the first model is evaluated, the weights are updated so that samples that were predicted incorrectly get higher weights and samples that were predicted correctly get lower weights.\n",
    "\n",
    "**QUESTION:** In the playground game \"Duck, Duck, Goose\", children are labeled as 'ducks' or 'geese' in the name of schoolyard mayhem. Suppose we want to build a classifier that predicts whether a sample is a duck or a goose based on two features: color, and whether or not it quacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds = pd.DataFrame({'color':['white', 'grey', 'white'],\n",
    "                      'quacks':['yes', 'yes', 'no'],\n",
    "                     'type':['duck', 'duck', 'goose']})\n",
    "birds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, all the weights are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds['weights'] = np.ones(3) / 3\n",
    "birds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial model in a sequence for Ada Boost outputs the following predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds['predictions'] = ['duck', 'goose', 'goose']\n",
    "birds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For samples 0, 1 and 2, state whether their corresponding weight would be adjusted higher, lower, or stay the same before the data is fed into the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "- Sample 0: \n",
    "- Sample 1: \n",
    "- Sample 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an Ada Boost Regressor is nearly identical to using the other regressors covered today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the regressor\n",
    "ada_reg = AdaBoostRegressor(base_estimator=None,  # default is decision tree \n",
    "                                    n_estimators=50,  # number to try before stopping\n",
    "                                    learning_rate=1.0,  # decrease influence of each additional estimator\n",
    "                                    random_state=10, # sets the random seed\n",
    "                                    loss='linear')  # also ‘square’, ‘exponential’\n",
    "\n",
    "# fit the data\n",
    "ada_model = ada_reg.fit(X_train, y_train)\n",
    "\n",
    "# score the model\n",
    "print(ada_model.score(X_train, y_train))\n",
    "print(ada_model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Use grid search to find better values for the Ada Boost parameters. Refer to section 2 as needed for code examples.\n",
    "1. Create the parameter grid. Use any Ada Boost parameters you'd like (check the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html) for more details).\n",
    "2. Use scikit-learn tools to do the grid seach\n",
    "3. Print the scores for the validation data using the trained, optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary with parameter names and values to try\n",
    "param_grid = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the grid search\n",
    "grid_search_ab = ...\n",
    "\n",
    "# fit the data using the best parameters you found in grid search\n",
    "...\n",
    "\n",
    "# score the data\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "- decision trees make predictions using 'if/then/else' rules to split data into subsets, but are highly subject to overfitting\n",
    "- grid search can be a useful tool for tuning hyperparameters\n",
    "- ensemble methods seek to improve models by averaging or boosting multiple models\n",
    "- random forest uses randomness and averaging to counter the overfitting problem for decision trees\n",
    "- Ada boost can be used on a variety of model types to taking an initial weak model and improve it with sequential boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you go! (Feedback survey) <a id='section 4'></a>\n",
    "\n",
    "We on the LEGALST-190 Data Science Modules team have had a blast making your notebooks this semester- now, we'd like your feedback on how we did! Please fill out [this short (<5 minutes) survey](https://docs.google.com/forms/d/e/1FAIpQLSf2FSqRHCU7mKfAMho4Easi9DnRGAXuRYXviFTTGbvW-2SRTQ/viewform) about the Jupyter notebooks you've used in  your labs this semester. Did you want more graphing? Less math? Let us know!\n",
    "\n",
    "Thanks, and best of luck with all your future data science explorations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest, ADA Boost, grid search code adapted from https://github.com/dlab-berkeley/python-machine-learning/blob/master\n",
    "- Ensemble methods general reference: http://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Keeley Takimoto\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
