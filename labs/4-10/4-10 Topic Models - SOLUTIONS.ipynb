{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-190] Lab 4/10: Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will cover latent dirichlet allocation and topic models using `gensim` and `scikit-learn`.\n",
    "\n",
    "*Estimated Time: 35 Minutes *\n",
    "\n",
    "### Table of Contents\n",
    "[The Data](#section data)<br>\n",
    "1 - [Using Gensim to Implement a LDA Model](#section 1)<br>\n",
    "2 - [Using scikit-learn](#section 2)<br>\n",
    "3 - [Finding topics from UN Debates](#section 3)<br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "!pip install gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## The Data<a id='section data'></a>\n",
    "\n",
    "For this lab, we'll use sci-kit learn's `20 newsgroups` dataset, which is a list of approximately 18,000 newsgroup posts. Because of its size, we'll only be working with about 750 posts. At the end of this lab, we'll also work with a selected portion of the UN Data. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Using Gensim to Implement a LDA Model<a id='section 1'></a>\n",
    "\n",
    "### What Is Latent Dirichlet Allocation?\n",
    "Latent dirichlet allocation is a way of discovering topics in a set of documents, generating topics based on word frequency. LDA is a probabilistic bag-of-words model that makes an assumption that documents are produced from a variety of topics that produce words with certain probilities. Then it backtracks, finding a set of certain topics that would have created the documents.\n",
    "\n",
    "----\n",
    "\n",
    "### Using `gensim`\n",
    "\n",
    "We'll use the LDA algorithm from `gensim`, a python library for topic modelling.\n",
    "\n",
    "Let's get working with the data. The `20 newsgroups` data is under the name `20newgroups_data.csv` in the data folder. \n",
    "\n",
    "**Question 1.1:** Load your data into a DataFrame and retrieve the posts. Assign the list to a variable named `documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n \\nI read somewhere, I think in Morton Smith's _Jesus the Magician_, that\\nold Lazarus wasn't dead, but going in the tomb was part of an initiation\\nrite for a magi-cult, of which Jesus was also a part.   It appears that\\na 3-day stay was normal.   I wonder .... ?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SOLUTION\n",
    "data = pd.read_csv('data/20newsgroups_data.csv')\n",
    "documents = data['posts'].values\n",
    "documents[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We now have data we can work with. Before we start anything, we must clean the text!\n",
    "\n",
    "Just to review, we want to process our text by:<br>\n",
    "1) Tokenizing our document<br>\n",
    "2) Removing stop words (remove meaningless words)<br>\n",
    "3) Stemming or merging words that have equivalent meanings<br>\n",
    "\n",
    "<a id='gensim'></a>**Question 1.2:** Tokenize and stem the text in `documents` and filter your tokens by using both `stop` and `more_stops` and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "more_stops = ['--', '``', \"''\", \"s'\", \"\\'s\", \"n\\'t\", \"...\", \"\\'m\", \"-*-\", \"-|\"]\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Potential Solution\n",
    "stop = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "more_stops = ['--', '``', \"''\", \"s'\", \"\\'s\", \"n\\'t\", \"...\", \"\\'m\", \"-*-\", \"-|\"] \n",
    "tokenized = []\n",
    "for i in documents:\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(i) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = [x for x in tokens if x not in punctuation and x not in more_stops]\n",
    "    stopped_tokens = [x for x in filtered_tokens if not x in stop]\n",
    "    stemmed_tokens = [stemmer.stem(i) for i in stopped_tokens]\n",
    "    tokenized.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokenized documents, we have to convert it to a *document-term matrix* which can be done by instantiating a `gensim` dictionary object. Our first step is to turn our tokenized documents into a \"dictionary\" that maps a word to its integer ID, like a bag-of-words model. <a id='Q1.3'></a>\n",
    "\n",
    "**Question 1.3:** Implement a gensim dictionary from the `corpora` package and assign it to a variable named `dictionary`. You can look [at the documentation](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary) for the corpora package if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "dictionary = corpora.Dictionary(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step before we implement the model! We must convert our documents to bag-of-words format using our dictionary. Every document is represented as a list of tuples of the word's integer ID and its frequency. This list of 400 documents represents our document-term matrix.\n",
    "\n",
    "**Question 1.4:** Using `dictionary` from the previous question, convert to your tokenzied documents into a bag-of-words format and store it to a variable named `corpus`. We want to use `doc2bow()` method ***for every document*** in our tokenized text. The documentation is linked [here](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow).\n",
    "\n",
    "You should end up with a list of tuples for each document and make sure that `len(corpus)` is 400. Calling `corpus[i]` for some integer i should return something of the form `[(16, 2), (58, 1), (59, 1),...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a document-term matrix, weâ€™re ready to generate an LDA model!\n",
    "\n",
    "The cell below is an example of an implementation of a `gensim` LDA model. Run the cell and take a look at what it displays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.006*\"use\" + 0.005*\"would\" + 0.005*\"one\" + 0.004*\"peopl\" + 0.004*\"like\" + 0.004*\"know\" + 0.004*\"get\" + 0.003*\"imag\" + 0.003*\"see\" + 0.003*\"also\"\n",
      "\n",
      "Topic 1\n",
      "0.008*\"1\" + 0.007*\"2\" + 0.006*\"would\" + 0.006*\"use\" + 0.005*\"one\" + 0.005*\"peopl\" + 0.004*\"like\" + 0.004*\"god\" + 0.004*\"know\" + 0.004*\"3\"\n",
      "\n",
      "Topic 2\n",
      "0.004*\"hiv\" + 0.004*\"use\" + 0.004*\"one\" + 0.003*\"new\" + 0.003*\"would\" + 0.003*\"also\" + 0.003*\"aid\" + 0.003*\"get\" + 0.003*\"health\" + 0.003*\"peopl\"\n",
      "\n",
      "Topic 3\n",
      "0.005*\"would\" + 0.004*\"one\" + 0.004*\"use\" + 0.004*\"problem\" + 0.003*\"time\" + 0.003*\"know\" + 0.003*\"get\" + 0.003*\"go\" + 0.003*\"need\" + 0.003*\"two\"\n",
      "\n",
      "Topic 4\n",
      "0.005*\"one\" + 0.005*\"system\" + 0.004*\"use\" + 0.003*\"get\" + 0.003*\"would\" + 0.003*\"like\" + 0.003*\"gm\" + 0.003*\"think\" + 0.003*\"year\" + 0.002*\"know\"\n",
      "\n",
      "Topic 5\n",
      "0.007*\"would\" + 0.005*\"one\" + 0.005*\"like\" + 0.004*\"get\" + 0.004*\"well\" + 0.004*\"use\" + 0.004*\"time\" + 0.003*\"peopl\" + 0.003*\"want\" + 0.003*\"make\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldamodel = models.LdaModel(corpus, \n",
    "                           id2word=dictionary,\n",
    "                           num_topics=6,\n",
    "                           chunksize=270, \n",
    "                           update_every=20,\n",
    "                           passes=3)\n",
    "#We have defined a helper function show_topics that takes in the model as an argument.\n",
    "show_topics(ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "There are quite a few parameters for generating the `LdaModel` that affect the topics returned. We'll go over some of the parameters that will be helpful in implementing the LDA model.\n",
    "\n",
    "| Required Parameters        |Value                          | Default | What it does  |\n",
    "| :-------------------------:|:-----------------------------:| --|:-------------:|\n",
    "|                corpus      | corpus (doc-term matrix) | None | This specifies your LDA model parameters. |\n",
    "| id2word     | `gensim` dictionary | None | Maps integer IDs from <br>   the doc-term matrix to words. |\n",
    "| num_topics<br> | integer | 100 |Specifies the number of underlying topics <br> in your documents. Usually, the fewer <br> documents you have, the smaller number <br> you assign [(this is highly debated!)](https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling).<br>|\n",
    "\n",
    "\n",
    "| Optional (but helpful) Parameters   |Default | What it does  |\n",
    "|:------------------------------------|------- |:-------------------------------:|\n",
    "| passes | 1 | How many times you want to iterate through the corpus.<br> The more passes, the more accurate your model will be, <br> although  it can take longer time if you have a large dataset. |\n",
    "|chunksize | 2000 | The size of the batch documents you want to run through.<br> e.g. chunksize = 10, we run 10 documents at a time.| \n",
    "|update_every |1 | Update the model after every `n` number of chunks. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5:** What is problematic about the previous model and its results? Which parameters do you think you should change first to get more explicit results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q1.6a'></a> **Question 1.6a:** Improve the previous LDA model by adjusting the parameters  [(documentation)](https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel). Try to get as clear topics as possible! Remember to call `show_topics` on your model to print out the words for your topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.004*\"key\" + 0.004*\"govern\" + 0.003*\"one\" + 0.003*\"new\" + 0.003*\"year\" + 0.003*\"encrypt\" + 0.003*\"use\" + 0.003*\"church\" + 0.003*\"peopl\" + 0.003*\"also\"\n",
      "\n",
      "Topic 1\n",
      "0.018*\"1\" + 0.017*\"2\" + 0.009*\"3\" + 0.007*\"4\" + 0.005*\"drive\" + 0.005*\"use\" + 0.004*\"6\" + 0.003*\"copi\" + 0.003*\"5\" + 0.003*\"car\"\n",
      "\n",
      "Topic 2\n",
      "0.010*\"would\" + 0.008*\"one\" + 0.007*\"like\" + 0.007*\"peopl\" + 0.006*\"know\" + 0.006*\"think\" + 0.006*\"go\" + 0.005*\"get\" + 0.005*\"use\" + 0.005*\"time\"\n",
      "\n",
      "Topic 3\n",
      "0.006*\"use\" + 0.005*\"file\" + 0.005*\"system\" + 0.005*\"comput\" + 0.004*\"imag\" + 0.004*\"list\" + 0.004*\"program\" + 0.004*\"send\" + 0.004*\"data\" + 0.003*\"x\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Possible Solution\n",
    "possible_model = models.LdaModel(corpus, \n",
    "                                 id2word=dictionary,\n",
    "                                 num_topics=4,\n",
    "                                 chunksize=75, \n",
    "                                 update_every=2,\n",
    "                                 passes=15)\n",
    "show_topics(possible_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6b:** Did you notice any patterns while changing values of certain parameters (how did num_topics change the quality of results)? What worked in giving you reasonable, clear topics and what didn't? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 2: Using `scikit-learn`<a id='section 2'></a>\n",
    "\n",
    "Along with `gensim`,  we can also use `scikit-learn` to implement a LDA model. Using the `scikit-learn` algorithm is less clear, since a lot of the work is done by the computer. But, by going through the `gensim` algorithm, we now have an idea how LDA works, and using the `scikit-learn` algorithm will be a little more clear. \n",
    "\n",
    "You may be wondering, *why did we do all of that work in the first section when we can just use `scikit-learn` to implement a LDA model?* \n",
    "\n",
    "The motivation to use `gensim` is that you have much more control over how you can implement your model. This is a really big benefit if you want to find topics without having too much overlap, repetition, or inconsistency. For example, with `gensim`, you can tokenize and stem your documents in any way you want. But with `scikit-learn`, you don't have as much control over how you manipulate your data. Another example is the ability to filter your `gensim` dictionary instance, which you will explore later in the last section. If you prefer a more explicit method of implementing a topic model and want command over your data, then `gensim` is a great option.\n",
    "\n",
    "Anyway, let's get started with using `scikit-learn`!\n",
    "\n",
    "----\n",
    "\n",
    "**Question 2.1:** In order to implement a LDA model using `scikit-learn`, we must extract features to a matrix using either the count vectorizer or the tf-idf vectorizer. Which one do we use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#SOLUTION\n",
    "Because LDA is a probabilistic model, we use the CountVectorizer because we need raw counts of numbers, not weighted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "If you answered a count vectorizer to the previous question, you're right! Since LDA is a probabilistic model, we only need the raw term counts.\n",
    "\n",
    "<a id='sklearn'></a>\n",
    "\n",
    "**Question 2.2:** Instantiate a count vectorizer with the parameters `max_df=.95`, `min_df=2`, and `stop_words='english'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "countvec = CountVectorizer(max_df=.95, \n",
    "                           min_df=2, \n",
    "                           stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the vectorizer, we can transform our data into a document-term matrix, as well as use `.get_feature_names()` to get the word to integer ID mapping like we did in [question 1.3](#Q1.3).\n",
    "\n",
    "**Question 2.3:** Use your vectorizer to transform the same dataset from the first section of this lab to a document-term matrix (concept is similar to Q1.4) and get the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "cv = countvec.fit_transform(documents)\n",
    "cv_feature_names = countvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done! The last step is to implement the model, so we can get our topics. Similar to `gensim`, there are parameters that should be adjusted to fit your documents.\n",
    "\n",
    "| Parameters <br> [(documentation)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)|Default | What it does  |\n",
    "| :-------------------------:|:-----------------------------:|:-------------:|\n",
    "|          n_components      | 10 | Equivalent to `num_topics` in the `gensim` model. This <br> specifies the number of latent topics in your documents. |\n",
    "| max_iter | 10 | Equivalent to `passes` in the `gensim` model. |\n",
    "| batch_size | 128 | Equivalent to `chunksize` |\n",
    "\n",
    "**Question 2.4:** Implement the LDA model using `LatentDirichletAllocation`. Don't forget to fit your document-term matrix from the previous question!\n",
    "\n",
    "**Note:** Set the `learning_method` parameter to `'online'`, which is its default (for the latest version of scikit-learn) but will throw a deprecation warning if not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "lda = LatentDirichletAllocation(n_components=5, \n",
    "                                max_iter=25, \n",
    "                                batch_size = 99,\n",
    "                                learning_method='online').fit(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function `topic_words` is defined for you (from `helper.py`) and it takes in three arguments:\n",
    "\n",
    "     1) model: your LDA model\n",
    "     2) feature_names: the feature names from the vectorizer\n",
    "     3) num_top_words: number of words you want displayed\n",
    "\n",
    "It prints out the topic number and the words that fall under that topic, although it does not display weight of words like `gensim`.\n",
    "\n",
    "**Question 2.5:** Specify the number of words you want displayed and call `topic_and_words` on the LDA model from the previous question. \n",
    "\n",
    "<sub>**Note:** If your topics are repetitive or aren't very coherent, try tweaking the parameters in the previous question.</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "starters traded turk captain ve viola van bu toronto da clark creation signing ermeni handed\n",
      "Topic 1:\n",
      "edu graphics space mail pub 10 send data ray information com 128 aids 3d ftp\n",
      "Topic 2:\n",
      "just like don know think use good car ve way problem new time got need\n",
      "Topic 3:\n",
      "windows key gm use know using scsi thanks card used 16 drive does monitor help\n",
      "Topic 4:\n",
      "people god just said time don think like know good government did say didn does\n"
     ]
    }
   ],
   "source": [
    "#Possible solution\n",
    "num_top_words = 15\n",
    "topic_words(lda, cv_feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Question 2.6:** Did your model yield clear, interpretable results? How does it compare to the LDA model you created in [section one](#Q1.6a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Section 3: Finding topics from UN General Debates<a id='section 3'></a>\n",
    "\n",
    "We have two ways of implementing a LDA model, let's try both on the UN General Debates dataset. We can now get an idea of what was discussed at a specific session through topic modelling!\n",
    "\n",
    "**Question 3.1**: Load `un-general-debates-2015` from the data folder and extract the data from the 'text' column. This file contains the data from the 70th session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un = pd.read_csv('data/un-general-debates-2015.csv')\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "un = pd.read_csv('data/un-general-debates-2015.csv')\n",
    "un_text = un['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2:** First implement a LDA model using `gensim`. Follow similar steps from the [first section](#gensim), and adjust your parameters accordingly! Use the `show_topics` function to display your topics.\n",
    "\n",
    "**Tip:** Use the `filter_extremes(no_below=<int>)` method [(documentation)](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes) on your `gensim` dictionary, which helps filter through tokens based on frequency (in this case it'll keep any tokens contained in at least the specified integer number of documents). Feel free to use other parameters in `filter_extremes` to optimize your topics! \n",
    "\n",
    "As mentioned at the beginning of [section 2](#section 2), you really do have a lot of control over your model and I encourage you to utilize these tools to refine your topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.013*\"caribbean\" + 0.009*\"weapon\" + 0.008*\"saint\" + 0.008*\"02/10/2015\" + 0.008*\"venezuela\" + 0.007*\"cuba\" + 0.006*\"diseas\" + 0.006*\"dominica\" + 0.005*\"treati\" + 0.005*\"bahama\"\n",
      "\n",
      "Topic 1\n",
      "0.017*\"palestinian\" + 0.016*\"islam\" + 0.016*\"israel\" + 0.014*\"palestin\" + 0.011*\"syrian\" + 0.010*\"iran\" + 0.010*\"isra\" + 0.009*\"yemen\" + 0.009*\"korea\" + 0.008*\"muslim\"\n",
      "\n",
      "Topic 2\n",
      "0.040*\"island\" + 0.026*\"ocean\" + 0.017*\"pacif\" + 0.015*\"fiji\" + 0.014*\"sea\" + 0.013*\"sid\" + 0.011*\"impact\" + 0.008*\"rise\" + 0.007*\"access\" + 0.007*\"surviv\"\n",
      "\n",
      "Topic 3\n",
      "0.008*\"european\" + 0.006*\"say\" + 0.005*\"america\" + 0.005*\"never\" + 0.004*\"migrant\" + 0.004*\"think\" + 0.004*\"know\" + 0.004*\"propos\" + 0.004*\"iraq\" + 0.004*\"case\"\n",
      "\n",
      "Topic 4\n",
      "0.007*\"african\" + 0.003*\"reaffirm\" + 0.003*\"mechan\" + 0.003*\"object\" + 0.003*\"despit\" + 0.003*\"central\" + 0.003*\"road\" + 0.003*\"south\" + 0.003*\"fair\" + 0.003*\"vision\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Possible solution using gensim\n",
    "more_stops = ['--', '``', \"''\", \"s'\", \"\\'s\", \"n\\'t\", \"...\"]\n",
    "un_corp = []\n",
    "for i in un_text:\n",
    "    un_tokens = [word.lower() for sent in nltk.sent_tokenize(i) for word in nltk.word_tokenize(sent)]\n",
    "    un_filtered_tokens = [x for x in un_tokens if x not in punctuation and x not in more_stops]\n",
    "    un_stopped_tokens = [x for x in un_filtered_tokens if not x in stop]\n",
    "    un_stemmed_tokens = [stemmer.stem(i) for i in un_stopped_tokens]\n",
    "    un_corp.append(un_stemmed_tokens)\n",
    "\n",
    "un_dictionary = corpora.Dictionary(un_corp)\n",
    "\n",
    "un_dictionary.filter_extremes(no_below=5, no_above=.4)\n",
    "\n",
    "un_d_t = [un_dictionary.doc2bow(i) for i in un_corp]\n",
    "\n",
    "un_lda = models.LdaModel(un_d_t, num_topics=5,\n",
    "                            id2word=un_dictionary, \n",
    "                            chunksize=20, \n",
    "                            update_every=1,\n",
    "                            passes=17)\n",
    "\n",
    "show_topics(un_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3:** Now, implement a model using `scikit-learn`. Again, follow similar steps from the [second section](#sklearn) and adjust parameters accordingly. You can display your topics using `topic_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Possible solution using sklearn\n",
    "countvec = CountVectorizer(max_df=.90, \n",
    "                           min_df=2, \n",
    "                           stop_words='english')\n",
    "\n",
    "un_cv = countvec.fit_transform(un_text)\n",
    "un_cv_feature_names = countvec.get_feature_names()\n",
    "\n",
    "un_lda = LatentDirichletAllocation(n_components=5, \n",
    "                                   max_iter=30,\n",
    "                                   learning_decay=0.7,\n",
    "                                   batch_size=30,\n",
    "                                   learning_method='online').fit(un_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "community global 15 rights terrorism organization support change agenda climate\n",
      "Topic 1:\n",
      "islands solomon papua sri tobago trinidad lanka algeria herzegovina bosnia\n",
      "Topic 2:\n",
      "republic community change climate global agenda economic national sustainable year\n",
      "Topic 3:\n",
      "global sustainable rights climate agenda change support community challenges efforts\n",
      "Topic 4:\n",
      "state council terrorism israel political war today time law government\n"
     ]
    }
   ],
   "source": [
    "topic_words(un_lda, un_cv_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Question 3.4:** Which algorithm yielded more well-defined topics? You can also skim the resolutions passed [here](http://research.un.org/en/docs/ga/quick/regular/70) for reference. What do you think are some factors that need to be considered about the data when choosing an algorithm and adjusting its parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5:** What are some differences that you noticed between the `gensim` and `scikit-learn` algorithms? What are some of their drawbacks? Do you prefer one over the other and if so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Awesome! Now you know how to implement a topic model two ways using `gensim` and `scikit-learn`. Even though `scikit-learn` is more straightforward and requires less work to implement, the control you have over `gensim` is very valuable and can result in more distinct topics.\n",
    "\n",
    "Ultimately, the choice is yours and I hope having both options helps you generate great topic models!\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    " - Chen, Edwin, Introduction to latent dirichlet allocation. http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    " - Use of `20newsgroups` data set adapted from Topic Modelling tutorial by Aneesha Bakharia\n",
    " https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    " - Resolutions from UN 70th session: http://research.un.org/en/docs/ga/quick/regular/70\n",
    " - Text cleaning code adapted from notebook by Alex Estes https://github.com/dlab-berkeley/python-text-analysis/blob/master/Intro_to_TextAnalysis/Intro_to_TextAnalysis.ipynb\n",
    "\n",
    "----\n",
    "Notebook developed by: Jason Jiang\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
