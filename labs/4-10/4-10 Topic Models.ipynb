{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-190] Lab 4/10: Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will cover parsing XML and attribute lookup, XPath, and web scraping.\n",
    "\n",
    "*Estimated Time: 30 Minutes *\n",
    "\n",
    "### Topics Covered:\n",
    "- implement topic model\n",
    "- create document-term matrix\n",
    "- discover latent topics contributing to the docs\n",
    "\n",
    "### Table of Contents\n",
    "[The Data](#section data)<br>\n",
    "1 - [XML Syntax](#section 1)<br>\n",
    "2 - [Using XPath and ElementTree to parse XML](#section 2)<br>\n",
    "3 - [Web Scraping](#section 3)<br>\n",
    "4 - [Putting it all in a dataframe](#section 4)<br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from gensim import corpora, models, similarities \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd #Imported to work with the UN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## The Data<a id='section data'></a>\n",
    "Blurb about old bailey again\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: LDA<a id='section 1'></a>\n",
    "\n",
    "#### First with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "\n",
    "###Extra functions for extracting text from scraped XML files.\n",
    "def fetchtextclean(xml_file):\n",
    "    thing = []\n",
    "    for element in ET.ElementTree(file=xml_file).getiterator():\n",
    "        if element.tag == \"p\":\n",
    "            for i in list(element.itertext()):\n",
    "                thing += [i]\n",
    "    stringed = []\n",
    "    for i in thing:\n",
    "        clean = i.strip().replace('\\n', '')\n",
    "        if clean:\n",
    "            stringed.append(clean.strip('\\n'))\n",
    "    return stringed\n",
    "\n",
    "def unnester(lst):\n",
    "    new = []\n",
    "    for i in lst:\n",
    "        if type(i) == list:\n",
    "            new.extend(unnester(i))\n",
    "        else:\n",
    "            new.append(i)\n",
    "    return new\n",
    "\n",
    "###Found online. Will give credit in citations when I complete notebook.\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = [x for x in tokens if x not in punctuation \n",
    "                       and x not in more_stops] #word tokenizer cuts the possessives\n",
    "    return filtered_tokens\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    stems = [stemmer.stem(x) for x in tokenize_only(text)]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "exclude = string.punctuation\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Individual words from each case\n",
    "corp = []\n",
    "for i in np.arange(1, 314):\n",
    "    text = [i for i in clean(' '.join(fetchtextclean('old-bailey/case-'+str(i)+'.xml'))).split() if i not in stop]\n",
    "    corp.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "###This is code adapted from Chris Hench's notebook. I still had to load and join the text from the XML file.\n",
    "extracted = [tokenize_only(x) for x in [' '.join(fetchtextclean('old-bailey/case-'+str(i)+'.xml')) for i in range(1, 314)]]\n",
    "extracted = [[stemmer.stem(x) for x in i if x not in more_stops] for i in extracted]\n",
    "extracted = [[x for x in i if x not in stopwords.words(\"english\") and x not in punctuation] for i in extracted]\n",
    "###it also has individual words split from each case in their respsective lists but the words are tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Full text from each case\n",
    "corpus = []\n",
    "for i in np.arange(1, 314):\n",
    "    text = ' '.join([i for i in clean(' '.join(fetchtextclean('old-bailey/case-'+str(i)+'.xml'))).split() if i not in stop])\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,5))\n",
    "\n",
    "more_stops = ['q', 's', 'm', 'transportation', 'branding', 'came', 'said']\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus) #fit the vectorizer to the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(extracted)\n",
    "\n",
    "#dictionary.filter_extremes(no_below=50, no_above=.7)\n",
    "\n",
    "new = [dictionary.doc2bow(i) for i in corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"child\" + 0.017*\"saw\" + 0.012*\"upon\" + 0.012*\"handkerchief\" + 0.011*\"went\" + 0.010*\"two\" + 0.009*\"woman\" + 0.009*\"time\" + 0.008*\"mr\" + 0.008*\"mother\"'),\n",
       " (1,\n",
       "  '0.021*\"mr\" + 0.014*\"went\" + 0.013*\"upon\" + 0.012*\"man\" + 0.012*\"time\" + 0.011*\"two\" + 0.011*\"one\" + 0.011*\"year\" + 0.010*\"know\" + 0.010*\"money\"'),\n",
       " (2,\n",
       "  '0.025*\"tankard\" + 0.012*\"prosecutor\" + 0.012*\"taylor\" + 0.011*\"fletcher\" + 0.010*\"never\" + 0.010*\"cloth\" + 0.009*\"court\" + 0.008*\"bibbey\" + 0.008*\"humberston\" + 0.008*\"jane\"'),\n",
       " (3,\n",
       "  '0.024*\"went\" + 0.015*\"watch\" + 0.014*\"mr\" + 0.014*\"took\" + 0.013*\"saw\" + 0.012*\"man\" + 0.011*\"would\" + 0.011*\"go\" + 0.011*\"two\" + 0.010*\"door\"'),\n",
       " (4,\n",
       "  '0.043*\"one\" + 0.017*\"good\" + 0.014*\"pair\" + 0.014*\"thing\" + 0.014*\"went\" + 0.014*\"see\" + 0.013*\"two\" + 0.012*\"took\" + 0.012*\"john\" + 0.012*\"val\"')]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(new, num_topics=5,\n",
    "                            id2word=dictionary, \n",
    "                            chunksize=25, \n",
    "                            update_every=5,\n",
    "                            passes=10)\n",
    "\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sci-kit learn LDA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from article on medium (https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730)\n",
    "#just wanted to test it using sk and see if there were any differences\n",
    "#it seems like the results are slightly better but I'm still not sure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "no_features = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9,\n",
    "                                 min_df=.1, stop_words='english',\n",
    "                                 use_idf=True)#, ngram_range=(1,5))\n",
    "tf = tfidf_vectorizer.fit_transform(corpus)\n",
    "tf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "prisoner property john stealing handkerchief value money transportation thomas said summary guilty prosecutor\n",
      "Topic 1:\n",
      "john prisoner feb went pound stealing money thomas good house property said acquitted\n",
      "Topic 2:\n",
      "sheet linen value val certain mary widow lodging good let linnen stealing 12\n",
      "Topic 3:\n",
      "said prisoner went came watch house man mr took saw asked thing money\n"
     ]
    }
   ],
   "source": [
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=4, max_iter=3, \n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 13\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### UN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "un = pd.read_csv('un-general-debates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,5))\n",
    "\n",
    "more_stops = ['q', 's', 'm', 'transportation', 'branding']\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "fitter = tfidf_vectorizer.fit_transform(un[un['session'] == 44]['text'].values) #fit the vectorizer to the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_t = [tokenize_only(x) for x in un[un['session'] == 44]['text'].values] #Taking all text from 44th session\n",
    "un_t = [[stemmer.stem(x) for x in i if x not in more_stops] for i in un_t]\n",
    "un_t = [[x for x in i if x not in stopwords.words(\"english\") and x not in punctuation] for i in un_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Gensim dictionary from the texts\n",
    "un_dictionary = corpora.Dictionary(un_t)\n",
    "\n",
    "un_dictionary.filter_extremes(no_below=10)\n",
    "\n",
    "un_d_t = [un_dictionary.doc2bow(i) for i in un_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"cambodia\" + 0.003*\"cambodian\" + 0.003*\"iraq\" + 0.003*\"iran\" + 0.003*\"cyprus\" + 0.003*\"occup\" + 0.003*\"interfer\" + 0.003*\"1988\" + 0.002*\"viet\" + 0.002*\"angola\"'),\n",
       " (1,\n",
       "  '0.006*\"europ\" + 0.005*\"democraci\" + 0.004*\"american\" + 0.004*\"european\" + 0.003*\"latin\" + 0.003*\"traffick\" + 0.003*\"individu\" + 0.003*\"terror\" + 0.003*\"violenc\" + 0.002*\"reform\"'),\n",
       " (2,\n",
       "  '0.003*\"peace-keep\" + 0.003*\"traffick\" + 0.003*\"democraci\" + 0.003*\"small\" + 0.003*\"centuri\" + 0.002*\"oper\" + 0.002*\"report\" + 0.002*\"strong\" + 0.002*\"japan\" + 0.002*\"fee\"')]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_lda = models.LdaModel(un_d_t, num_topics=3,\n",
    "                            id2word=un_dictionary, \n",
    "                            chunksize=25, \n",
    "                            update_every=10,\n",
    "                            passes=5)\n",
    "\n",
    "un_lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
