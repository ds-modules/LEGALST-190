{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-190] Lab 4/10: Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will cover latent dirichlet allocation and topic models using `gensim` and `scikit-learn`.\n",
    "\n",
    "*Estimated Time: 35 Minutes *\n",
    "\n",
    "### Table of Contents\n",
    "[The Data](#section data)<br>\n",
    "1 - [Using Gensim to Implement a LDA Model](#section 1)<br>\n",
    "2 - [Using scikit-learn](#section 2)<br>\n",
    "3 - [Finding topics from UN Debates](#section 3)<br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## The Data<a id='section data'></a>\n",
    "\n",
    "For this lab, we'll use sci-kit learn's `20 newsgroups` dataset, which is a list of approximately 18,000 newsgroup posts. At the end of this lab, we'll also work with a selected portion of the UN Data. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Using Gensim to Implement a LDA Model<a id='section 1'></a>\n",
    "\n",
    "### What Is Latent Dirichlet Allocation?\n",
    "Latent dirichlet allocation is a way of discovering topics in a set of documents, generating topics based on word frequency. LDA is a probabilistic bag-of-words model that makes an assumption that documents are produced from a variety of topics that produce words with certain probilities. Then it backtracks, finding a set of certain topics that would have created the documents.\n",
    "\n",
    "----\n",
    "\n",
    "### Using `Gensim`\n",
    "\n",
    "We'll use the LDA algorithm from `Gensim`, a python library for topic modelling.\n",
    "\n",
    "Let's get working with the data. Run the cell below to load the data from `20newsgroups`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've loaded the dataset, use the `.data` method on the set to get the list of newgroups posts.\n",
    "\n",
    "**Question 1.1:** Retrieve the first 400 documents in the set and assign it to a variable named `documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n \\nI read somewhere, I think in Morton Smith's _Jesus the Magician_, that\\nold Lazarus wasn't dead, but going in the tomb was part of an initiation\\nrite for a magi-cult, of which Jesus was also a part.   It appears that\\na 3-day stay was normal.   I wonder .... ?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SOLUTION\n",
    "documents = dataset.data[:400]\n",
    "documents[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If nothing returns, you're good to go.\n",
    "assert len(documents) == 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We now have data we can work with. Before we start anything, we must clean the text!\n",
    "\n",
    "Just to review, we want to process our text by:<br>\n",
    "1) Tokenizing our document<br>\n",
    "2) Removing stop words (remove meaningless words)<br>\n",
    "3) Stemming or merging words that have equivalent meanings<br>\n",
    "\n",
    "<a id='gensim'></a>**Question 1.2:** Tokenize and stem the text in `documents`. The first line of code is provided, as well as a stop words (use both `stop` and `more_stops`) and punctuation that should be filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We used four (incl. given one) list comprehensions in total, and appended the final list of cleaned text to a list.\n",
    "stop = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "more_stops = ['--', '``', \"''\", \"s'\", \"\\'s\", \"n\\'t\", \"...\", \"\\'m\", \"-*-\"]\n",
    "tokenized = []\n",
    "for i in documents:\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(i) for word in nltk.word_tokenize(sent)]\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "stop = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "more_stops = ['--', '``', \"''\", \"s'\", \"\\'s\", \"n\\'t\", \"...\", \"\\'m\", \"-*-\"] \n",
    "tokenized = []\n",
    "for i in documents:\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(i) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = [x for x in tokens if x not in punctuation and x not in more_stops]\n",
    "    stopped_tokens = [x for x in filtered_tokens if not x in stop]\n",
    "    stemmed_tokens = [stemmer.stem(i) for i in stopped_tokens]\n",
    "    tokenized.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokenized documents, we have to convert it to a *document-term matrix* which can be done by instantiating a `gensim` dictionary object. Our first step is to turn our tokenized documents into a \"dictionary\" that maps a word to its integer ID, like a bag-of-words model. <a id='Q1.3'></a>\n",
    "\n",
    "**Question 1.3:** Implement a gensim dictionary from the `corpora` package and assign it to a variable named `dictionary`. You can look [at the documentation](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary) for the corpora package if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "dictionary = corpora.Dictionary(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step before we implement the model! We must convert our documents to bag-of-words format using our dictionary. Every document is represented as a list of tuples of the word's integer ID and its frequency. This list of 400 documents represents our document-term matrix.\n",
    "\n",
    "**Question 1.4:** Using `dictionary` from the previous question, convert to your tokenzied documents into a bag-of-words format and store it to a variable named `corpus`. We want to use `doc2bow()` method ***for every document*** in our tokenized text. The documentation is linked [here](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow).\n",
    "\n",
    "You should end up with a list of tuples for each document and make sure that `len(corpus)` is 400. Calling `corpus[i]` for some integer i should return `[(16, 2), (58, 1), (59, 1),...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a document-term matrix, weâ€™re ready to generate an LDA model! There are quite a few parameters for generating the `LdaModel` that affect the topics returned. We'll go over some of the parameters that will be helpful in implementing the LDA model.\n",
    "\n",
    "| Required Parameters        |Value                          | Default | What it does  |\n",
    "| :-------------------------:|:-----------------------------:| --|:-------------:|\n",
    "|                corpus      | corpus (doc-term matrix) | None | This specifies your LDA model parameters. |\n",
    "| num_topics<br> | integer | 100 |Specifies the number of underlying<br>topics in  your documents. Usually, the <br> fewer documents  you have, the  smaller <br> number you assign [(this is highly debated!)](https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling).<br>|\n",
    "| id2word     | `gensim` dictionary | None | Maps integer IDs from <br>   the doc-term matrix to words. |\n",
    "\n",
    "| Optional (but helpful) Parameters   |Default | What it does  |\n",
    "|:------------------------------------|------- |:-------------------------------:|\n",
    "| passes | 1 | How many times you want to iterate through the corpus.<br> The more passes, the more accurate your model will be, <br> although  it can take longer time if you have a large dataset. |\n",
    "|chunksize | 2000 | The size of the batch documents you want to run through.<br> e.g. chunksize = 10, we run 10 documents at a time.| \n",
    "|update_every |1 | Update the model after every `n` number of chunks. |\n",
    "\n",
    "<a id='Q1.5A'></a> **Question 1.5a:** Implement a LDA model using `LdaModel` and adjust the parameters in the function so that you get `num_topics` distinct topics back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One of many solutions\n",
    "ldamodel = models.LdaModel(corpus, \n",
    "                           num_topics=4,\n",
    "                           id2word=dictionary, \n",
    "                           chunksize=150, \n",
    "                           update_every=1,\n",
    "                           passes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to display the words that represent underlying topics in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.009*\"one\" + 0.007*\"use\" + 0.007*\"would\" + 0.006*\"get\" + 0.005*\"like\" + 0.005*\"year\" + 0.004*\"time\" + 0.004*\"two\" + 0.004*\"could\" + 0.004*\"good\"\n",
      "\n",
      "Topic 1\n",
      "0.009*\"peopl\" + 0.009*\"would\" + 0.005*\"one\" + 0.004*\"like\" + 0.004*\"god\" + 0.004*\"make\" + 0.003*\"say\" + 0.003*\"state\" + 0.003*\"said\" + 0.003*\"hiv\"\n",
      "\n",
      "Topic 2\n",
      "0.005*\"file\" + 0.005*\"imag\" + 0.004*\"send\" + 0.004*\"mail\" + 0.004*\"graphic\" + 0.004*\"object\" + 0.004*\"format\" + 0.003*\"also\" + 0.003*\"packag\" + 0.003*\"3d\"\n",
      "\n",
      "Topic 3\n",
      "0.007*\"orbit\" + 0.006*\"1\" + 0.005*\"2\" + 0.005*\"launch\" + 0.005*\"3\" + 0.005*\"probe\" + 0.005*\"mission\" + 0.005*\"gm\" + 0.004*\"titan\" + 0.004*\"earth\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ldamodel.show_topics():\n",
    "    print('Topic ' + str(i[0]))\n",
    "    print(i[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5b:** From your model, what are some topics that you can infer? Does changing `num_topics` affect the quality of your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6:** How did you go about adjusting the parameters of your `LdaModel`? Did you notice any patterns while changing values of certain parameters? What worked in giving you reasonable, clear topics and what didn't? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Section 2: Using scikit-learn<a id='section 2'></a>\n",
    "\n",
    "Along with `gensim`,  we can also use `scikit-learn` to implement a LDA model. Using the `scikit-learn` algorithm is less clear, since a lot of the work is done by the computer. But, by going through the `gensim` algorithm, we now have an idea how LDA works, and using `scikit-learn` algorithm will be a litt.\n",
    "\n",
    "**Question 2.1:** In order to implement a LDA model using `scikit-learn`, we must extract features to a matrix using either the count vectorizer or the tf-idf vectorizer. Which one do we use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#SOLUTION\n",
    "Because LDA is a probabilistic model, we use the CountVectorizer because we need raw counts of numbers, not weighted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "If you answered a count vectorizer to the previous question, you're right! Since LDA is a probabilistic model, we only need the raw term counts.\n",
    "\n",
    "<a id='sklearn'></a>\n",
    "\n",
    "**Question 2.2:** Implement a count vectorizer with the parameters `max_df=.95`, `min_df=2`, and `stop_words='english'`. For more explanation on these parameters, [look at the documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "countvec = CountVectorizer(max_df=.95, \n",
    "                           min_df=2, \n",
    "                           stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the vectorizer, we can transform our data into a document-term matrix, as well as use `.get_feature_names()` to get the word to integer ID mapping like we did in [question 1.3](#Q1.3).\n",
    "\n",
    "**Question 2.3:** Use your vectorizer to transform the same dataset from the first section of this lab to a document-term matrix (concept is similar to Q1.4) and get the feature names. The specified methods to call are found [here (link to documentation)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "cv = countvec.fit_transform(documents[:400])\n",
    "cv_feature_names = countvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done! The last step is to implement the model, so we can get our topics. Similar to `gensim`, there are parameters that should be adjusted to fit your documents.\n",
    "\n",
    "| Parameters <br> [(documentation)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)|Default | What it does  |\n",
    "| :-------------------------:|:-----------------------------:|:-------------:|\n",
    "|          n_components      | 10 | Equivalent to `num_topics` in the `gensim` model. This <br> specifies the number of latent topics in your documents. |\n",
    "| max_iter | 10 | Equivalent to `passes` in the `gensim` model. |\n",
    "| batch_size | 128 | Equivalent to `chunksize` |\n",
    "\n",
    "**Question 2.4:** Implement the LDA model using `LatentDirichletAllocation`. Don't forget to fit your document-term matrix from the previous question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "lda = LatentDirichletAllocation(n_components=5, \n",
    "                                max_iter=25, \n",
    "                                learning_method='online').fit(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function `topic_words` is defined for you (from `helper.py`) and it takes in three arguments:\n",
    "\n",
    "     1) model: your LDA model\n",
    "     2) feature_names: the feature names from the vectorizer\n",
    "     3) num_top_words: number of words you want displayed\n",
    "\n",
    "It prints out the topic number and the words that fall under that topic, although it does not display weight of words like `gensim`.\n",
    "\n",
    "**Question 2.5:** Specify the number of words you want displayed and call `topic_and_words` on the LDA model from the previous question. \n",
    "\n",
    "<sub>**Note:** If your topics are repetitive or aren't very coherent, try tweaking the parameters in the previous question.</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "edu graphics like mail know pub don comp using send mac want program things ray\n",
      "Topic 1:\n",
      "key government armenians keys encryption chip people clipper know going public karabagh phone armenia door\n",
      "Topic 2:\n",
      "aids gm health van ik information infected patients medical fake said care met people trials\n",
      "Topic 3:\n",
      "years space people god little time build didn ve built use generation great rocket just\n",
      "Topic 4:\n",
      "just food good time way like think don msg power car question year monitor people\n"
     ]
    }
   ],
   "source": [
    "#Possible solution\n",
    "num_top_words = 15\n",
    "topic_words(lda, cv_feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Question 2.6:** Did your model yield clear, interpretable results? How does it compare to the LDA model you created in [section one](#Q1.5A)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Section 3: Finding topics from UN General Debates<a id='section 3'></a>\n",
    "\n",
    "We have two ways of implementing a LDA model, let's try both on the UN General Debates dataset. We can now get an idea of what was discussed at a certain session through topic modelling! \n",
    "\n",
    "**Question 3.1**: Load `un-general-debates-25` from the data folder and extract the data from the 'text' column. This csv contains the data from the 25th session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un = pd.read_csv('data/un-general-debates-25.csv')\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "un = pd.read_csv('data/un-general-debates-25.csv')\n",
    "un_text = un['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2:** First implement a LDA model using `gensim`. Follow similar steps from the [first section](#gensim), but adjust your parameters accordingly!\n",
    "\n",
    "**Note:** Use the `filter_extremes(no_below=10)` [(documentation)](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes) method on your `gensim` dictionary, which helps filtering through the tokens based on frequency (in this case it'll keep any words with a count/frequency less than 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"african\" + 0.010*\"southern\" + 0.007*\"portug\" + 0.007*\"racist\" + 0.006*\"portugues\" + 0.006*\"rhodesia\" + 0.006*\"lusaka\" + 0.005*\"oppress\" + 0.005*\"namibia\" + 0.004*\"guinea\"'),\n",
       " (1,\n",
       "  '0.019*\"imperialist\" + 0.014*\"american\" + 0.012*\"cambodia\" + 0.012*\"palestinian\" + 0.012*\"liber\" + 0.011*\"imperi\" + 0.009*\"socialist\" + 0.008*\"democrat\" + 0.008*\"korea\" + 0.007*\"europ\"'),\n",
       " (2,\n",
       "  '0.007*\"sea\" + 0.005*\"american\" + 0.005*\"seab\" + 0.005*\"per\" + 0.004*\"latin\" + 0.004*\"jurisdict\" + 0.004*\"cent\" + 0.004*\"draft\" + 0.004*\"floor\" + 0.003*\"concept\"'),\n",
       " (3,\n",
       "  '0.015*\"israel\" + 0.005*\"ceasefir\" + 0.004*\"land\" + 0.004*\"neighbor\" + 0.004*\"jordan\" + 0.004*\"hijack\" + 0.003*\"europ\" + 0.003*\"coexist\" + 0.003*\"aid\" + 0.003*\"palestin\"'),\n",
       " (4,\n",
       "  '0.016*\"european\" + 0.015*\"europ\" + 0.005*\"draft\" + 0.005*\"context\" + 0.005*\"feder\" + 0.005*\"factor\" + 0.005*\"chemic\" + 0.004*\"submit\" + 0.004*\"stress\" + 0.004*\"court\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Possible solution using gensim\n",
    "more_stops = ['--', '``', \"''\", \"s'\", \"\\'s\", \"n\\'t\", \"...\"]\n",
    "un_corp = []\n",
    "for i in un_text:\n",
    "    un_tokens = [word.lower() for sent in nltk.sent_tokenize(i) for word in nltk.word_tokenize(sent)]\n",
    "    un_filtered_tokens = [x for x in un_tokens if x not in punctuation and x not in more_stops]\n",
    "    un_stopped_tokens = [x for x in un_filtered_tokens if not x in stop]\n",
    "    un_stemmed_tokens = [stemmer.stem(i) for i in un_stopped_tokens]\n",
    "    un_corp.append(un_stemmed_tokens)\n",
    "\n",
    "un_dictionary = corpora.Dictionary(un_corp)\n",
    "\n",
    "un_dictionary.filter_extremes(no_below=9)\n",
    "\n",
    "un_d_t = [un_dictionary.doc2bow(i) for i in un_corp]\n",
    "\n",
    "un_lda = models.LdaModel(un_d_t, num_topics=5,\n",
    "                            id2word=un_dictionary, \n",
    "                            chunksize=20, \n",
    "                            update_every=2,\n",
    "                            passes=17)\n",
    "\n",
    "un_lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3:** Now, implement a model using `scikit-learn`. Again, follow similar steps from the [second section](#sklearn) and adjust parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Possible solution using sklearn\n",
    "countvec = CountVectorizer(max_df=.95, \n",
    "                           min_df=2, \n",
    "                           stop_words='english')\n",
    "\n",
    "un_cv = countvec.fit_transform(un_text)\n",
    "un_cv_feature_names = countvec.get_feature_names()\n",
    "\n",
    "un_lda = LatentDirichletAllocation(n_components=4, \n",
    "                                   max_iter=30,\n",
    "                                   learning_decay=0.4,\n",
    "                                   batch_size=15,\n",
    "                                   learning_method='online').fit(un_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "flying absolve regionally riches falling fatal possesses accredited surprised augury\n",
      "Topic 1:\n",
      "people great republic arab africa rights political problems human principles\n",
      "Topic 2:\n",
      "law committee sea regional conference latin work north problems council\n",
      "Topic 3:\n",
      "israel india pakistan ceasefire lebanon jordan aircraft kashmir hijackers egypt\n"
     ]
    }
   ],
   "source": [
    "topic_words(un_lda, un_cv_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4:** Which algorithm yielded more well-defined topics (you can also skim the resolutions passed [here](http://research.un.org/en/docs/ga/quick/regular/25) if you're interested)? What do you think are some are factors that need to be considered when choosing an algorithm and adjusting its parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5:** What are some differences between the `gensim` and `scikit-learn` algorithms? What are some of their drawbacks? Do you prefer one over the other? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    " - Chen, Edwin, Introduction to latent dirichlet allocation. http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    " - Use of `20newsgroups` data set adapted from Topic Modelling tutorial by Aneesha Bakharia\n",
    " https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    " - UN 25th session from http://research.un.org/en/docs/ga/quick/regular/25\n",
    " - Text cleaning code adapted from notebook by Alex Estes https://github.com/dlab-berkeley/python-text-analysis/blob/master/Intro_to_TextAnalysis/Intro_to_TextAnalysis.ipynb\n",
    "\n",
    "----\n",
    "Notebook developed by: Jason Jiang\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
