{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-190] Lab 4/12: Morality and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will cover morality and sentiment analysis using the *Moral Foundations Theory* with dictionary-based analysis, connecting to topic modeling and classifications ideas from previous labs.\n",
    "\n",
    "### Table of Contents\n",
    "[The Data](#section data)<br>\n",
    "[Goal and Question](#section goal)<br>\n",
    "1 - [Text Pre-processing](#section 1)<br>\n",
    "2 - [Polarity](#section 2)<br>\n",
    "3 - [Moral Foundations Theory](#section 3)<br>\n",
    "4 - [Non-negative matrix factorization](#section 4)<br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## The Data<a id='section data'></a>\n",
    "\n",
    "For this lab, we'll use the Old Bailey dataset, something you all should be familiar with now. The size of the dataset is also rather large so we will compare two year-long periods, one from before 1827 and one after. Read the question to better understand why we look at 1827."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal and Question<a id='section goal'></a>\n",
    "\n",
    "The goal of today's lab is to explore sentiment analysis with three different approaches – [polarity scoring](#section 2), [topic-specific dictionary methods](#section 3), and [topic modeling](#section 4).\n",
    "\n",
    "We'll look at sentiment in the context of the following question: \n",
    "\n",
    "**Did the way judges, prosecutors, and witnesses talk about moral culpability change after the Bloody Code was mostly repealed in 1827 (at the leading edge of a wave of legal reform in England)?**\n",
    "\n",
    "*Note: this is a question that could encompass an entire research project. Today's lab uses a very small subset of data due to datahub memory limitations, and skips over many of the steps needed for truly robust conclusions. *\n",
    "\n",
    "Something to think about: What are some things you would need to consider before answering this question?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Text Pre-processing<a id='section 1'></a>\n",
    "\n",
    "### Before we start\n",
    "This dataset we are about to look at is incredibly large, so to avoid crashing our datahub kernel, we only consider two years: 1822 and 1832. These two years were chosen as periods that were equally far from 1827 (when the Bloody Code was mostly repealed), while not being so far from each other that we'd expect to see major language usage change due only to time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Let's get working with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains Old Bailey trial data from 1822 and 1832\n",
    "old_bailey = pd.read_csv('data/obc_1822_1832.csv', index_col='trial_id')\n",
    "# select only the columns we need for this lab \n",
    "old_bailey = old_bailey.loc[:, ['year', 'transcript']]\n",
    "old_bailey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We now have data we can work with. Before we start anything, we must clean the text!\n",
    "\n",
    "Just to review, we want to process our text by:<br>\n",
    "1) Lowercasing the words<br>\n",
    "2) Cleaning up punctuation<br>\n",
    "3) Splitting into individual words<br>\n",
    "4) Stemming the word tokens<br>\n",
    "\n",
    "For the sake of time (and to get to the good stuff), we've provided the pre-processing code below. This a big data set, so the code will take up to a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the data\n",
    "lower_cased = old_bailey['transcript'].str.lower()\n",
    "punct_re = r'[^\\w\\s]'\n",
    "lower_no_punc = lower_cased.str.replace(punct_re, ' ')\n",
    "tokens = lower_no_punc.str.split()\n",
    "old_bailey['tokens'] = tokens\n",
    "stemmer = SnowballStemmer('english')\n",
    "stem_lists = []\n",
    "\n",
    "for token_list in old_bailey['tokens']:\n",
    "    stem_lists.append([stemmer.stem(wd) for wd in token_list])\n",
    "\n",
    "old_bailey['stemmed_tokens'] = stem_lists\n",
    "\n",
    "\n",
    "old_bailey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 2: Polarity <a id='section 2'></a>\n",
    "\n",
    "One way to measure the tone of a text is to look at the text **polarity**: a measure of how positive or negative it is perceived to be. For example, a sentence like \"I love Berkeley!\" would be considered positive, while a sentence like \"Stanford is terrible!\" would be negative. And, because polarity is represented as a scale, some words have stronger positive or negative sentiment than others- \"I like data science\" is positive, but not as positive as \"I love data science.\"\n",
    "\n",
    "We will use the [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis) tools to analyze the sentiment of Old Bailey. TextBlob provides access to many common text-processing operations, and includes a  lexicon and rule-based sentiment analysis tool.\n",
    "\n",
    "A TextBlob is created around string of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a sentiment analyzer\n",
    "blob = TextBlob(\"This is a super exciting, totally awesome test sentence.\")\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the sentiment by using `.sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sentiment` returns two values: the **polarity** and the **subjectivity**. The polarity ranges between -1 and 1 where -1 is a very negative text and 1 is a very positive text. Subjectivity ranges between 0 and 1 where 0 is a very objective text and 1 is a very subjective text (i.e. one that can be interpreted many different ways). You can get the polarity by using `.polarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity is calculated fairly simply: TextBlob accesses a dictionary of words that have been assigned polarity and subjectivity scores, looks up each word in the given text, and averages over the sentence. It also employs a few rules, such as changing the polarity of a word that comes after a negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = TextBlob('Happy')\n",
    "print(happy.sentiment.polarity)\n",
    "\n",
    "negation = TextBlob('Not')\n",
    "print(negation.sentiment.polarity)\n",
    "\n",
    "negated_happy = TextBlob('Not happy')\n",
    "print(negated_happy.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Try calculating the polarity scores of a few of your own sentences in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the polarity scoring for different sentences\n",
    "my_blob = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to get the average polarity for each transcript. \n",
    "\n",
    "**EXERCISE:** define a function that will take in a string of text and return the polarity of that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(text):\n",
    "    \"\"\"Return the polarity of TEXT\"\"\"\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Using `.apply` and your `get_polarity` function, get the polarity of every transcript in the Old Bailey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the polarities as a column\n",
    "old_bailey['polarity'] = polarities\n",
    "old_bailey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** \n",
    "- What was the most negative transcript/transcripts?\n",
    "- What was the most positive transcript/transcripts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the transcript with the highest polarity\n",
    "most_pos = ...\n",
    "most_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the transcript with the lowest polarity\n",
    "most_neg = ...\n",
    "most_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Let's take a look at violin plots of these two datasets to better compare how the average compound polarity is distributed for each of the two years, before and after 1827.\n",
    "\n",
    "To show both years at once, it's easiest to use the Seaborn (abbreviated as `sns`) visualization library function. `y` is set to the name of the variable (a string) whose distributions we want to see. `x` is set to the name of the variable (also a string)that we want to compare distributions for . `data` is set to the dataframe (not a string) with all the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the next line and fill in the code to create the violin plots\n",
    "#sns.violinplot(x=..., y=..., data=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What does this plot show us? \n",
    "\n",
    "What are some advantages to using polarity as a way to measure moral tone? What are some issues with this approach? Consider also how these answers might change for a different data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Section 3: Moral Foundations Theory<a id='section 3'></a>\n",
    "\n",
    "Another approach is to create specialized dictionaries containing specific words of interest to try to analyze sentiment from a particular angle (i.e. use a **dictionary method**). One set of researchers did just that from the perspective of [Moral Foundations Theory](http://moralfoundations.org/). We will now use it to see if we can understand more about the moral tone of Old Bailey transcripts than by using general polarity. You should be doing something like this for your homework. We will be using a provided moral foundations dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/haidt_dict.json') as json_data:\n",
    "    mft_dict = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moral Foundations Theory posits that there are five (with an occasional sixth) innane, universal psychological foundations of morality, and that those foundations shape human cultures and institutions (including legal). The keys of the dictionary correspond to the five foundations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the keys of the dictionary provided\n",
    "keys = mft_dict.keys()\n",
    "list(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the values of the dictionary are lists of words associated with each foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mft_dict[list(keys)[0]] #one example of the values provided for the first key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Percentages \n",
    "\n",
    "In this approach, we'll use the frequency of Moral Foundations-related words as a measure of how the transcripts talk about morality and see if there's a difference between pre- and post-1827 trends. \n",
    "\n",
    "As a first step, we need to know the total number of words in each transcript. \n",
    "\n",
    "**EXERCISE:** Add a column to `old_bailey` with the number of words corresponding to each transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column called 'total_words'\n",
    "old_bailey['total_words'] = ...\n",
    "old_bailey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to calculate the number of matches to entries in our dictionary for each foundation for each speech.\n",
    "\n",
    "Run the next cell to add six new columns to `old_bailey`, one per foundation, that show the number of word matches. This cell will also likely take some time to run (no more than a minute). Note that by now, you have the skills to write all the code in the next cell- we're just giving it to you because it's long, fiddly, and writing nested for-loops is not the focus of this lab. Make sure you know what it does before you move on, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a bit of time to run due to the large size.\n",
    "\n",
    "# do the following code for each foundation\n",
    "for foundation in mft_dict.keys():\n",
    "    # create a new, empty column\n",
    "    num_match_words = np.zeros(len(old_bailey))\n",
    "    stems = mft_dict[foundation]\n",
    "    \n",
    "    # do the following code for each foundation word\n",
    "    for stem in stems:\n",
    "        # find related word matches\n",
    "        wd_count = np.array([sum([wd == stem for wd in transcript])for transcript in old_bailey['stemmed_tokens']])\n",
    "        # add the number of matches to the total\n",
    "        num_match_words += wd_count\n",
    "        \n",
    "    # create a new column for each foundation with the number of related words per transcript\n",
    "    old_bailey[foundation] = num_match_words\n",
    "\n",
    "old_bailey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** The columns for each foundation currently contain the number of words related to that foundation for each of the trials. Calculate the *percentage* of foundation words per trial by dividing the number of matched words by the number of total words and multiplying by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this for each foundation column\n",
    "for foundation in mft_dict.keys():\n",
    "    old_bailey[foundation] = ...\n",
    "\n",
    "old_bailey.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the average percentage of foundation words per transcript for the two dates, 1822, and 1832.\n",
    "\n",
    "**EXERCISE**: Create a dataframe that only has columns for the five foundations plus the year. Then, use the pandas dataframe function `groupby` to group rows by the year, and call the `mean` function on the `groupby` output to get the averages for each foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the names of the columns we want to keep\n",
    "mft_columns = ['authority/subversion', 'care/harm', 'fairness/cheating', 'loyalty/betrayal',\n",
    " 'sanctity/degradation', 'year']\n",
    "\n",
    "# create a data frame with only the above columns included\n",
    "mft_df = ...\n",
    "\n",
    "# groups the rows of mft_df by year, then take the mean\n",
    "foundation_avgs = ...\n",
    "\n",
    "foundation_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a bar graph. The simplest way is to call `.plot.barh()` on your dataframe of the averages. \n",
    "\n",
    "Also try calling `.transpose()` on your averages dataframe, then making a bar graph of that. The transpose function flips the rows and columns and can make it easier to compare the percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar graph\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What do you see from the bar graphs you created? \n",
    "\n",
    "Why would this be a good approach to answering the question of how talk about morality changed between these two periods? What are some limitations of this approach (Hint: look at the values on the graphs you calculated, and remember: these are *percentages*, not proportions)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Section 4: Non-negative matrix factorization<a id='section 4'></a>\n",
    "\n",
    "In this section, you can get an idea of sentiment using topic modeling algorithms, something you touched on in the 4/10 lab earlier this week, to help look for patterns.\n",
    "\n",
    "On Tuesday, you explored Latent Dirichlet Allocation (LDA) in gensim to look for topics in a corpus. Non-negative matrix factorization (NMF), not included in gensim, is another such way to look for topics in unstructured text data. The two methods differ in what kinds of math they use 'under the hood': LDA relies on probabilistic graphical modeling, while NMF uses linear algebra. \n",
    "\n",
    "We want to generate the topics found for 1822 and 1832 trials, look for topics related to tone or morality, and see if there's a difference between the two.\n",
    "\n",
    "Run the cell below to make two lists: one list of the trial transcripts for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial transcripts for 1822\n",
    "transcripts_1822 = old_bailey[old_bailey['year'] == 1822]['transcript']\n",
    "\n",
    "# trial transcripts for 1832\n",
    "transcripts_1832 = old_bailey[old_bailey['year'] == 1832]['transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at 1822. The following cell creates the tfidf vectorizer, fits the text data, and assigns the list of feature name (i.e. the words in the document) to `tfidf_feature_names_1822`.\n",
    "\n",
    "Check out the [documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) if you need a refresher on what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vectorizer\n",
    "tfidf_vectorizer_1822 = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "# fit the data\n",
    "tfidf_1822 = tfidf_vectorizer_1822.fit_transform(transcripts_1822)\n",
    "# get the feature names\n",
    "tfidf_feature_names_1822 = tfidf_vectorizer_1822.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Create the TfidfVectorizer, fit_transform the data, and get the feature names for 1832."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vectorizer\n",
    "tfidf_vectorizer_1832 = ...\n",
    "# fit the data\n",
    "tfidf_1832 = ...\n",
    "# get the feature names\n",
    "tfidf_feature_names_1832 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously the algorithms are not able to automatically determine the number of topics and this value must be set when running the algorithm. Initialising NMF with ‘nndsvd’ rather than random initialisation improves the time it takes for NMF to converge.`random_state` gives the seed for the random number generator to use: this lets us reproduce our results in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "# Run NMF for 1822\n",
    "nmf_1822 = NMF(n_components=num_topics, random_state=1, init='nndsvd').fit(tfidf_1822)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Run NMF using `num_topics` for the number of components on the data from 1832."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF for 1832\n",
    "nmf_1832 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided you the function to display the topics shown by the NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    \"\"\"Displays NUM_TOP_WORDS topics for MODEL \"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "# the number of words to display per topic\n",
    "num_top_words = 10\n",
    "\n",
    "# display the topics for 1822\n",
    "display_topics(nmf_1822, tfidf_feature_names_1822, num_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the topics for 1832\n",
    "display_topics(nmf_1832, tfidf_feature_names_1832, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in LDA, it often takes some hyperparameter tuning before you get a coherent set of topics. Go back and tune the parameters for tfidf and NMF to see if you can get topics that show something about the moral sentiment of the transcripts. `num_topics` usually makes the most dramatic difference.\n",
    "\n",
    "Once you've compared several sets of topics, answer the next question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What did the best set of topics you found tell you about the tone of the documents for the two periods? Why might this approach be a good way to study sentiment? What are some issues with this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Fantastic! Now you know how to approach sentiment analysis several ways using general sentiment analysis with `VADER`, Moral Foundations Theory, and Non-negative matrix factorization.\n",
    "\n",
    "Ultimately, there were limitations to all of these methods. In your homework, you'll explore how to generate your own dictionary to try to overcome some of these limitations.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "\n",
    " - Moral Foundations Theory background and dictionary: *Jesse Graham and Jonathan Haidt http://moralfoundations.org/*\n",
    " - Moral Foundations analysis code adapted from *https://github.com/ds-modules/XRHETOR-R1A/blob/master/02-Moral-Foundations-Analysis/02-Moral-Foundations-Analysis.ipynb*\n",
    " - NMF code and explanation adapted from *Aneesha Bakharia. 'Topic Modeling With Scikit-Learn.' https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730*\n",
    "\n",
    "----\n",
    "Notebook developed by: Gibson Chu, Keeley Takimoto\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
