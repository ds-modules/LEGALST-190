{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEGALST-190 Lab 3/6\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, students will learn about dominant language models in natural language processing and the basics of how to implement it in Python. We'll be using the data you extracted from the last lab (un-debates-2001-clean.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Here we will discuss one widely used representation of text:\n",
    "- <b>Bag-of-Words Encoding</b>: encodes text by the frequency of each word\n",
    "\n",
    "This model was very popular in early text analysis, and continues to be used today. In fact, the models that have replaced it are still very difficult to actually interpret, giving the BoW approach a slight advantage if we want to understand why the model makes certain decisions. Once we have our BoW model we can analyze it in a high-dimensional vector space, which gives us more insights into the similarities and clustering of different texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>session</th> <th>year</th> <th>country</th> <th>text</th> <th>tokens</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>56     </td> <td>2001</td> <td>COM    </td> <td>﻿On\n",
       "behalf of the Comorian delegation, which I have the\n",
       " ...</td> <td>﻿on behalf comorian deleg i honour lead behalf i offer s ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>56     </td> <td>2001</td> <td>RWA    </td> <td>﻿It is a\n",
       "great honour for me, on behalf of the Rwandan\n",
       "d ...</td> <td>﻿it great honour behalf rwandan deleg join previous spea ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>56     </td> <td>2001</td> <td>MMR    </td> <td>﻿On behalf of the\n",
       "delegation of the Union of Myanmar, I  ...</td> <td>﻿on behalf deleg union myanmar i wish extend warmest con ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>56     </td> <td>2001</td> <td>PHL    </td> <td>﻿Let me begin by\n",
       "congratulating Your Excellency, Mr. Han ...</td> <td>﻿let begin congratul your excel mr han seungsoo elect pr ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "        <tr>\n",
       "            <td>56     </td> <td>2001</td> <td>MRT    </td> <td>﻿I\n",
       "am delighted to be able to congratulate you, Sir, on\n",
       " ...</td> <td>﻿i delight abl congratul sir behalf deleg islam republ m ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (184 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## retrieve our data\n",
    "data = Table.read_table('data/un-debates-2001-clean.csv', index_col=0)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's store our text and our tokens into a list\n",
    "text_list = data['text']\n",
    "tokens_list = data['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Encoding\n",
    "\n",
    "The bag-of-words encoding is widely used and a standard representation for text in many of the popular text clustering algorithms.\n",
    "\n",
    "__Key Things to Note:__\n",
    "\n",
    "1. __Stop words are removed.__ Stop-words are words like is and about that in isolation contain very little information about the meaning of the sentence. \n",
    "2. __Word order information is lost.__ \n",
    "3. __Capitalization and punctuation__ are typically removed.\n",
    "4. __Sparse Encoding:__ is necessary to represent the bag-of-words efficiently. There are millions of possible words (including terminology, names, and misspellings) and so instantiating a 0 for every word that is not in each record would be incredibly inefficient.\n",
    "\n",
    "Why is it called a __bag-of-words__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION:__ A bag is another term for a __multiset__: _an unordered collection which may contain multiple instances of each element._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Bag-of-words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Tokens\n",
    "\n",
    "If you remember from the last lab, we created tokens and added it to our table. Normally at this point of the stage, you would create tokens for yourself to use, so let's introduce a new term called `Counter`.\n",
    "\n",
    "The easiest way to count tokens is using the `Counter` object from `collections`. This will give you back a dictionary with the token counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'11': 2,\n",
       "         '1999': 1,\n",
       "         '20': 1,\n",
       "         '2000': 1,\n",
       "         '2001': 2,\n",
       "         '2010': 1,\n",
       "         '21': 1,\n",
       "         '22': 1,\n",
       "         '30': 1,\n",
       "         'a': 1,\n",
       "         'abid': 1,\n",
       "         'abov': 1,\n",
       "         'accept': 1,\n",
       "         'access': 1,\n",
       "         'accid': 1,\n",
       "         'accompani': 1,\n",
       "         'accord': 1,\n",
       "         'account': 1,\n",
       "         'achiev': 1,\n",
       "         'act': 3,\n",
       "         'action': 2,\n",
       "         'activ': 1,\n",
       "         'addit': 2,\n",
       "         'administ': 1,\n",
       "         'admit': 1,\n",
       "         'adopt': 1,\n",
       "         'advoc': 1,\n",
       "         'aeroplan': 1,\n",
       "         'affect': 4,\n",
       "         'afflict': 2,\n",
       "         'africa': 1,\n",
       "         'african': 2,\n",
       "         'aid': 4,\n",
       "         'all': 1,\n",
       "         'allow': 3,\n",
       "         'alongsid': 1,\n",
       "         'alqud': 1,\n",
       "         'alsharif': 1,\n",
       "         'also': 5,\n",
       "         'american': 2,\n",
       "         'among': 3,\n",
       "         'anarchi': 1,\n",
       "         'anjouan': 1,\n",
       "         'annan': 1,\n",
       "         'announc': 1,\n",
       "         'anoth': 1,\n",
       "         'appeal': 1,\n",
       "         'approach': 2,\n",
       "         'appropri': 3,\n",
       "         'april': 1,\n",
       "         'arab': 3,\n",
       "         'archipelago': 1,\n",
       "         'ardent': 1,\n",
       "         'area': 1,\n",
       "         'as': 2,\n",
       "         'aspect': 1,\n",
       "         'assembl': 2,\n",
       "         'assist': 2,\n",
       "         'assoumani': 1,\n",
       "         'assum': 2,\n",
       "         'attack': 2,\n",
       "         'attent': 2,\n",
       "         'author': 4,\n",
       "         'awar': 1,\n",
       "         'award': 1,\n",
       "         'azali': 1,\n",
       "         'back': 1,\n",
       "         'barbar': 1,\n",
       "         'basi': 1,\n",
       "         'becom': 1,\n",
       "         'behalf': 3,\n",
       "         'believ': 2,\n",
       "         'benefit': 1,\n",
       "         'bereav': 1,\n",
       "         'best': 1,\n",
       "         'better': 3,\n",
       "         'beyond': 1,\n",
       "         'bleak': 1,\n",
       "         'blow': 1,\n",
       "         'boléro': 1,\n",
       "         'born': 1,\n",
       "         'bridg': 1,\n",
       "         'broad': 1,\n",
       "         'brother': 1,\n",
       "         'brought': 1,\n",
       "         'brussel': 2,\n",
       "         'call': 2,\n",
       "         'capit': 2,\n",
       "         'care': 1,\n",
       "         'caus': 2,\n",
       "         'central': 1,\n",
       "         'certain': 3,\n",
       "         'challeng': 3,\n",
       "         'chang': 2,\n",
       "         'charter': 1,\n",
       "         'chines': 2,\n",
       "         'cite': 1,\n",
       "         'citi': 2,\n",
       "         'claim': 3,\n",
       "         'clear': 2,\n",
       "         'colonel': 1,\n",
       "         'combat': 2,\n",
       "         'commit': 4,\n",
       "         'communiti': 6,\n",
       "         'comoran': 4,\n",
       "         'comorian': 2,\n",
       "         'comoro': 17,\n",
       "         'compass': 1,\n",
       "         'complet': 1,\n",
       "         'compon': 1,\n",
       "         'concern': 5,\n",
       "         'conclus': 1,\n",
       "         'concret': 1,\n",
       "         'condemn': 2,\n",
       "         'condit': 2,\n",
       "         'condol': 2,\n",
       "         'confer': 3,\n",
       "         'confid': 2,\n",
       "         'confront': 1,\n",
       "         'confus': 1,\n",
       "         'congratul': 1,\n",
       "         'consid': 1,\n",
       "         'consider': 2,\n",
       "         'constitut': 3,\n",
       "         'contin': 3,\n",
       "         'continent': 1,\n",
       "         'continu': 2,\n",
       "         'contribut': 1,\n",
       "         'convent': 1,\n",
       "         'convey': 1,\n",
       "         'cooper': 1,\n",
       "         'cosmopolitan': 1,\n",
       "         'could': 3,\n",
       "         'count': 1,\n",
       "         'counterpart': 1,\n",
       "         'countri': 19,\n",
       "         'cours': 2,\n",
       "         'creat': 1,\n",
       "         'crise': 2,\n",
       "         'crisi': 2,\n",
       "         'current': 2,\n",
       "         'cut': 1,\n",
       "         'daili': 1,\n",
       "         'dakar': 1,\n",
       "         'danger': 2,\n",
       "         'death': 1,\n",
       "         'decad': 1,\n",
       "         'decent': 1,\n",
       "         'dedic': 1,\n",
       "         'deep': 1,\n",
       "         'deepli': 1,\n",
       "         'defi': 1,\n",
       "         'defin': 1,\n",
       "         'definit': 1,\n",
       "         'deleg': 3,\n",
       "         'demonstr': 1,\n",
       "         'depend': 1,\n",
       "         'desir': 1,\n",
       "         'despit': 1,\n",
       "         'destabil': 1,\n",
       "         'deterior': 1,\n",
       "         'develop': 5,\n",
       "         'devot': 1,\n",
       "         'diagnosi': 1,\n",
       "         'dialogu': 1,\n",
       "         'differ': 1,\n",
       "         'difficult': 3,\n",
       "         'difficulti': 1,\n",
       "         'direct': 1,\n",
       "         'discuss': 1,\n",
       "         'diseas': 6,\n",
       "         'distinguish': 1,\n",
       "         'document': 1,\n",
       "         'doha': 2,\n",
       "         'dominican': 1,\n",
       "         'draft': 2,\n",
       "         'due': 1,\n",
       "         'duti': 1,\n",
       "         'earlier': 1,\n",
       "         'earth': 1,\n",
       "         'econom': 1,\n",
       "         'economi': 3,\n",
       "         'educ': 1,\n",
       "         'effect': 3,\n",
       "         'effort': 4,\n",
       "         'elect': 2,\n",
       "         'eloqu': 1,\n",
       "         'empti': 1,\n",
       "         'end': 1,\n",
       "         'engag': 1,\n",
       "         'enlighten': 1,\n",
       "         'ensur': 2,\n",
       "         'entail': 1,\n",
       "         'entir': 6,\n",
       "         'entiti': 2,\n",
       "         'environ': 1,\n",
       "         'equip': 1,\n",
       "         'especi': 1,\n",
       "         'essenti': 1,\n",
       "         'establish': 4,\n",
       "         'europ': 1,\n",
       "         'european': 1,\n",
       "         'even': 1,\n",
       "         'event': 1,\n",
       "         'ever': 1,\n",
       "         'everi': 5,\n",
       "         'everyon': 2,\n",
       "         'evil': 1,\n",
       "         'evolv': 1,\n",
       "         'excel': 3,\n",
       "         'exemplari': 1,\n",
       "         'exercis': 1,\n",
       "         'expect': 1,\n",
       "         'experi': 1,\n",
       "         'expertis': 1,\n",
       "         'express': 4,\n",
       "         'extend': 2,\n",
       "         'extrem': 2,\n",
       "         'extremist': 1,\n",
       "         'eye': 1,\n",
       "         'factor': 1,\n",
       "         'fail': 1,\n",
       "         'famili': 3,\n",
       "         'famin': 2,\n",
       "         'feder': 5,\n",
       "         'fellow': 1,\n",
       "         'fight': 3,\n",
       "         'final': 1,\n",
       "         'financ': 1,\n",
       "         'find': 2,\n",
       "         'firm': 1,\n",
       "         'first': 1,\n",
       "         'focus': 1,\n",
       "         'fold': 1,\n",
       "         'follow': 2,\n",
       "         'followup': 1,\n",
       "         'forc': 1,\n",
       "         'form': 1,\n",
       "         'fortun': 1,\n",
       "         'framework': 4,\n",
       "         'franc': 1,\n",
       "         'francophoni': 1,\n",
       "         'fratern': 1,\n",
       "         'free': 1,\n",
       "         'french': 1,\n",
       "         'friend': 1,\n",
       "         'from': 3,\n",
       "         'fulli': 2,\n",
       "         'fundament': 2,\n",
       "         'furthermor': 1,\n",
       "         'futur': 2,\n",
       "         'gain': 1,\n",
       "         'gap': 2,\n",
       "         'general': 3,\n",
       "         'generat': 1,\n",
       "         'geograph': 1,\n",
       "         'give': 1,\n",
       "         'given': 2,\n",
       "         'global': 1,\n",
       "         'gloom': 1,\n",
       "         'good': 1,\n",
       "         'govern': 16,\n",
       "         'great': 4,\n",
       "         'greater': 1,\n",
       "         'griefstricken': 1,\n",
       "         'group': 2,\n",
       "         'guarante': 2,\n",
       "         'hamada': 1,\n",
       "         'hand': 2,\n",
       "         'happi': 1,\n",
       "         'harmoni': 1,\n",
       "         'harri': 1,\n",
       "         'harsh': 1,\n",
       "         'hate': 1,\n",
       "         'head': 2,\n",
       "         'health': 2,\n",
       "         'heart': 1,\n",
       "         'held': 4,\n",
       "         'help': 2,\n",
       "         'high': 2,\n",
       "         'highest': 1,\n",
       "         'his': 2,\n",
       "         'hivaid': 1,\n",
       "         'holidaymak': 1,\n",
       "         'holkeri': 1,\n",
       "         'honour': 1,\n",
       "         'hope': 2,\n",
       "         'hospit': 1,\n",
       "         'howev': 1,\n",
       "         'human': 5,\n",
       "         'hunger': 1,\n",
       "         'i': 13,\n",
       "         'ignor': 1,\n",
       "         'import': 1,\n",
       "         'improv': 1,\n",
       "         'in': 8,\n",
       "         'incid': 1,\n",
       "         'includ': 1,\n",
       "         'inde': 2,\n",
       "         'independ': 1,\n",
       "         'individu': 1,\n",
       "         'indubit': 1,\n",
       "         'influx': 1,\n",
       "         'initi': 1,\n",
       "         'injustic': 1,\n",
       "         'innoc': 1,\n",
       "         'institut': 3,\n",
       "         'integr': 1,\n",
       "         'interest': 4,\n",
       "         'intern': 11,\n",
       "         'islam': 7,\n",
       "         'island': 6,\n",
       "         'januari': 1,\n",
       "         'join': 1,\n",
       "         'joint': 1,\n",
       "         'june': 1,\n",
       "         'kill': 1,\n",
       "         'know': 1,\n",
       "         'known': 1,\n",
       "         'kofi': 1,\n",
       "         'la': 1,\n",
       "         'last': 8,\n",
       "         'launch': 1,\n",
       "         'law': 3,\n",
       "         'lay': 1,\n",
       "         'layer': 1,\n",
       "         'lead': 4,\n",
       "         'leadership': 2,\n",
       "         'leagu': 1,\n",
       "         'least': 1,\n",
       "         'led': 4,\n",
       "         'legitim': 2,\n",
       "         'life': 3,\n",
       "         'like': 2,\n",
       "         'likewis': 1,\n",
       "         'limit': 1,\n",
       "         'link': 1,\n",
       "         'live': 1,\n",
       "         'long': 1,\n",
       "         'lose': 1,\n",
       "         'loss': 2,\n",
       "         'love': 1,\n",
       "         'low': 1,\n",
       "         'made': 5,\n",
       "         'madi': 1,\n",
       "         'main': 1,\n",
       "         'major': 3,\n",
       "         'make': 4,\n",
       "         'malaria': 3,\n",
       "         'man': 2,\n",
       "         'mani': 4,\n",
       "         'manifest': 2,\n",
       "         'mankind': 1,\n",
       "         'manner': 1,\n",
       "         'may': 2,\n",
       "         'mayott': 4,\n",
       "         'mean': 3,\n",
       "         'measur': 1,\n",
       "         'mechan': 1,\n",
       "         'meet': 1,\n",
       "         'merit': 1,\n",
       "         'met': 1,\n",
       "         'millennium': 3,\n",
       "         'million': 2,\n",
       "         'mind': 1,\n",
       "         'minist': 2,\n",
       "         'mobil': 1,\n",
       "         'monday': 1,\n",
       "         'month': 1,\n",
       "         'move': 1,\n",
       "         'movement': 1,\n",
       "         'mr': 3,\n",
       "         'much': 2,\n",
       "         'must': 6,\n",
       "         'my': 2,\n",
       "         'nation': 20,\n",
       "         'natur': 2,\n",
       "         'necessari': 3,\n",
       "         'need': 2,\n",
       "         'negoti': 1,\n",
       "         'neighbour': 1,\n",
       "         'network': 1,\n",
       "         'new': 4,\n",
       "         'nobel': 1,\n",
       "         'north': 2,\n",
       "         'notion': 1,\n",
       "         'object': 1,\n",
       "         'occas': 2,\n",
       "         'occupi': 1,\n",
       "         'offer': 3,\n",
       "         'often': 1,\n",
       "         'on': 1,\n",
       "         'one': 6,\n",
       "         'open': 1,\n",
       "         'oppress': 1,\n",
       "         'opt': 1,\n",
       "         'order': 7,\n",
       "         'organ': 10,\n",
       "         'orphan': 1,\n",
       "         'our': 1,\n",
       "         'outstand': 1,\n",
       "         'overlook': 1,\n",
       "         'pain': 3,\n",
       "         'palestinian': 2,\n",
       "         'part': 4,\n",
       "         'parti': 1,\n",
       "         'particip': 1,\n",
       "         'particular': 3,\n",
       "         'partner': 1,\n",
       "         'pay': 2,\n",
       "         'peac': 10,\n",
       "         'peopl': 12,\n",
       "         'period': 2,\n",
       "         'perpetu': 2,\n",
       "         'perspect': 1,\n",
       "         'phase': 1,\n",
       "         'place': 2,\n",
       "         'play': 2,\n",
       "         'pleas': 1,\n",
       "         'plung': 2,\n",
       "         'point': 4,\n",
       "         'polit': 2,\n",
       "         'poor': 1,\n",
       "         'popul': 1,\n",
       "         'posit': 2,\n",
       "         'possibl': 1,\n",
       "         'potenti': 1,\n",
       "         'poverti': 2,\n",
       "         'prais': 1,\n",
       "         'precari': 1,\n",
       "         'precious': 1,\n",
       "         'predecessor': 1,\n",
       "         'prepar': 1,\n",
       "         'present': 2,\n",
       "         'preserv': 1,\n",
       "         'presid': 1,\n",
       "         'pretext': 1,\n",
       "         'prevail': 2,\n",
       "         'preval': 1,\n",
       "         'previous': 2,\n",
       "         'primari': 1,\n",
       "         'prime': 2,\n",
       "         'prioriti': 1,\n",
       "         'prize': 1,\n",
       "         'problem': 6,\n",
       "         'process': 1,\n",
       "         'proclaim': 1,\n",
       "         'profound': 1,\n",
       "         'programm': 3,\n",
       "         'progress': 1,\n",
       "         'prompt': 1,\n",
       "         'proof': 1,\n",
       "         'prosper': 1,\n",
       "         'protect': 1,\n",
       "         'provid': 2,\n",
       "         'provinc': 1,\n",
       "         'provis': 2,\n",
       "         'psychosi': 1,\n",
       "         'qatar': 1,\n",
       "         'rapid': 1,\n",
       "         'rate': 1,\n",
       "         'reach': 2,\n",
       "         'read': 1,\n",
       "         'reaffirm': 1,\n",
       "         'realiti': 2,\n",
       "         'recogn': 2,\n",
       "         'recommend': 3,\n",
       "         'reconcili': 2,\n",
       "         'reconstruct': 2,\n",
       "         'referendum': 1,\n",
       "         'reflect': 1,\n",
       "         'reform': 1,\n",
       "         'regardless': 1,\n",
       "         'region': 8,\n",
       "         'reifi': 1,\n",
       "         'reintegr': 1,\n",
       "         'reiter': 1,\n",
       "         'relat': 1,\n",
       "         'relev': 1,\n",
       "         'religion': 1,\n",
       "         'remain': 3,\n",
       "         'remedi': 1,\n",
       "         'reprehens': 1,\n",
       "         'republ': 6,\n",
       "         'requir': 3,\n",
       "         'resist': 1,\n",
       "         'resolut': 2,\n",
       "         'respect': 8,\n",
       "         'respons': 7,\n",
       "         'rest': 1,\n",
       "         'return': 1,\n",
       "         'revit': 1,\n",
       "         'rich': 1,\n",
       "         'right': 5,\n",
       "         'ring': 1,\n",
       "         'role': 3,\n",
       "         'roll': 1,\n",
       "         'rostrum': 2,\n",
       "         'safe': 2,\n",
       "         'safeguard': 1,\n",
       "         'save': 1,\n",
       "         'scourg': 2,\n",
       "         'search': 1,\n",
       "         'season': 1,\n",
       "         'secessionist': 2,\n",
       "         'secretarygener': 1,\n",
       "         'sector': 1,\n",
       "         'secur': 3,\n",
       "         'see': 2,\n",
       "         'seek': 1,\n",
       "         'seen': 1,\n",
       "         'selfdetermin': 1,\n",
       "         'sensit': 1,\n",
       "         'separ': 1,\n",
       "         'septemb': 2,\n",
       "         'serious': 2,\n",
       "         'servic': 1,\n",
       "         'session': 6,\n",
       "         'set': 1,\n",
       "         'sever': 1,\n",
       "         'shaken': 1,\n",
       "         'shame': 1,\n",
       "         'share': 1,\n",
       "         'sight': 1,\n",
       "         'sign': 1,\n",
       "         'signific': 1,\n",
       "         'sinc': 2,\n",
       "         'sincer': 2,\n",
       "         'sir': 1,\n",
       "         'situat': 5,\n",
       "         'social': 2,\n",
       "         'societi': 1,\n",
       "         'solid': 1,\n",
       "         'solidar': 1,\n",
       "         'solut': 4,\n",
       "         'some': 1,\n",
       "         'sort': 1,\n",
       "         'sourc': 1,\n",
       "         'south': 2,\n",
       "         'spare': 1,\n",
       "         'speak': 2,\n",
       "         'special': 1,\n",
       "         'spoke': 2,\n",
       "         'spread': 2,\n",
       "         'stabil': 2,\n",
       "         'state': 6,\n",
       "         'statement': 1,\n",
       "         'still': 3,\n",
       "         'strategi': 1,\n",
       "         'strengthen': 1,\n",
       "         'strong': 2,\n",
       "         'struggl': 2,\n",
       "         'submit': 1,\n",
       "         'subregion': 1,\n",
       "         'subsist': 1,\n",
       "         'succeed': 1,\n",
       "         'success': 1,\n",
       "         'sudden': 1,\n",
       "         'summit': 2,\n",
       "         'support': 2,\n",
       "         'suppress': 1,\n",
       "         'surviv': 1,\n",
       "         'sustain': 1,\n",
       "         'swift': 1,\n",
       "         'sympathi': 1,\n",
       "         'taiwan': 1,\n",
       "         'take': 3,\n",
       "         'talent': 1,\n",
       "         'task': 1,\n",
       "         'technic': 1,\n",
       "         'tension': 1,\n",
       "         'terribl': 1,\n",
       "         'territori': 2,\n",
       "         'terror': 9,\n",
       "         'terrorist': 3,\n",
       "         'thank': 1,\n",
       "         'that': 3,\n",
       "         'the': 11,\n",
       "         'therefor': 1,\n",
       "         'these': 1,\n",
       "         'third': 1,\n",
       "         'this': 7,\n",
       "         'threat': 2,\n",
       "         'threaten': 2,\n",
       "         'throughout': 2,\n",
       "         'thus': 6,\n",
       "         'time': 1,\n",
       "         'today': 6,\n",
       "         'togeth': 3,\n",
       "         'toler': 1,\n",
       "         'took': 3,\n",
       "         'tool': 1,\n",
       "         'tourism': 1,\n",
       "         'tourist': 1,\n",
       "         'toward': 1,\n",
       "         'tragic': 1,\n",
       "         'transit': 1,\n",
       "         'tribut': 2,\n",
       "         'true': 1,\n",
       "         'turn': 1,\n",
       "         'type': 1,\n",
       "         'ultim': 1,\n",
       "         'unaid': 1,\n",
       "         'undeni': 1,\n",
       "         'undermin': 1,\n",
       "         'undp': 1,\n",
       "         'unfortun': 2,\n",
       "         'union': 1,\n",
       "         'unit': 10,\n",
       "         'uniti': 2,\n",
       "         'upcom': 1,\n",
       "         'urg': 1,\n",
       "         'urgent': 2,\n",
       "         'us': 5,\n",
       "         'use': 1,\n",
       "         'valu': 1,\n",
       "         'valuabl': 1,\n",
       "         'vari': 1,\n",
       "         'victim': 2,\n",
       "         'view': 2,\n",
       "         'violat': 1,\n",
       "         'violenc': 1,\n",
       "         'visàvi': 1,\n",
       "         'want': 1,\n",
       "         'war': 1,\n",
       "         'warmest': 1,\n",
       "         'way': 5,\n",
       "         'we': 6,\n",
       "         'welcom': 1,\n",
       "         'welfar': 1,\n",
       "         'while': 1,\n",
       "         'whole': 1,\n",
       "         'whose': 2,\n",
       "         'willing': 1,\n",
       "         'wisdom': 1,\n",
       "         'wish': 1,\n",
       "         'with': 1,\n",
       "         'within': 2,\n",
       "         'without': 1,\n",
       "         'work': 2,\n",
       "         'world': 13,\n",
       "         'would': 3,\n",
       "         'year': 3,\n",
       "         'york': 2,\n",
       "         '—': 1,\n",
       "         '’': 6,\n",
       "         '“': 1,\n",
       "         '”': 1,\n",
       "         '\\ufeffon': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "first_speech = tokens_list[0].split()\n",
    "counter = Counter(first_speech) \n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `most_common()` method can be called on a Counter to return the most common tokens and their counts. What are the most common tokens in the first speech? What do these common words tell you about the content or tone of the speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nation', 20),\n",
       " ('countri', 19),\n",
       " ('comoro', 17),\n",
       " ('govern', 16),\n",
       " ('i', 13),\n",
       " ('world', 13),\n",
       " ('peopl', 12),\n",
       " ('the', 11),\n",
       " ('intern', 11),\n",
       " ('organ', 10),\n",
       " ('peac', 10),\n",
       " ('unit', 10),\n",
       " ('terror', 9),\n",
       " ('last', 8),\n",
       " ('in', 8),\n",
       " ('respect', 8),\n",
       " ('region', 8),\n",
       " ('respons', 7),\n",
       " ('order', 7),\n",
       " ('this', 7),\n",
       " ('islam', 7),\n",
       " ('session', 6),\n",
       " ('we', 6),\n",
       " ('entir', 6),\n",
       " ('communiti', 6),\n",
       " ('thus', 6),\n",
       " ('state', 6),\n",
       " ('republ', 6),\n",
       " ('must', 6),\n",
       " ('today', 6),\n",
       " ('’', 6),\n",
       " ('one', 6),\n",
       " ('problem', 6),\n",
       " ('diseas', 6),\n",
       " ('island', 6),\n",
       " ('also', 5),\n",
       " ('us', 5),\n",
       " ('feder', 5),\n",
       " ('way', 5),\n",
       " ('concern', 5),\n",
       " ('right', 5),\n",
       " ('everi', 5),\n",
       " ('situat', 5),\n",
       " ('human', 5),\n",
       " ('develop', 5),\n",
       " ('made', 5),\n",
       " ('lead', 4),\n",
       " ('express', 4),\n",
       " ('led', 4),\n",
       " ('great', 4),\n",
       " ('interest', 4),\n",
       " ('new', 4),\n",
       " ('mani', 4),\n",
       " ('make', 4),\n",
       " ('point', 4),\n",
       " ('framework', 4),\n",
       " ('effort', 4),\n",
       " ('held', 4),\n",
       " ('commit', 4),\n",
       " ('establish', 4),\n",
       " ('author', 4),\n",
       " ('part', 4),\n",
       " ('solut', 4),\n",
       " ('affect', 4),\n",
       " ('aid', 4),\n",
       " ('comoran', 4),\n",
       " ('mayott', 4),\n",
       " ('behalf', 3),\n",
       " ('deleg', 3),\n",
       " ('offer', 3),\n",
       " ('general', 3),\n",
       " ('mr', 3),\n",
       " ('excel', 3),\n",
       " ('togeth', 3),\n",
       " ('terrorist', 3),\n",
       " ('pain', 3),\n",
       " ('particular', 3),\n",
       " ('famili', 3),\n",
       " ('took', 3),\n",
       " ('millennium', 3),\n",
       " ('role', 3),\n",
       " ('secur', 3),\n",
       " ('requir', 3),\n",
       " ('better', 3),\n",
       " ('effect', 3),\n",
       " ('appropri', 3),\n",
       " ('institut', 3),\n",
       " ('difficult', 3),\n",
       " ('take', 3),\n",
       " ('contin', 3),\n",
       " ('allow', 3),\n",
       " ('claim', 3),\n",
       " ('challeng', 3),\n",
       " ('act', 3),\n",
       " ('confer', 3),\n",
       " ('arab', 3),\n",
       " ('necessari', 3),\n",
       " ('constitut', 3),\n",
       " ('certain', 3),\n",
       " ('mean', 3),\n",
       " ('fight', 3),\n",
       " ('economi', 3),\n",
       " ('from', 3),\n",
       " ('among', 3),\n",
       " ('life', 3),\n",
       " ('that', 3),\n",
       " ('law', 3),\n",
       " ('remain', 3),\n",
       " ('major', 3),\n",
       " ('year', 3),\n",
       " ('could', 3),\n",
       " ('programm', 3),\n",
       " ('recommend', 3),\n",
       " ('malaria', 3),\n",
       " ('still', 3),\n",
       " ('would', 3),\n",
       " ('comorian', 2),\n",
       " ('elect', 2),\n",
       " ('assembl', 2),\n",
       " ('hope', 2),\n",
       " ('work', 2),\n",
       " ('leadership', 2),\n",
       " ('my', 2),\n",
       " ('pay', 2),\n",
       " ('tribut', 2),\n",
       " ('previous', 2),\n",
       " ('as', 2),\n",
       " ('man', 2),\n",
       " ('11', 2),\n",
       " ('septemb', 2),\n",
       " ('plung', 2),\n",
       " ('attack', 2),\n",
       " ('american', 2),\n",
       " ('york', 2),\n",
       " ('citi', 2),\n",
       " ('capit', 2),\n",
       " ('rostrum', 2),\n",
       " ('follow', 2),\n",
       " ('unfortun', 2),\n",
       " ('condol', 2),\n",
       " ('victim', 2),\n",
       " ('whose', 2),\n",
       " ('loss', 2),\n",
       " ('place', 2),\n",
       " ('extend', 2),\n",
       " ('sincer', 2),\n",
       " ('cours', 2),\n",
       " ('summit', 2),\n",
       " ('continu', 2),\n",
       " ('play', 2),\n",
       " ('realiti', 2),\n",
       " ('fulli', 2),\n",
       " ('assum', 2),\n",
       " ('scourg', 2),\n",
       " ('ensur', 2),\n",
       " ('crise', 2),\n",
       " ('caus', 2),\n",
       " ('high', 2),\n",
       " ('threat', 2),\n",
       " ('spread', 2),\n",
       " ('throughout', 2),\n",
       " ('call', 2),\n",
       " ('attent', 2),\n",
       " ('urgent', 2),\n",
       " ('action', 2),\n",
       " ('safe', 2),\n",
       " ('danger', 2),\n",
       " ('group', 2),\n",
       " ('perpetu', 2),\n",
       " ('million', 2),\n",
       " ('strong', 2),\n",
       " ('condemn', 2),\n",
       " ('combat', 2),\n",
       " ('doha', 2),\n",
       " ('african', 2),\n",
       " ('struggl', 2),\n",
       " ('brussel', 2),\n",
       " ('manifest', 2),\n",
       " ('approach', 2),\n",
       " ('clear', 2),\n",
       " ('legitim', 2),\n",
       " ('sinc', 2),\n",
       " ('hand', 2),\n",
       " ('extrem', 2),\n",
       " ('given', 2),\n",
       " ('prime', 2),\n",
       " ('minist', 2),\n",
       " ('addit', 2),\n",
       " ('need', 2),\n",
       " ('threaten', 2),\n",
       " ('polit', 2),\n",
       " ('prevail', 2),\n",
       " ('territori', 2),\n",
       " ('palestinian', 2),\n",
       " ('reach', 2),\n",
       " ('believ', 2),\n",
       " ('view', 2),\n",
       " ('guarante', 2),\n",
       " ('condit', 2),\n",
       " ('fundament', 2),\n",
       " ('inde', 2),\n",
       " ('serious', 2),\n",
       " ('health', 2),\n",
       " ('famin', 2),\n",
       " ('afflict', 2),\n",
       " ('poverti', 2),\n",
       " ('gap', 2),\n",
       " ('north', 2),\n",
       " ('south', 2),\n",
       " ('futur', 2),\n",
       " ('posit', 2),\n",
       " ('much', 2),\n",
       " ('help', 2),\n",
       " ('his', 2),\n",
       " ('speak', 2),\n",
       " ('provis', 2),\n",
       " ('within', 2),\n",
       " ('period', 2),\n",
       " ('2001', 2),\n",
       " ('spoke', 2),\n",
       " ('may', 2),\n",
       " ('resolut', 2),\n",
       " ('occas', 2),\n",
       " ('current', 2),\n",
       " ('chang', 2),\n",
       " ('recogn', 2),\n",
       " ('confid', 2),\n",
       " ('uniti', 2),\n",
       " ('chines', 2),\n",
       " ('natur', 2),\n",
       " ('entiti', 2),\n",
       " ('secessionist', 2),\n",
       " ('find', 2),\n",
       " ('stabil', 2),\n",
       " ('everyon', 2),\n",
       " ('like', 2),\n",
       " ('reconcili', 2),\n",
       " ('head', 2),\n",
       " ('present', 2),\n",
       " ('draft', 2),\n",
       " ('assist', 2),\n",
       " ('provid', 2),\n",
       " ('consider', 2),\n",
       " ('support', 2),\n",
       " ('social', 2),\n",
       " ('reconstruct', 2),\n",
       " ('crisi', 2),\n",
       " ('see', 2),\n",
       " ('\\ufeffon', 1),\n",
       " ('honour', 1),\n",
       " ('sir', 1),\n",
       " ('warmest', 1),\n",
       " ('congratul', 1),\n",
       " ('presid', 1),\n",
       " ('ardent', 1),\n",
       " ('enlighten', 1),\n",
       " ('success', 1),\n",
       " ('ring', 1),\n",
       " ('predecessor', 1),\n",
       " ('harri', 1),\n",
       " ('holkeri', 1),\n",
       " ('manner', 1),\n",
       " ('secretarygener', 1),\n",
       " ('kofi', 1),\n",
       " ('annan', 1),\n",
       " ('prais', 1),\n",
       " ('merit', 1),\n",
       " ('talent', 1),\n",
       " ('exemplari', 1),\n",
       " ('wisdom', 1),\n",
       " ('dedic', 1),\n",
       " ('servic', 1),\n",
       " ('nobel', 1),\n",
       " ('prize', 1),\n",
       " ('award', 1),\n",
       " ('concret', 1),\n",
       " ('proof', 1),\n",
       " ('outstand', 1),\n",
       " ('valu', 1),\n",
       " ('on', 1),\n",
       " ('gloom', 1),\n",
       " ('anarchi', 1),\n",
       " ('network', 1),\n",
       " ('defi', 1),\n",
       " ('reprehens', 1),\n",
       " ('global', 1),\n",
       " ('hospit', 1),\n",
       " ('cosmopolitan', 1),\n",
       " ('—', 1),\n",
       " ('fail', 1),\n",
       " ('duti', 1),\n",
       " ('convey', 1),\n",
       " ('deep', 1),\n",
       " ('sympathi', 1),\n",
       " ('compass', 1),\n",
       " ('tragic', 1),\n",
       " ('event', 1),\n",
       " ('griefstricken', 1),\n",
       " ('terribl', 1),\n",
       " ('share', 1),\n",
       " ('sudden', 1),\n",
       " ('death', 1),\n",
       " ('furthermor', 1),\n",
       " ('deepli', 1),\n",
       " ('move', 1),\n",
       " ('live', 1),\n",
       " ('aeroplan', 1),\n",
       " ('accid', 1),\n",
       " ('monday', 1),\n",
       " ('dominican', 1),\n",
       " ('bereav', 1),\n",
       " ('central', 1),\n",
       " ('broad', 1),\n",
       " ('reaffirm', 1),\n",
       " ('statement', 1),\n",
       " ('reform', 1),\n",
       " ('main', 1),\n",
       " ('reflect', 1),\n",
       " ('seen', 1),\n",
       " ('prioriti', 1),\n",
       " ('ever', 1),\n",
       " ('strengthen', 1),\n",
       " ('equip', 1),\n",
       " ('differ', 1),\n",
       " ('abov', 1),\n",
       " ('“', 1),\n",
       " ('save', 1),\n",
       " ('succeed', 1),\n",
       " ('generat', 1),\n",
       " ('war', 1),\n",
       " ('”', 1),\n",
       " ('cite', 1),\n",
       " ('charter', 1),\n",
       " ('primari', 1),\n",
       " ('sort', 1),\n",
       " ('precari', 1),\n",
       " ('demonstr', 1),\n",
       " ('task', 1),\n",
       " ('vari', 1),\n",
       " ('focus', 1),\n",
       " ('greater', 1),\n",
       " ('compon', 1),\n",
       " ('extremist', 1),\n",
       " ('regardless', 1),\n",
       " ('pretext', 1),\n",
       " ('psychosi', 1),\n",
       " ('innoc', 1),\n",
       " ('harsh', 1),\n",
       " ('blow', 1),\n",
       " ('mankind', 1),\n",
       " ('therefor', 1),\n",
       " ('hate', 1),\n",
       " ('barbar', 1),\n",
       " ('form', 1),\n",
       " ('mobil', 1),\n",
       " ('dakar', 1),\n",
       " ('month', 1),\n",
       " ('activ', 1),\n",
       " ('reifi', 1),\n",
       " ('meet', 1),\n",
       " ('brought', 1),\n",
       " ('europ', 1),\n",
       " ('20', 1),\n",
       " ('abid', 1),\n",
       " ('howev', 1),\n",
       " ('first', 1),\n",
       " ('defin', 1),\n",
       " ('notion', 1),\n",
       " ('individu', 1),\n",
       " ('distinguish', 1),\n",
       " ('selfdetermin', 1),\n",
       " ('indubit', 1),\n",
       " ('exercis', 1),\n",
       " ('without', 1),\n",
       " ('confront', 1),\n",
       " ('often', 1),\n",
       " ('met', 1),\n",
       " ('resist', 1),\n",
       " ('even', 1),\n",
       " ('oppress', 1),\n",
       " ('final', 1),\n",
       " ('essenti', 1),\n",
       " ('confus', 1),\n",
       " ('know', 1),\n",
       " ('link', 1),\n",
       " ('religion', 1),\n",
       " ('known', 1),\n",
       " ('toler', 1),\n",
       " ('open', 1),\n",
       " ('advoc', 1),\n",
       " ('love', 1),\n",
       " ('fellow', 1),\n",
       " ('solidar', 1),\n",
       " ('with', 1),\n",
       " ('highest', 1),\n",
       " ('visàvi', 1),\n",
       " ('mechan', 1),\n",
       " ('januari', 1),\n",
       " ('2000', 1),\n",
       " ('sign', 1),\n",
       " ('convent', 1),\n",
       " ('suppress', 1),\n",
       " ('financ', 1),\n",
       " ('engag', 1),\n",
       " ('join', 1),\n",
       " ('continent', 1),\n",
       " ('forc', 1),\n",
       " ('destabil', 1),\n",
       " ('object', 1),\n",
       " ('read', 1),\n",
       " ('area', 1),\n",
       " ('tension', 1),\n",
       " ('perspect', 1),\n",
       " ('occupi', 1),\n",
       " ('earth', 1),\n",
       " ('happi', 1),\n",
       " ('prosper', 1),\n",
       " ('definit', 1),\n",
       " ('independ', 1),\n",
       " ('alqud', 1),\n",
       " ('alsharif', 1),\n",
       " ('anoth', 1),\n",
       " ('safeguard', 1),\n",
       " ('welfar', 1),\n",
       " ('entail', 1),\n",
       " ('protect', 1),\n",
       " ('access', 1),\n",
       " ('educ', 1),\n",
       " ('care', 1),\n",
       " ('sever', 1),\n",
       " ('deterior', 1),\n",
       " ('environ', 1),\n",
       " ('violat', 1),\n",
       " ('some', 1),\n",
       " ('evil', 1),\n",
       " ('eloqu', 1),\n",
       " ('ultim', 1),\n",
       " ('bridg', 1),\n",
       " ('heart', 1),\n",
       " ('discuss', 1),\n",
       " ('kill', 1),\n",
       " ('empti', 1),\n",
       " ('creat', 1),\n",
       " ('orphan', 1),\n",
       " ('bleak', 1),\n",
       " ('despit', 1),\n",
       " ('relat', 1),\n",
       " ('low', 1),\n",
       " ('rate', 1),\n",
       " ('incid', 1),\n",
       " ('awar', 1),\n",
       " ('geograph', 1),\n",
       " ('preval', 1),\n",
       " ('tourism', 1),\n",
       " ('subregion', 1),\n",
       " ('factor', 1),\n",
       " ('contribut', 1),\n",
       " ('rapid', 1),\n",
       " ('influx', 1),\n",
       " ('tourist', 1),\n",
       " ('season', 1),\n",
       " ('holidaymak', 1),\n",
       " ('devot', 1),\n",
       " ('use', 1),\n",
       " ('tool', 1),\n",
       " ('give', 1),\n",
       " ('possibl', 1),\n",
       " ('undeni', 1),\n",
       " ('joint', 1),\n",
       " ('hivaid', 1),\n",
       " ('unaid', 1),\n",
       " ('particip', 1),\n",
       " ('measur', 1),\n",
       " ('hamada', 1),\n",
       " ('madi', 1),\n",
       " ('boléro', 1),\n",
       " ('june', 1),\n",
       " ('special', 1),\n",
       " ('born', 1),\n",
       " ('mind', 1),\n",
       " ('overlook', 1),\n",
       " ('layer', 1),\n",
       " ('societi', 1),\n",
       " ('becom', 1),\n",
       " ('sourc', 1),\n",
       " ('beyond', 1),\n",
       " ('strategi', 1),\n",
       " ('welcom', 1),\n",
       " ('initi', 1),\n",
       " ('proclaim', 1),\n",
       " ('2010', 1),\n",
       " ('decad', 1),\n",
       " ('roll', 1),\n",
       " ('back', 1),\n",
       " ('africa', 1),\n",
       " ('earlier', 1),\n",
       " ('separ', 1),\n",
       " ('third', 1),\n",
       " ('least', 1),\n",
       " ('count', 1),\n",
       " ('21', 1),\n",
       " ('rich', 1),\n",
       " ('poor', 1),\n",
       " ('adopt', 1),\n",
       " ('qatar', 1),\n",
       " ('lay', 1),\n",
       " ('solid', 1),\n",
       " ('basi', 1),\n",
       " ('sustain', 1),\n",
       " ('revit', 1),\n",
       " ('our', 1),\n",
       " ('expect', 1),\n",
       " ('best', 1),\n",
       " ('integr', 1),\n",
       " ('urg', 1),\n",
       " ('consid', 1),\n",
       " ('reintegr', 1),\n",
       " ('provinc', 1),\n",
       " ('taiwan', 1),\n",
       " ('especi', 1),\n",
       " ('sensit', 1),\n",
       " ('experi', 1),\n",
       " ('firm', 1),\n",
       " ('swift', 1),\n",
       " ('type', 1),\n",
       " ('undermin', 1),\n",
       " ('prompt', 1),\n",
       " ('turn', 1),\n",
       " ('import', 1),\n",
       " ('eye', 1),\n",
       " ('followup', 1),\n",
       " ('wish', 1),\n",
       " ('due', 1),\n",
       " ('account', 1),\n",
       " ('true', 1),\n",
       " ('diagnosi', 1),\n",
       " ('seek', 1),\n",
       " ('remedi', 1),\n",
       " ('surviv', 1),\n",
       " ('depend', 1),\n",
       " ('neighbour', 1),\n",
       " ('likewis', 1),\n",
       " ('long', 1),\n",
       " ('hunger', 1),\n",
       " ('ignor', 1),\n",
       " ('violenc', 1),\n",
       " ('injustic', 1),\n",
       " ('subsist', 1),\n",
       " ('shame', 1),\n",
       " ('potenti', 1),\n",
       " ('a', 1),\n",
       " ('aspect', 1),\n",
       " ('evolv', 1),\n",
       " ('pleas', 1),\n",
       " ('announc', 1),\n",
       " ('process', 1),\n",
       " ('colonel', 1),\n",
       " ('azali', 1),\n",
       " ('assoumani', 1),\n",
       " ('opt', 1),\n",
       " ('direct', 1),\n",
       " ('dialogu', 1),\n",
       " ('brother', 1),\n",
       " ('anjouan', 1),\n",
       " ('progress', 1),\n",
       " ('daili', 1),\n",
       " ('time', 1),\n",
       " ('whole', 1),\n",
       " ('submit', 1),\n",
       " ('referendum', 1),\n",
       " ('end', 1),\n",
       " ('all', 1),\n",
       " ('parti', 1),\n",
       " ('document', 1),\n",
       " ('benefit', 1),\n",
       " ('includ', 1),\n",
       " ('la', 1),\n",
       " ('francophoni', 1),\n",
       " ('leagu', 1),\n",
       " ('expertis', 1),\n",
       " ('alongsid', 1),\n",
       " ('prepar', 1),\n",
       " ('upcom', 1),\n",
       " ('undp', 1),\n",
       " ('european', 1),\n",
       " ('union', 1),\n",
       " ('technic', 1),\n",
       " ('transit', 1),\n",
       " ('phase', 1),\n",
       " ('toward', 1),\n",
       " ('spare', 1),\n",
       " ('improv', 1),\n",
       " ('popul', 1),\n",
       " ('decent', 1),\n",
       " ('accept', 1),\n",
       " ('these', 1),\n",
       " ('signific', 1),\n",
       " ('sector', 1),\n",
       " ('difficulti', 1),\n",
       " ('achiev', 1),\n",
       " ('harmoni', 1),\n",
       " ('econom', 1),\n",
       " ('launch', 1),\n",
       " ('appeal', 1),\n",
       " ('valuabl', 1),\n",
       " ('lose', 1),\n",
       " ('sight', 1),\n",
       " ('administ', 1),\n",
       " ('franc', 1),\n",
       " ('complet', 1),\n",
       " ('cut', 1),\n",
       " ('rest', 1),\n",
       " ('archipelago', 1),\n",
       " ('limit', 1),\n",
       " ('free', 1),\n",
       " ('movement', 1),\n",
       " ('good', 1),\n",
       " ('want', 1),\n",
       " ('while', 1),\n",
       " ('willing', 1),\n",
       " ('french', 1),\n",
       " ('cooper', 1),\n",
       " ('counterpart', 1),\n",
       " ('negoti', 1),\n",
       " ('reiter', 1),\n",
       " ('desir', 1),\n",
       " ('22', 1),\n",
       " ('accord', 1),\n",
       " ('relev', 1),\n",
       " ('admit', 1),\n",
       " ('set', 1),\n",
       " ('return', 1),\n",
       " ('fold', 1),\n",
       " ('profound', 1),\n",
       " ('thank', 1),\n",
       " ('fratern', 1),\n",
       " ('friend', 1),\n",
       " ('partner', 1),\n",
       " ('accompani', 1),\n",
       " ('search', 1),\n",
       " ('shaken', 1),\n",
       " ('preserv', 1),\n",
       " ('precious', 1),\n",
       " ('gain', 1),\n",
       " ('30', 1),\n",
       " ('april', 1),\n",
       " ('1999', 1),\n",
       " ('fortun', 1),\n",
       " ('conclus', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix\n",
    "\n",
    "We can use sklearn to construct a bag-of-words representation of text. Create an instance of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Construct the tokenizer.\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the CountVectorizer object, use `fit_transform` it on our list of documents. `fit_transform` takes in the list of documents we want to represent (in this case, the list of tokenized text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<189x7874 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 109126 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = cv.fit_transform(tokens_list)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this? A sparse matrix just means that some cells in the table don't have value. \n",
    "\n",
    "We can get a better look at what's going on by turning the sparse matrix into a data frame. First, get the list of words in our 'bag-of-words' by using `get_feature_names()` on your CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['024',\n",
       " '03',\n",
       " '033',\n",
       " '04',\n",
       " '07',\n",
       " '071',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '105',\n",
       " '106',\n",
       " '10817',\n",
       " '10year',\n",
       " '11',\n",
       " '1135',\n",
       " '117',\n",
       " '1192',\n",
       " '12',\n",
       " '120',\n",
       " '120000',\n",
       " '1244',\n",
       " '125',\n",
       " '127',\n",
       " '1278',\n",
       " '12month',\n",
       " '12step',\n",
       " '13',\n",
       " '1300',\n",
       " '130000',\n",
       " '1306',\n",
       " '1314',\n",
       " '1325',\n",
       " '1333',\n",
       " '134',\n",
       " '1343',\n",
       " '1345',\n",
       " '135',\n",
       " '1359',\n",
       " '1365',\n",
       " '1368',\n",
       " '1371',\n",
       " '1373',\n",
       " '1375',\n",
       " '1376',\n",
       " '1377',\n",
       " '1378',\n",
       " '14',\n",
       " '140',\n",
       " '14000',\n",
       " '1419',\n",
       " '142',\n",
       " '1440s',\n",
       " '145',\n",
       " '147',\n",
       " '1492',\n",
       " '15',\n",
       " '150',\n",
       " '15000',\n",
       " '150000',\n",
       " '1514',\n",
       " '1580',\n",
       " '15th',\n",
       " '15year',\n",
       " '16',\n",
       " '160',\n",
       " '164',\n",
       " '167',\n",
       " '17',\n",
       " '1700',\n",
       " '17000',\n",
       " '18',\n",
       " '1800',\n",
       " '181',\n",
       " '182',\n",
       " '1833',\n",
       " '187',\n",
       " '1884',\n",
       " '189',\n",
       " '1890',\n",
       " '19',\n",
       " '1907',\n",
       " '1917',\n",
       " '1920s',\n",
       " '1930s',\n",
       " '194',\n",
       " '1940s',\n",
       " '1942',\n",
       " '1945',\n",
       " '1946',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '1950s',\n",
       " '1952',\n",
       " '1955',\n",
       " '1958',\n",
       " '19591960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1963',\n",
       " '1964',\n",
       " '1967',\n",
       " '1970',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '19911996',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '19971998',\n",
       " '1998',\n",
       " '1999',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000',\n",
       " '200000',\n",
       " '20002001',\n",
       " '20002009',\n",
       " '2001',\n",
       " '20012005',\n",
       " '20012010',\n",
       " '2002',\n",
       " '20022003',\n",
       " '2003',\n",
       " '2005',\n",
       " '2010',\n",
       " '2014',\n",
       " '2015',\n",
       " '2020',\n",
       " '2024',\n",
       " '2025',\n",
       " '2050',\n",
       " '20year',\n",
       " '21',\n",
       " '212',\n",
       " '2190',\n",
       " '22',\n",
       " '220',\n",
       " '23',\n",
       " '24',\n",
       " '242',\n",
       " '2469',\n",
       " '25',\n",
       " '25000',\n",
       " '253',\n",
       " '26',\n",
       " '260',\n",
       " '26000',\n",
       " '260000',\n",
       " '27',\n",
       " '2758',\n",
       " '28',\n",
       " '29',\n",
       " '29000',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30000',\n",
       " '300000',\n",
       " '31',\n",
       " '32',\n",
       " '320',\n",
       " '33',\n",
       " '338',\n",
       " '34',\n",
       " '340',\n",
       " '34th',\n",
       " '35',\n",
       " '350',\n",
       " '3500',\n",
       " '36',\n",
       " '360',\n",
       " '361',\n",
       " '37',\n",
       " '37000',\n",
       " '37th',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '4000',\n",
       " '40000',\n",
       " '41',\n",
       " '410',\n",
       " '4100',\n",
       " '42',\n",
       " '425',\n",
       " '4261st',\n",
       " '42nd',\n",
       " '43',\n",
       " '44',\n",
       " '44th',\n",
       " '45',\n",
       " '450',\n",
       " '46',\n",
       " '46206',\n",
       " '48',\n",
       " '4826',\n",
       " '49',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50000',\n",
       " '500000',\n",
       " '5080',\n",
       " '51',\n",
       " '53',\n",
       " '53152',\n",
       " '54',\n",
       " '55',\n",
       " '5511',\n",
       " '55146',\n",
       " '55158',\n",
       " '55161',\n",
       " '552',\n",
       " '5522',\n",
       " '55280',\n",
       " '5548',\n",
       " '56',\n",
       " '561',\n",
       " '567',\n",
       " '56year',\n",
       " '58',\n",
       " '587',\n",
       " '60',\n",
       " '600',\n",
       " '6000',\n",
       " '60000',\n",
       " '61',\n",
       " '630',\n",
       " '64',\n",
       " '6484',\n",
       " '65',\n",
       " '66',\n",
       " '666000',\n",
       " '68',\n",
       " '6844',\n",
       " '70',\n",
       " '71',\n",
       " '731',\n",
       " '748',\n",
       " '75',\n",
       " '750',\n",
       " '7500',\n",
       " '75000',\n",
       " '76',\n",
       " '77',\n",
       " '79',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '800000',\n",
       " '81',\n",
       " '810',\n",
       " '822',\n",
       " '83',\n",
       " '830',\n",
       " '84',\n",
       " '85',\n",
       " '853',\n",
       " '857',\n",
       " '86',\n",
       " '874',\n",
       " '88',\n",
       " '883',\n",
       " '884',\n",
       " '90',\n",
       " '90day',\n",
       " '91',\n",
       " '915',\n",
       " '93',\n",
       " '95',\n",
       " '984',\n",
       " 'a541',\n",
       " 'a55305',\n",
       " 'a55pv7',\n",
       " 'a561',\n",
       " 'a56210',\n",
       " 'a56326',\n",
       " 'a56388',\n",
       " 'ababa',\n",
       " 'abandon',\n",
       " 'abdelaziz',\n",
       " 'abdelbaset',\n",
       " 'abdikassim',\n",
       " 'abdoulay',\n",
       " 'abduct',\n",
       " 'abdul',\n",
       " 'abdullah',\n",
       " 'abelaziz',\n",
       " 'aberr',\n",
       " 'abet',\n",
       " 'abhorr',\n",
       " 'abid',\n",
       " 'abidin',\n",
       " 'abil',\n",
       " 'abject',\n",
       " 'abkhaz',\n",
       " 'abkhazia',\n",
       " 'abkhazian',\n",
       " 'abl',\n",
       " 'abli',\n",
       " 'abm',\n",
       " 'abolish',\n",
       " 'abolit',\n",
       " 'abomin',\n",
       " 'abort',\n",
       " 'about',\n",
       " 'abov',\n",
       " 'abovement',\n",
       " 'abraham',\n",
       " 'abreast',\n",
       " 'abroad',\n",
       " 'absenc',\n",
       " 'absolut',\n",
       " 'absolutist',\n",
       " 'abstain',\n",
       " 'abstent',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'abtahi',\n",
       " 'abu',\n",
       " 'abuja',\n",
       " 'abund',\n",
       " 'abus',\n",
       " 'abysm',\n",
       " 'abyss',\n",
       " 'academ',\n",
       " 'academi',\n",
       " 'acced',\n",
       " 'acceler',\n",
       " 'accentu',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessori',\n",
       " 'accid',\n",
       " 'accident',\n",
       " 'acclaim',\n",
       " 'accolad',\n",
       " 'accommod',\n",
       " 'accompani',\n",
       " 'accompli',\n",
       " 'accomplic',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'accra',\n",
       " 'accredit',\n",
       " 'accru',\n",
       " 'accumul',\n",
       " 'accur',\n",
       " 'accuraci',\n",
       " 'accus',\n",
       " 'accustom',\n",
       " 'aceh',\n",
       " 'achiev',\n",
       " 'acknowledg',\n",
       " 'acordo',\n",
       " 'acp',\n",
       " 'acquiesc',\n",
       " 'acquir',\n",
       " 'acquisit',\n",
       " 'acquit',\n",
       " 'acr',\n",
       " 'acronym',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actionori',\n",
       " 'activ',\n",
       " 'activist',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'acuiti',\n",
       " 'acut',\n",
       " 'ad',\n",
       " 'adag',\n",
       " 'adam',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'addi',\n",
       " 'addict',\n",
       " 'addit',\n",
       " 'address',\n",
       " 'adequ',\n",
       " 'adher',\n",
       " 'adhes',\n",
       " 'adjac',\n",
       " 'adject',\n",
       " 'adjud',\n",
       " 'adjust',\n",
       " 'administ',\n",
       " 'administr',\n",
       " 'admir',\n",
       " 'admiss',\n",
       " 'admit',\n",
       " 'adopt',\n",
       " 'adrift',\n",
       " 'adroit',\n",
       " 'adult',\n",
       " 'advanc',\n",
       " 'advantag',\n",
       " 'advent',\n",
       " 'adventur',\n",
       " 'advers',\n",
       " 'adversari',\n",
       " 'advic',\n",
       " 'advis',\n",
       " 'advisori',\n",
       " 'advoc',\n",
       " 'advocaci',\n",
       " 'aegean',\n",
       " 'aegi',\n",
       " 'aeroplan',\n",
       " 'afar',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affili',\n",
       " 'affin',\n",
       " 'affirm',\n",
       " 'afflict',\n",
       " 'affluenc',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'affront',\n",
       " 'afghan',\n",
       " 'afghani',\n",
       " 'afghanistan',\n",
       " 'aforement',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africancaribbeanpacif',\n",
       " 'africanl',\n",
       " 'africanown',\n",
       " 'africawid',\n",
       " 'after',\n",
       " 'aftermath',\n",
       " 'aftershock',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'agadez',\n",
       " 'agadir',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'agenc',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'ageold',\n",
       " 'aggrav',\n",
       " 'aggreg',\n",
       " 'aggress',\n",
       " 'aggressor',\n",
       " 'ago',\n",
       " 'agoa',\n",
       " 'agon',\n",
       " 'agoni',\n",
       " 'agre',\n",
       " 'agreement',\n",
       " 'agricultur',\n",
       " 'ahead',\n",
       " 'ahm',\n",
       " 'ahmad',\n",
       " 'aid',\n",
       " 'aidcredit',\n",
       " 'aim',\n",
       " 'aimak',\n",
       " 'aimless',\n",
       " 'air',\n",
       " 'airbus',\n",
       " 'aircraft',\n",
       " 'airdrop',\n",
       " 'airlift',\n",
       " 'airlin',\n",
       " 'airplan',\n",
       " 'airport',\n",
       " 'airspac',\n",
       " 'airtight',\n",
       " 'ak47',\n",
       " 'akayev',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alaikum',\n",
       " 'alandalus',\n",
       " 'alarm',\n",
       " 'alazhar',\n",
       " 'albania',\n",
       " 'albanian',\n",
       " 'albashir',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'aldurra',\n",
       " 'alert',\n",
       " 'algeria',\n",
       " 'algerian',\n",
       " 'algier',\n",
       " 'alhaji',\n",
       " 'ali',\n",
       " 'alia',\n",
       " 'alibi',\n",
       " 'alid',\n",
       " 'alien',\n",
       " 'align',\n",
       " 'alik',\n",
       " 'aliv',\n",
       " 'alkhalifa',\n",
       " 'all',\n",
       " 'allafrica',\n",
       " 'allah',\n",
       " 'allay',\n",
       " 'alleg',\n",
       " 'allegi',\n",
       " 'allencompass',\n",
       " 'allevi',\n",
       " 'alli',\n",
       " 'allianc',\n",
       " 'allinclus',\n",
       " 'alloc',\n",
       " 'allot',\n",
       " 'allout',\n",
       " 'allow',\n",
       " 'alltoo',\n",
       " 'alltoofamiliar',\n",
       " 'alm',\n",
       " 'almati',\n",
       " 'almighti',\n",
       " 'almost',\n",
       " 'alnahyan',\n",
       " 'alon',\n",
       " 'along',\n",
       " 'alongsid',\n",
       " 'aloof',\n",
       " 'aloud',\n",
       " 'alqud',\n",
       " 'alreadi',\n",
       " 'alsharif',\n",
       " 'alshifa',\n",
       " 'also',\n",
       " 'alter',\n",
       " 'altern',\n",
       " 'although',\n",
       " 'altitud',\n",
       " 'altogeth',\n",
       " 'alway',\n",
       " 'alyaksandr',\n",
       " 'am',\n",
       " 'amara',\n",
       " 'amass',\n",
       " 'amaz',\n",
       " 'ambassador',\n",
       " 'ambigu',\n",
       " 'ambit',\n",
       " 'ambiti',\n",
       " 'ambival',\n",
       " 'ambush',\n",
       " 'amend',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanl',\n",
       " 'amic',\n",
       " 'amid',\n",
       " 'amin',\n",
       " 'amiti',\n",
       " 'ammonialik',\n",
       " 'ammunit',\n",
       " 'amnesti',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'amphetaminetyp',\n",
       " 'ampl',\n",
       " 'ampli',\n",
       " 'amplifi',\n",
       " 'amput',\n",
       " 'ampute',\n",
       " 'amsterdam',\n",
       " 'amu',\n",
       " 'an',\n",
       " 'anachron',\n",
       " 'anachronist',\n",
       " 'analog',\n",
       " 'analys',\n",
       " 'analysi',\n",
       " 'analyst',\n",
       " 'anarchi',\n",
       " 'anarchist',\n",
       " 'anathema',\n",
       " 'ancestor',\n",
       " 'ancestr',\n",
       " 'ancestri',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'ancillari',\n",
       " 'and',\n",
       " 'andalusia',\n",
       " 'andean',\n",
       " 'andes',\n",
       " 'andor',\n",
       " 'andorra',\n",
       " 'andorran',\n",
       " 'andré',\n",
       " 'andth',\n",
       " 'anew',\n",
       " 'angefélix',\n",
       " 'anger',\n",
       " 'angl',\n",
       " 'anglosaxon',\n",
       " 'angola',\n",
       " 'angolan',\n",
       " 'angri',\n",
       " 'anguish',\n",
       " 'ani',\n",
       " 'anim',\n",
       " 'animos',\n",
       " 'anjouan',\n",
       " 'ankara',\n",
       " 'annal',\n",
       " 'annan',\n",
       " 'annex',\n",
       " 'annihil',\n",
       " 'anniversari',\n",
       " 'announc',\n",
       " 'annual',\n",
       " 'annul',\n",
       " 'annum',\n",
       " 'anomali',\n",
       " 'anonym',\n",
       " 'anoth',\n",
       " 'answer',\n",
       " 'antagon',\n",
       " 'antagonist',\n",
       " 'anthrax',\n",
       " 'anthraxrel',\n",
       " 'anti',\n",
       " 'antiballist',\n",
       " 'anticip',\n",
       " 'anticoloni',\n",
       " 'antidiscrimin',\n",
       " 'antidot',\n",
       " 'antidrug',\n",
       " 'antidump',\n",
       " 'antielitist',\n",
       " 'antifascist',\n",
       " 'antiglob',\n",
       " 'antigraft',\n",
       " 'antigua',\n",
       " 'antimeasur',\n",
       " 'antimodern',\n",
       " 'antimoneylaund',\n",
       " 'antinarcot',\n",
       " 'antipathi',\n",
       " 'antipersonnel',\n",
       " 'antipoverti',\n",
       " 'antiqu',\n",
       " 'antiretrovir',\n",
       " 'antiterror',\n",
       " 'antiterrorist',\n",
       " 'antithesi',\n",
       " 'anxieti',\n",
       " 'anxious',\n",
       " 'anybodi',\n",
       " 'anymor',\n",
       " 'anyon',\n",
       " 'anyth',\n",
       " 'anywher',\n",
       " 'apac',\n",
       " 'apach',\n",
       " 'apart',\n",
       " 'apartheid',\n",
       " 'apathi',\n",
       " 'apec',\n",
       " 'aphorist',\n",
       " 'apoge',\n",
       " 'apolog',\n",
       " 'appal',\n",
       " 'appar',\n",
       " 'apparatus',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appeas',\n",
       " 'appel',\n",
       " 'applaud',\n",
       " 'appli',\n",
       " 'applianc',\n",
       " 'applic',\n",
       " 'appoint',\n",
       " 'apport',\n",
       " 'apprais',\n",
       " 'appreci',\n",
       " 'apprehend',\n",
       " 'apprehens',\n",
       " 'appris',\n",
       " 'approach',\n",
       " 'appropri',\n",
       " 'approv',\n",
       " 'approxim',\n",
       " 'april',\n",
       " 'apt',\n",
       " 'apv1208',\n",
       " 'arab',\n",
       " 'arabia',\n",
       " 'arabian',\n",
       " 'arabisra',\n",
       " 'arabl',\n",
       " 'arafat',\n",
       " 'aral',\n",
       " 'arap',\n",
       " 'arbitr',\n",
       " 'arbitrari',\n",
       " 'arbitrarili',\n",
       " 'archipelag',\n",
       " 'archipelago',\n",
       " 'architect',\n",
       " 'architectur',\n",
       " 'archiv',\n",
       " 'ardent',\n",
       " 'arduous',\n",
       " 'are',\n",
       " 'area',\n",
       " 'arena',\n",
       " 'argentin',\n",
       " 'argentina',\n",
       " 'argu',\n",
       " 'argument',\n",
       " 'arid',\n",
       " 'aright',\n",
       " 'aris',\n",
       " 'arisen',\n",
       " 'aristid',\n",
       " 'arm',\n",
       " 'armageddon',\n",
       " 'armament',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenianazerbaijani',\n",
       " 'armi',\n",
       " 'armistic',\n",
       " 'armour',\n",
       " 'armouri',\n",
       " 'armyfirst',\n",
       " 'aros',\n",
       " 'around',\n",
       " 'arous',\n",
       " 'arrang',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arriv',\n",
       " 'arrog',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'arta',\n",
       " 'arteri',\n",
       " 'arthur',\n",
       " 'articl',\n",
       " 'articul',\n",
       " 'artifici',\n",
       " 'artist',\n",
       " 'arusha',\n",
       " 'as',\n",
       " 'ascertain',\n",
       " 'ascrib',\n",
       " 'asean',\n",
       " 'aseanchina',\n",
       " 'asept',\n",
       " 'ash',\n",
       " 'asia',\n",
       " 'asiaeurop',\n",
       " 'asian',\n",
       " 'asiapacif',\n",
       " 'asid',\n",
       " 'ask',\n",
       " 'askar',\n",
       " 'aspect',\n",
       " 'aspir',\n",
       " 'assail',\n",
       " 'assalam',\n",
       " 'assassin',\n",
       " 'assault',\n",
       " 'assembl',\n",
       " 'assert',\n",
       " 'assess',\n",
       " 'asset',\n",
       " 'assidu',\n",
       " 'assign',\n",
       " 'assist',\n",
       " 'associ',\n",
       " 'assort',\n",
       " 'assoumani',\n",
       " 'assum',\n",
       " 'assumpt',\n",
       " 'assur',\n",
       " 'astonish',\n",
       " 'astray',\n",
       " 'astut',\n",
       " 'asylum',\n",
       " 'asymmetr',\n",
       " 'asymmetri',\n",
       " 'at',\n",
       " 'atambua',\n",
       " 'athlet',\n",
       " 'atlant',\n",
       " 'atmospher',\n",
       " 'atol',\n",
       " 'atom',\n",
       " 'atroc',\n",
       " 'atroci',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attain',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attent',\n",
       " 'attest',\n",
       " 'attitud',\n",
       " 'attorney',\n",
       " 'attract',\n",
       " 'attribut',\n",
       " 'au',\n",
       " 'audienc',\n",
       " 'audit',\n",
       " 'augment',\n",
       " 'augur',\n",
       " 'august',\n",
       " 'aura',\n",
       " 'auspic',\n",
       " 'auspici',\n",
       " 'australia',\n",
       " 'austria',\n",
       " 'autarki',\n",
       " 'authent',\n",
       " 'author',\n",
       " 'authorit',\n",
       " 'authoritarian',\n",
       " 'authorship',\n",
       " 'autocraci',\n",
       " 'autocrat',\n",
       " 'automat',\n",
       " 'autonom',\n",
       " 'autonomi',\n",
       " 'autumn',\n",
       " 'avail',\n",
       " 'avalanch',\n",
       " 'aveng',\n",
       " 'avenu',\n",
       " 'averag',\n",
       " 'avert',\n",
       " 'aviat',\n",
       " 'avoid',\n",
       " 'aw',\n",
       " 'await',\n",
       " 'awaken',\n",
       " 'awar',\n",
       " 'award',\n",
       " 'away',\n",
       " 'awesom',\n",
       " 'axiom',\n",
       " 'axiomat',\n",
       " 'azali',\n",
       " 'azerbaijan',\n",
       " 'azerbaijani',\n",
       " 'azizi',\n",
       " 'back',\n",
       " 'backbon',\n",
       " 'backdrop',\n",
       " 'background',\n",
       " 'backtrack',\n",
       " 'backup',\n",
       " 'backward',\n",
       " 'backyard',\n",
       " 'bacteriolog',\n",
       " 'bad',\n",
       " 'bader',\n",
       " 'baggag',\n",
       " 'bahama',\n",
       " 'bahrain',\n",
       " 'bahraini',\n",
       " 'baker',\n",
       " 'baku',\n",
       " 'balanc',\n",
       " 'bald',\n",
       " 'balkan',\n",
       " 'ballist',\n",
       " 'ballot',\n",
       " 'baltic',\n",
       " 'balticnord',\n",
       " 'baluchi',\n",
       " 'bamako',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'bananamarket',\n",
       " 'bananaproduc',\n",
       " 'band',\n",
       " 'banditri',\n",
       " 'bandung',\n",
       " 'bangladesh',\n",
       " 'bangladeshi',\n",
       " 'banish',\n",
       " 'bank',\n",
       " 'banner',\n",
       " 'bar',\n",
       " 'barbado',\n",
       " 'barbar',\n",
       " 'barbuda',\n",
       " 'barcelona',\n",
       " 'bare',\n",
       " 'bargain',\n",
       " 'barren',\n",
       " 'barricad',\n",
       " 'barrier',\n",
       " 'base',\n",
       " 'baseless',\n",
       " 'basi',\n",
       " 'basic',\n",
       " 'basin',\n",
       " 'basket',\n",
       " 'basson',\n",
       " 'bastion',\n",
       " 'baton',\n",
       " 'battalion',\n",
       " 'batter',\n",
       " 'battl',\n",
       " 'battlefield',\n",
       " 'be',\n",
       " 'beacon',\n",
       " 'bear',\n",
       " 'bearer',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beauti',\n",
       " 'becam',\n",
       " 'becaus',\n",
       " 'beckl',\n",
       " 'becom',\n",
       " 'bed',\n",
       " 'bedevil',\n",
       " 'befal',\n",
       " 'befallen',\n",
       " 'befel',\n",
       " 'befit',\n",
       " 'befor',\n",
       " 'beg',\n",
       " 'began',\n",
       " 'beget',\n",
       " 'begin',\n",
       " 'begum',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behav',\n",
       " 'behaviour',\n",
       " 'behind',\n",
       " 'behoov',\n",
       " 'behov',\n",
       " 'beij',\n",
       " 'beijing5',\n",
       " 'beirut',\n",
       " 'beit',\n",
       " 'belarus',\n",
       " 'belarusian',\n",
       " 'belat',\n",
       " 'beleagu',\n",
       " 'belgian',\n",
       " 'belgium',\n",
       " 'belgrad',\n",
       " 'belief',\n",
       " 'believ',\n",
       " 'beliz',\n",
       " 'belizean',\n",
       " 'bell',\n",
       " 'belliger',\n",
       " 'belong',\n",
       " 'belov',\n",
       " 'ben',\n",
       " 'benchmark',\n",
       " 'bend',\n",
       " 'benefici',\n",
       " 'beneficiari',\n",
       " 'benefit',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create labels for columns. look up the documentation on how to create this word_list.\n",
    "word_list = cv.get_feature_names()\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then de-sparsify the sparse matrix by turning it into an array. Try using `toarray()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-sparsify by turning dtm into an array\n",
    "desparse = dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have everything you need to convert your sparse matrix to a DataFrame. Double-check the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) for a reminder of how to construct the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>024</th>\n",
       "      <th>03</th>\n",
       "      <th>033</th>\n",
       "      <th>04</th>\n",
       "      <th>07</th>\n",
       "      <th>071</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>...</th>\n",
       "      <th>zia</th>\n",
       "      <th>ziaur</th>\n",
       "      <th>zimbabw</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zine</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zone</th>\n",
       "      <th>àvis</th>\n",
       "      <th>état</th>\n",
       "      <th>être</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   024  03  033  04  07  071  10  100  1000  10000  ...   zia  ziaur  zimbabw  \\\n",
       "0    0   0    0   0   0    0   0    0     0      0  ...     0      0        0   \n",
       "1    0   0    0   0   0    0   0    0     0      0  ...     0      0        0   \n",
       "2    0   0    0   0   0    0   1    0     0      0  ...     0      0        0   \n",
       "3    0   0    0   0   0    0   1    0     0      0  ...     0      0        0   \n",
       "4    0   0    0   0   0    0   1    0     0      0  ...     0      0        0   \n",
       "\n",
       "   zimbabwean  zine  zionist  zone  àvis  état  être  \n",
       "0           0     0        0     0     0     0     0  \n",
       "1           0     0        0     0     0     0     0  \n",
       "2           0     0        0     0     0     0     0  \n",
       "3           0     0        0     0     0     0     0  \n",
       "4           0     0        0     0     0     0     0  \n",
       "\n",
       "[5 rows x 7874 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new table with pandas data frame\n",
    "dtm_df = pd.DataFrame(columns=word_list, data=desparse)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we call a Document Term Matrix, a core concept in NLP and text analysis.\n",
    "\n",
    "As you can see, there are columns for each word in the entire list. Each row is for each text. The values are the word count for that word in the corresponding text. Note that there are many 0s, hence the matrix is 'sparse'. \n",
    "\n",
    "Why are there so many zeros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the frequency of a word across documents, index that word's column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      2\n",
       "6      0\n",
       "7      0\n",
       "8      1\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15     2\n",
       "16     0\n",
       "17     0\n",
       "18     0\n",
       "19     0\n",
       "20     3\n",
       "21     0\n",
       "22     0\n",
       "23     0\n",
       "24     0\n",
       "25     1\n",
       "26     0\n",
       "27     0\n",
       "28     0\n",
       "29     2\n",
       "      ..\n",
       "159    0\n",
       "160    0\n",
       "161    0\n",
       "162    0\n",
       "163    0\n",
       "164    1\n",
       "165    0\n",
       "166    0\n",
       "167    0\n",
       "168    0\n",
       "169    0\n",
       "170    0\n",
       "171    0\n",
       "172    0\n",
       "173    0\n",
       "174    0\n",
       "175    0\n",
       "176    0\n",
       "177    0\n",
       "178    1\n",
       "179    0\n",
       "180    0\n",
       "181    2\n",
       "182    1\n",
       "183    0\n",
       "184    0\n",
       "185    0\n",
       "186    0\n",
       "187    0\n",
       "188    0\n",
       "Name: zone, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can easily find the frequencies for each of the given words\n",
    "dtm_df['zone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the total number of times the word 'zone' pops up?\n",
    "sum(dtm_df['zone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1553"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words appear in the 100th document?\n",
    "np.sum(dtm_df.loc[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Let's see if we can take another step and try to make equal comparisons across each of the texts. We can normalize the values by dividing each word count by the total number of words in the text. We'll need to sum on axis=1, or summing the row, as each row is a text), as opposed to summing up the column.\n",
    "\n",
    "Once we have the total number of words in the text, we can get a percentage of words that one particular word accounts for, thus applying this method to every other word across the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>024</th>\n",
       "      <th>03</th>\n",
       "      <th>033</th>\n",
       "      <th>04</th>\n",
       "      <th>07</th>\n",
       "      <th>071</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>...</th>\n",
       "      <th>zia</th>\n",
       "      <th>ziaur</th>\n",
       "      <th>zimbabw</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zine</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zone</th>\n",
       "      <th>àvis</th>\n",
       "      <th>état</th>\n",
       "      <th>être</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   024   03  033   04   07  071        10  100  1000  10000  ...   zia  ziaur  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0   0.0    0.0  ...   0.0    0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0   0.0    0.0  ...   0.0    0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.000924  0.0   0.0    0.0  ...   0.0    0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.001126  0.0   0.0    0.0  ...   0.0    0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.001653  0.0   0.0    0.0  ...   0.0    0.0   \n",
       "\n",
       "   zimbabw  zimbabwean  zine  zionist  zone  àvis  état  être  \n",
       "0      0.0         0.0   0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "1      0.0         0.0   0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "2      0.0         0.0   0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "3      0.0         0.0   0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.0         0.0   0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 7874 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if you can fill this out on your own following the steps listed above.\n",
    "row_sums = np.sum(desparse, axis=1)\n",
    "normed = desparse/row_sums[:,None]\n",
    "dtm_df = pd.DataFrame(columns=word_list, data=normed)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When would it be most important to normalize word counts?\n",
    "1. When you have a lot of documents\n",
    "2. When you have very few documents\n",
    "3. When the documents are of many different lengths\n",
    "4. When the documents are all around the same length\n",
    "\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: 3. A document with 10,000 words may use a word the same number of times as a document with 100 words, but the frequencies will be very different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlining\n",
    "\n",
    "Overall, this was a lot of work and if it is as common as we say it is in NLP, shouldn't someone have streamlined it before? In fact, we can simply instruct CountVectorizer not to include stopwords at all and another function, TfidfTransformer, normalizes easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<189x11883 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 113584 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "engl_stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# fill out this beginning part\n",
    "cv = CountVectorizer(stop_words=engl_stop_words)\n",
    "dtm = cv.fit_transform(text_list)\n",
    "\n",
    "# this is what allows us to easily streamline\n",
    "tt = TfidfTransformer(norm='l1',use_idf=False)\n",
    "dtm_tf = tt.fit_transform(dtm)\n",
    "dtm_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! There's no need to directly answer this question, but think about how we could perhaps remove the numbers from the matrix in addition to the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bibliography\n",
    "\n",
    "- Document Term Matrix, normalization markdown and code adapted from materials by Chris Hench: https://github.com/henchc/textxd-2017/blob/master/06-DTM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Gibson Chu\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
