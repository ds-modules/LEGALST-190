{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEGALST-190 Lab 3/6\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, students will learn about dominant language models in natural language processing and the basics of how to implement it in Python. We'll be using the data you extracted from the last lab (un-debates-2001-clean.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Here we will discuss one widely used representation of text:\n",
    "- <b>Bag-of-Words Encoding</b>: encodes text by the frequency of each word\n",
    "\n",
    "This model was very popular in early text analysis, and continues to be used today. In fact, the models that have replaced it are still very difficult to actually interpret, giving the BoW approach a slight advantage if we want to understand why the model makes certain decisions. Once we have our BoW model we can analyze it in a high-dimensional vector space, which gives us more insights into the similarities and clustering of different texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrieve our data\n",
    "data = Table.read_table(..., index_col=0)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's store our text and our tokens into a list\n",
    "text_list = ...\n",
    "tokens_list = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Encoding\n",
    "\n",
    "The bag-of-words encoding is widely used and a standard representation for text in many of the popular text clustering algorithms.\n",
    "\n",
    "__Key Things to Note:__\n",
    "\n",
    "1. __Stop words are removed.__ Stop-words are words like 'is' and 'about' that in isolation contain very little information about the meaning of the sentence.\n",
    "2. __Word order information is lost.__ \n",
    "3. __Capitalization and punctuation__ are typically removed.\n",
    "4. __Sparse Encoding:__ is necessary to represent the bag-of-words efficiently. There are millions of possible words (including terminology, names, and misspellings) and so instantiating a 0 for every word that is not in each record would be incredibly inefficient.\n",
    "\n",
    "Why is it called a __bag-of-words__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Bag-of-words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Tokens\n",
    "\n",
    "If you remember from the last lab, we created tokens and added it to our table. Normally at this point of the stage, you would create tokens for yourself to use, so let's introduce a new term called `Counter`.\n",
    "\n",
    "The easiest way to count tokens is using the `Counter` object from `collections`. This will give you back a dictionary with the token counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# extract the first speech in tokens_list, split it by whitespace, then put it into a Counter\n",
    "first_speech = ...\n",
    "counter = Counter(first_speech) \n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `most_common()` method can be called on a Counter to return the most common tokens and their counts. What are the most common tokens in the first speech? What do these common words tell you about the content or tone of the speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix\n",
    "\n",
    "We can use sklearn to construct a bag-of-words representation of text. Create an instance of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Construct the tokenizer.\n",
    "cv = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the CountVectorizer object, use `fit_transform` on it. `fit_transform` takes in the list of documents we want to represent (in this case, the list of tokenized text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = ...\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this? A sparse matrix just means that many cells in the table don't have value. \n",
    "\n",
    "We can get a better look at what's going on by turning the sparse matrix into a data frame. First, get the list of words in our 'bag-of-words' by using `get_feature_names()` on your CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create labels for columns.\n",
    "word_list = ...\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then de-sparsify the sparse matrix by turning it into an array. Try using `toarray()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-sparsify by turning dtm into an array\n",
    "desparse = ...\n",
    "desparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have everything you need to convert your sparse matrix to a DataFrame. Double-check the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) for a reminder of how to construct the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with words as columns and the sparse matrix data as the data\n",
    "dtm_df = ...\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we call a Document Term Matrix, a core concept in NLP and text analysis.\n",
    "\n",
    "As you can see, there are columns for each word in the entire list. Each row is for each text. The values are the word count for that word in the corresponding text. Note that there are many 0s, hence the matrix is 'sparse'. \n",
    "\n",
    "Why are there so many zeros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the frequency of a word across documents, index that word's column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can easily find the frequencies for each of the given words\n",
    "dtm_df['zone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the total number of times the word 'zone' pops up?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words appear in the 100th document?\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Let's see if we can take another step and try to make equal comparisons across each of the texts. We can normalize the values by dividing each word count by the total number of words in the text. We'll need to sum on axis=1, or summing the row, as each row is a text), as opposed to summing up the column.\n",
    "\n",
    "Once we have the total number of words in the text, we can get a percentage of words that one particular word accounts for, thus applying this method to every other word across the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if you can fill this out on your own following the steps listed above.\n",
    "row_sums = ... # sum up the desparse on axis=1\n",
    "normed = ... # divide this over the total number of row_sums\n",
    "dtm_df = ... # create a data frame using the word_list and the new data\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When would it be most important to normalize word counts?\n",
    "1. When you have a lot of documents\n",
    "2. When you have very few documents\n",
    "3. When the documents are of many different lengths\n",
    "4. When the documents are all around the same length\n",
    "\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlining\n",
    "\n",
    "Overall, this was a lot of work and if it is as common as we say it is in NLP, shouldn't someone have streamlined it before? In fact, we can simply instruct CountVectorizer not to include stopwords at all (so we could use it on our non-tokenized text), and another function, TfidfTransformer, normalizes easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "engl_stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# fill out this beginning part\n",
    "# when you create your CountVectorizer, set the stop_words argument equal to engl_stop_words\n",
    "cv = ...\n",
    "dtm = ...\n",
    "\n",
    "# this is what allows us to easily streamline\n",
    "tt = TfidfTransformer(norm='l1',use_idf=False)\n",
    "dtm_tf = tt.fit_transform(dtm)\n",
    "dtm_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! There's no need to directly answer this question, but think about how we could perhaps remove the numbers from the matrix in addition to the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bibliography\n",
    "\n",
    "- Document Term Matrix, normalization markdown and code adapted from materials by Chris Hench: https://github.com/henchc/textxd-2017/blob/master/06-DTM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Gibson Chu\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
