{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-190] Lab 4/17: Feature Selection\n",
    "\n",
    "This lab will cover feature selection in order to train a machine learning model using `scikit-learn`. With complex datasets, we can run into the problems of overfitting your model and a long run time if all features were to be used. Feature selection is used in machine learning to avoid those type of issues. \n",
    "\n",
    "Estimated time: 35 minutes\n",
    "\n",
    "### Table of Contents\n",
    "[The Data](#section data)<br>\n",
    "1 - [Second Model](# section 1)<br>\n",
    "2 - [Intro to Feature Removal Intuition](#section 2)<br>\n",
    "3 - [Checking Results](#section 3)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all libraries \n",
    "\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#matplotlin\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Bike Sharing<a id='section data'></a>\n",
    "\n",
    "By now, you have been exposed to bike sharing dataset several times. This lab's data describes one such bike sharing system in Washington D.C., from UC Irvine's Machine Learning Repository.\n",
    "\n",
    "Information about the dataset: http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike = pd.read_csv('data/day.csv', index_col=0)\n",
    "# reformat the date column to integers that represent the day of the year, 001-366\n",
    "bike['dteday'] = pd.to_datetime(bike['dteday'].unique()).strftime('%j')\n",
    "\n",
    "# drop casual and registered riders because we want to predict the number of total riders\n",
    "bike = bike.drop(['casual', 'registered'], axis = 1)\n",
    "\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to become familiar with this data set again, feel free to refer back to lab 2-22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how feature selection can change the accuracy for the better or worse, let's start by making a Linear Regression classifer that uses all features. This will act as our baseline.\n",
    "\n",
    "This cell also splits the data into train, validation, and test sets. Just like for model selection, we'll train our data on the training set, use the validation data to compare accuracy for different possible models (in this case, different combinations of features), and only use the test data once after we've finalized our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features used to predict riders\n",
    "X = bike.drop(['cnt'], axis = 1)\n",
    "\n",
    "# the number of riders (target)\n",
    "y = bike['cnt']\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data with 0.20 proportion for test size\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test \n",
    "X, X_test, y, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.80, test_size=0.20)\n",
    "# split the remaining data with 0.75 proportion for train size and 0.25 for validation size\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "# create a linear regression model\n",
    "first_model_reg = ...\n",
    "\n",
    "#fit your model\n",
    "first_model = first_model_reg.fit(..., ...)\n",
    "\n",
    "#predict X_train using your model\n",
    "first_pred = first_model.predict(...)\n",
    "\n",
    "#predict X_valusing your model\n",
    "val_pred = first_model.predict(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check the error between the predicted values and the actual values, we have defined the root mean square error for you. Recall that the equation is the square root of the average differences between the predicted and actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, actual):\n",
    "    return np.sqrt(np.mean((pred - actual) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-d706167cca28>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-d706167cca28>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    first_val_error = rmse(.., ...)\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# check the rmse of your models\n",
    "\n",
    "first_train_error = rmse(..., ...)\n",
    "\n",
    "first_val_error = rmse(.., ...)\n",
    "\n",
    "print(\"Training RMSE:\", first_train_error)\n",
    "print(\"Validation RMSE:\", first_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Second Model\n",
    "\n",
    "Our training and validation errors seem to be pretty high. Let's see how we can improve our model by using feature selection. This process is often accompanied by lots of Exploratory Data Analysis (EDA). First we will look at which features correlate to our target feature (`cnt`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1:** Plot a few EDA yourself to become familar with the correlation values between certain features with the number of riders.\n",
    "\n",
    "**hint:** the `.corr()` method can be called on a dataframe to get the correlation matrix, and heat maps are helpful to visualize correlation (use `sns.heatmap(<data>)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the correlations between different features and the rider count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2:** Looking at your EDA, how will that help you select which features to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3:** List out features that would be important to select for your model. Make sure to not include registered or casual riders in your features list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ...\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4:** Now create a `linear regression` model with the features that you have selected to predict the number of riders(`cnt`). \n",
    "\n",
    "First, create new subsets of the training and validation data that only contain your chosen features. Then, initialize and fit your model to this new training data and use the model to predict the number of riders for the training set.\n",
    "\n",
    "Remember: any transformations you do on the training data also need to be done for the validation data (and the test data, if you end up using this model).\n",
    "\n",
    "*Note that Lasso and Ridge models would use the same steps below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features used to predict riders\n",
    "X_train_my_feats = ...\n",
    "\n",
    "# make sure to select the features for your validation data too!\n",
    "X_val_my_feats = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear regression model\n",
    "lin_reg = ...\n",
    "\n",
    "# fit the model\n",
    "lin_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the number of riders\n",
    "\n",
    "lin_pred = ...\n",
    "\n",
    "# plot the residuals on a scatter plot\n",
    "plt.scatter(y_train, lin_pred)\n",
    "\n",
    "plt.title('Linear Model (OLS)')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5:** What is the rmse for both the prediction of X_train and X_val?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict your X_val here\n",
    "lin_val_pred = ...\n",
    "\n",
    "second_train_error = ...\n",
    "second_val_error = ...\n",
    "\n",
    "print(\"Training RMSE:\", second_train_error)\n",
    "print(\"Validation RMSE:\", second_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm... maybe our selected features did not improve the error as much. Let's see how we can improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Introduction to Feature Removal Intuition<a id = 'section 2'></a>\n",
    "\n",
    "As a good rule of thumb, we typically wish to pick features that are highly correlated with the target column. Also, even though not relevant to the bike sharing dataset, it is often best to remove columns that contain a high ratio of null values. However, sometimes null values represent 0 instead of data actually missing! So always be on the look out when you have to clean data.\n",
    "\n",
    "Of course, with any tedious and error prone process there is always a short cut that reduces time and human error. In part 1, you used your own intuition to pick out features that correlate the highest with the target feature. However, we can use `scikit-learn` to help pick the important features for us. \n",
    "\n",
    "Feature selection methods can give you useful information on the relative importance or relevance of features for a given problem. You can use this information to create filtered versions of your dataset and increase the accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Features with Low Variance\n",
    "\n",
    "As you might remember from Data-8 (or other introductory statistics classes), the **variance** of data can give an idea of how spread out it is. This can be useful for feature selection. In removing features with low variance, all features whose variance does not meet some threshold are removed- the idea being that if a feature has low variability, it will be less helpful in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1:** What is the current shape of X_train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `VarianceThreshold` to filter out features that are below a 0.1 threshold. \n",
    "\n",
    "Then you can use `transform` on the X_train. This will select features that match the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use VarianceThreshold \n",
    "sel = ...\n",
    "sel.fit_transform(...)\n",
    "\n",
    "# Subset features with transform\n",
    "X_new_train = sel.transform(...)\n",
    "\n",
    "#make sure to also transform X_val so it will match dimensions of X_train\n",
    "X_new_val = sel.transform(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, check out the shape of X_new_train to see how many features were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice how many features are then selected compared to X_train's original features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4dae595fd51d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# New Linear Regression model for your X_new. Recall that X_new is the X_train with selected features.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_lin_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnew_lin_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# New Linear Regression model for your X_new. Recall that X_new is the X_train with selected features.\n",
    "new_lin_reg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "new_lin_model = ...\n",
    "\n",
    "#predict X_new_train\n",
    "new_train_pred = ...\n",
    "\n",
    "#predict X_new_validation\n",
    "new_val_pred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**STOP!**\n",
    "There is something very sketchy about using this method for feature removal on this particular data set. Can you say why? Hint: look again at the intuition behind the method, the kinds of features in the data set, and the variance of each feature (easily gotten using `sel.variances_`).\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the variances for each feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Issues\n",
    "1. Our bike sharing data includes variables with many *different units*: Celsius temperature, windspeed, and  humidity. Comparing variances for features with different scales is like comparing apples to oranges- if they have difference variances, that might just be due to the fact that they're measured on different scales.\n",
    "2. Some of our features are *categorical*: things like 'holiday' or 'year' have integer values that represent categories rather than vary on a continuous scale. For example, although we have holiday values of 0 and 1, we can't have a holiday of anything in between (like 0.5) because a day is either a holiday (1) or it isn't (0). This means the variance for these features isn't meaningful in the same way as for the numerical features.\n",
    "\n",
    "So why did we have you do the previous section if it isn't valid? Variance thresholds may be a valuable feature removal tool if you're working with data that has many features with comparable units- for example, data for an athletic competition where event results for many different events are measured in milliseconds. Knowing the process may come in handy for other data sets or projects. Just be sure to take these results with a grain of salt, since we know the features would have been filtered out in a somewhat arbitrary way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  How does your root mean square error change compared to your model in section 1? Try changing the threshold to different values to see how it affects the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_train_error = ...\n",
    "third_val_error = ...\n",
    "\n",
    "print(\"Training RMSE:\", third_train_error)\n",
    "print(\"Validation RMSE:\", third_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimiation with scikit-learn\n",
    "\n",
    "According to [Feature Selection in Python with Scikit-Learn](https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/), recursive feature elimination works by “recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.”\n",
    "\n",
    "So, recursive feature elimination takes care of much of the trial-end-error feature removal process for you. `RFE` takes a model (in this case, a `LinearRegression()` object) and the desired number of features to keep. It is then fit on the training X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a base classifier used to evaluate a subset of attributes\n",
    "model = LinearRegression()\n",
    "\n",
    "# create the RFE model and select 10 attributes\n",
    "rfe = RFE(..., ...)\n",
    "rfe.fit(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check which features have been selected, we can use rfe.support_ to show mask of selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfe.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using rfe, predict your training set\n",
    "new_pred_train = rfe.predict(...)\n",
    "\n",
    "# now predict your validation set\n",
    "new_pred_val = rfe.predict(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time for errors\n",
    "fourth_train_error = ...\n",
    "fourth_val_error = ...\n",
    "\n",
    "print(\"Training RMSE:\", fourth_train_error)\n",
    "print(\"Validation RMSE:\", fourth_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3:**  How does recursive feature elimination change your error? Does the error get better or worse when you tell RFE to use less than 10 or more than 10 attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Feature importance is selecting features that are most important from a previous classifier. For example, selecting the most important features from a number of randomized decision trees. \"A decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions.\" If you would like to read more, feel free to [click here](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052). The main idea behind using the randomized trees is to use many of them to perform prediction. This helps the model to be more robust.\n",
    "\n",
    "Methods that use ensembles of decision trees (like `Random Forest` or `Extra Trees`) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process. In this lab, we will be using `Extra Trees`, Random forest will be introduced in the next lab.\n",
    "\n",
    "Below shows the construction of an Extra Trees ensemble of the bike share dataset and the display of the relative feature importance.\n",
    "\n",
    "Once you use `ExtraTreesClassifer` to create a new model, fit the model. Afterwards, you can use `SelectFromModel` to select features using the classifier. Make sure to `transform` your X_train to obtain the selected important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits a number of randomized decision trees. set n_estimators=15 (this value was arbitrarily chosen)\n",
    "# this allows us to select features\n",
    "model = ExtraTreesClassifier(...)\n",
    "\n",
    "#fit your model to the training x and y\n",
    "model.fit(..., ...)\n",
    "\n",
    "# Select the important features of previous model\n",
    "sel = SelectFromModel(model, prefit=True)\n",
    "\n",
    "# Subset features by calling transform on your training X\n",
    "select_X_train = sel.transform(...)\n",
    "\n",
    "# We want to create a train model *hint this model is exactly the same as model ^^\n",
    "sel_model = ExtraTreesClassifier(...)\n",
    "\n",
    "#fit your sel_model with the new X_train\n",
    "sel_model.fit(..., ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict X_train using the new model\n",
    "y_train_pred = ...\n",
    "\n",
    "# we must also transform X_val to have number of features match up with the model\n",
    "select_X_val = ...\n",
    "\n",
    "#predict y using select_X_val\n",
    "y_pred = ...\n",
    "\n",
    "fifth_train_error = ...\n",
    "fifth_val_error = ...\n",
    "\n",
    "print(\"Training RMSE:\", fifth_train_error)\n",
    "print(\"Validation RMSE:\", fifth_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4:** How does using the Extra Trees change your error? Does the error get better or worse when you change the number of estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Checking Results<a id = 'section 3'></a>\n",
    "\n",
    "Note that since Linear Regression is not the only model option, you can use the above methods to fit a new model using either `Lasso` or `Ridge`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:**  Within the scope of this class, what other methods besides feature selection can be used to improve estimation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** Now that we have gone through different methods of feature selection, let's see how the error changes with each method. We have created the dataframe for you, now graph it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['all_features', 'own_selection', 'variance_theshold', 'rfe', 'important']\n",
    "methods = pd.DataFrame(columns = labels)\n",
    "methods['all_features'] = [first_train_error, first_val_error]\n",
    "methods['own_selection'] = [second_train_error, second_val_error]\n",
    "methods['variance_theshold'] = [third_train_error, third_val_error]\n",
    "methods['rfe'] = [fourth_train_error, fourth_val_error]\n",
    "methods['important'] = [fifth_train_error, fifth_val_error]\n",
    "\n",
    "methods = methods.rename(index={0: 'train'})\n",
    "methods = methods.rename(index={1: 'validation'})\n",
    "methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph the data in 'methods'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  How do the different error rates compare for the different methods of feature selection? Which ones change significantly for training and validation data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    " - Brownlee Jason, An Introduction to Feature Selection. https://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    " - Brownlee Jason, Feature Selection in Python with Scikit-Learn. https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/\n",
    " - Asaithambi Sudharsan, Why, How and When to apply Feature Selection. https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2\n",
    " - Use of `Bike Share` data set adapted from UC Irvine's Machine Learning Repository. http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    " - Some code adapted from Celia Siu: https://csiu.github.io/blog/update/2017/03/06/day10.html\n",
    "\n",
    "----\n",
    "Notebook developed by: Tina Nguyen\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
