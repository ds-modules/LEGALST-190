{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-190] Lab 4/17: Feature Selection\n",
    "\n",
    "This lab will cover feature selection in order to train a machine learning model using `scikit-learn`.\n",
    "\n",
    "Estimated time: 35 minutes\n",
    "\n",
    "### Table of Contents\n",
    "[The Data](#section data)<br>\n",
    "1 - [First Model](# section 1)<br>\n",
    "2 - [Intro to Feature Removal Intuition](#section 2)<br>\n",
    "3 - [Improving Your Model](#section 3)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all libraries \n",
    "\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#matplotlin\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Bike Sharing<a id='section data'></a>\n",
    "\n",
    "By now, I'm sure you have been exposed to bike sharing dataset several times in this lab. This lab's data describes one such bike sharing system in Washington D.C., from UC Irvine's Machine Learning Repository.\n",
    "\n",
    "Information about the dataset: http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the bike data set \n",
    "bike = pd.read_csv(('data/day.csv'))\n",
    "\n",
    "# reformat the date column to integers that represent the day of the year, 001-366\n",
    "bike['dteday'] = pd.to_datetime(bike['dteday'].unique()).strftime('%j')\n",
    "\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to become familiar with this data set again, feel free to refer back to lab 2-22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: First Model\n",
    "\n",
    "Before defining a model, it is good practice to determine which features to select. This process is often accompanied by lots of Exploratory Data Analysis (EDA). First we will look at which features correlate to our target feature (`cnt`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1:** Plot a few EDA yourself to become familar with the correlation values between certain features with the number of riders.\n",
    "\n",
    "**hint:** I recommend looking into heat maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2:** Looking at your EDA, how will that help you select which features to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3:** List out features that would be important to select for your model. Make sure to not include registered or casual riders in your features list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [...]\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4:** Now create a `linear regression` model with the features that you have selected to predict the number of riders(`cnt`). \n",
    "\n",
    "First, separate your data into two parts, a dataframe containing the features used to make our prediction (X) and an array of the true values (y). To start, let's predict the total number of riders (y) using every feature that isn't a rider count (X). Then split the train_size and test_size containing 80% and 20% of the data respectively. Scikit-learn's `test_train_split function` will help here.\n",
    "\n",
    "You can refer back to lab 2-22 if needed.\n",
    "\n",
    "*Note that Lasso and Ridge models would use the same steps below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features used to predict riders\n",
    "X = bike[features]\n",
    "\n",
    "# the number of riders\n",
    "y = bike['cnt']\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(..., ..., ..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a linear regression model\n",
    "lin_reg = ...\n",
    "\n",
    "# fit the model\n",
    "lin_model = lin_reg.fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the number of riders\n",
    "lin_pred = ...\n",
    "\n",
    "# plot the residuals on a scatter plot\n",
    "plt.scatter(y_train, lin_pred)\n",
    "\n",
    "plt.title('Linear Model (OLS)')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check the error between the predicted values and the actual values, I have defined the root mean square error for you. Recall that the equation is the mean squared error of a predicted set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, actual):\n",
    "    error = np.sqrt(np.mean((pred - actual) ** 2))\n",
    "    return print('Root Mean Square Error: ', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5:** What is the rmse for both the prediction of X_train and X_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict your X_test here\n",
    "lin_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Introduction to Feature Removal Intuition<a id = 'section 2'></a>\n",
    "\n",
    "As a good rule of thumb, we typically wish to pick features that have roughly more than a 0.50 correlation with the target column. Also, even though not relevant to the bike sharing dataset, it is often best to remove columns that contain a high ratio of null values. However, sometimes null values represent 0 instead of data actually missing! So always be on the look out when you have to clean data.\n",
    "\n",
    "Of course, with any tedious and error prone process there is always a short cut that reduces time and human error. In part 1, you used your own intuition to pick out features that correlate the highest with the target feature. However, we can use `scikit-learn` to help pick the important features for us. \n",
    "\n",
    "Feature selection methods can give you useful information on the relative importance or relevance of features for a given problem. You can use this information to create filtered versions of your dataset and increase the accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Features with Low Variance\n",
    "\n",
    "In removing features with low variance, all features whose variance does not meet some threshold are removed. In order to remove features that have low variance, you must use `normalize` on the columns before using VarianceThreshold. This is necessary to bring all the features to same scale. Ensuring standardised feature values implicitly weights all features equally. Otherwise, the variance estimates can be misleading between higher value features and lower value features. By default, normalization is not included in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will reload the X and y values for you without the features you selected earlier\n",
    "X = bike\n",
    "\n",
    "# the number of riders\n",
    "y = bike['cnt']\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.80, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1:** What is the current shape of X_train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize your X_train using `preprocessing.normalize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize your X_train\n",
    "X_train = ...\n",
    "\n",
    "#what's the shape?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `VarianceThreshold` to filter out features that match a 0.1 threshold. If your code produces an error ('No feature in X meets the variance threshold 0.10000') and you believe it is correct, try rerunning the `train_test_split` code block above and rerun the code below.\n",
    "\n",
    "Then you can use `transform` on the X_train. This will select features that match the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into the documentation for VarianceThreshold \n",
    "sel = ...\n",
    "\n",
    "#transform your X_train\n",
    "sel.fit_transform(...)\n",
    "\n",
    "# Subset features with transform\n",
    "X_new = sel.transform(...)\n",
    "\n",
    "#notice how many features are then selected compared to X_train's original features\n",
    "X_new.shape\n",
    "\n",
    "#make sure to also transform X_test so it will match dimensions of X_train\n",
    "X_new_test = sel.transform(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new Linear Regression model for your X_new. Recall that X_new is the X_train with selected features.\n",
    "new_lin_reg = ...\n",
    "\n",
    "# fit the model\n",
    "new_lin_model = ...\n",
    "\n",
    "#predict X_new\n",
    "new_lin_pred = ...\n",
    "\n",
    "#predict X_new_test\n",
    "new_test_pred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1:** How does the number of features from X_train compare to X_new?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2:** How does your root mean square error change compared to your model in section 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimiation with scikit-learn\n",
    "\n",
    "According to [Feature Selection in Python with Scikit-Learn](https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/), recursive feature elimination works by “recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.”\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear regression model to evaluate a subset of attributes\n",
    "model = ...\n",
    "\n",
    "# create the RFE model and select 10 attributes\n",
    "rfe = RFE(model, ...)\n",
    "\n",
    "#fit the RFE model\n",
    "...\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using rfe, predict your training set\n",
    "new_pred = ...\n",
    "\n",
    "# what's the error?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now predict your test set\n",
    "new_test_pred = ...\n",
    "\n",
    "# what's the error?\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3:** How does recursive feature elimination improve your error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Feature importance is selecting features that are most important from a previous classifier. For example, selecting the most important features from a number of randomized decision trees. The idea behind random trees is to use many of them to perform prediction. This helps the model to be more robust.\n",
    "\n",
    "Methods that use ensembles of decision trees (like `Random Forest` or `Extra Trees`) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process. In this lab, we will be using `Extra Trees`, Random forest will be introduced in the next lab.\n",
    "\n",
    "Below shows the construction of an Extra Trees ensemble of the bike share dataset and the display of the relative feature importance.\n",
    "\n",
    "Once you use `ExtraTreesClassifer` to create a new model, fit the model. Afterwards, you can use `SelectFromModel` to select features using the classifier. Make sure to `transform` your X_train to obtain the selected important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits a number of randomized decision trees. Use 30 estimators (this value was arbitrarily chosen)\n",
    "model = ExtraTreesClassifier(...)\n",
    "\n",
    "#fit your model\n",
    "...\n",
    "\n",
    "# Select the important features of previous model\n",
    "sel = SelectFromModel(model, prefit=True)\n",
    "\n",
    "# Subset features of X_train\n",
    "X_new = sel.transform(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4:** Which method gives the best results? Check errors rates between all methods mentioned in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Improving your model<a id = 'section 3'></a>\n",
    "\n",
    "Since Linear Regression is not the only model option, use the above methods to fit a new model using either `Lasso` or `Ridge`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** Within the scope of this class, what are other methods that can be used to improve estimation? Then apply the adjustments you came up with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* possible answers\n",
    "- testing multiple models\n",
    "- averging models\n",
    "- stacking models\n",
    "- change number of attributes in Recursive Feature Elimiation method\n",
    "- increase number of estimates in Feature Importance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#space to make your new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#space to plot your values\n",
    "\n",
    "plt.scatter(y_train, ...)\n",
    "plt.title('... Model')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    " - Brownlee Jason, An Introduction to Feature Selection. https://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    " - Brownlee Jason, Feature Selection in Python with Scikit-Learn. https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/\n",
    " - Asaithambi Sudharsan, Why, How and When to apply Feature Selection. https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2\n",
    " - Use of `Bike Share` data set adapted from UC Irvine's Machine Learning Repository. http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    " - Some code adapted from Celia Siu: https://csiu.github.io/blog/update/2017/03/06/day10.html\n",
    "\n",
    "----\n",
    "Notebook developed by: Tina Nguyen\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
