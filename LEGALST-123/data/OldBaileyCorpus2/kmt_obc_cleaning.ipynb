{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, re, sys, json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of xml files\n",
    "xmls = np.array(os.listdir('OBO_XML_7-2/sessionsPapers')[1:])\n",
    "\n",
    "# split files into 25 or 50 year spans\n",
    "span_labels = ['1674-1699', '1700-1749', '1750-1799', '1800-1824', '1825-1849', \n",
    "        '1850-1874', '1875-1899', '1900-1913']\n",
    "spans = [xmls[np.all([xmls > s[:4], xmls < s[5:] + '9999.xml'], axis=0)] for s in span_labels]\n",
    "\n",
    "# check that split files add up to total\n",
    "sum([len(s) for s in spans]) == len(xmls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_to_df(xml, data):\n",
    "    ''' Return a dataframe with one row per trial in the session'''\n",
    "\n",
    "    # read in file\n",
    "    with open(xml) as f:\n",
    "        try:\n",
    "            session = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(xml + \" couldn't be read\")\n",
    "            return data\n",
    "\n",
    "    soup = BeautifulSoup(session, 'xml')\n",
    "\n",
    "    # separate session into trials\n",
    "    trials = soup.find_all('div1', type='trialAccount')\n",
    "    # get the session id\n",
    "    session_id = soup.find('div0').get('id')\n",
    "\n",
    "\n",
    "    # iterate through trials\n",
    "    for trial in trials:\n",
    "\n",
    "        trial_soup = BeautifulSoup(str(trial), 'xml')\n",
    "\n",
    "        # get the text\n",
    "        trial_txt = trial_soup.get_text()\n",
    "        # remove leading/trailing new lines, extra new lines, extra spaces\n",
    "        trial_txt = re.sub(r'^\\n+|\\n+$', '', trial_txt)\n",
    "        trial_txt = re.sub(r'\\n', '', trial_txt)\n",
    "        trial_txt = re.sub(r'\\s\\s+', ' ', trial_txt)\n",
    "        # add to data dictionary\n",
    "        data['transcript'].append(trial_txt)\n",
    "        \n",
    "        # Get offense category and subcategory;\n",
    "        # Note: we simplify here - only a small percentage of trials have\n",
    "        # ... several offense categories, and when they do, it is nearly always\n",
    "        # ... various subcategories of theft.\n",
    "        # So we only save the first offense-suboffense we encounter for each trial.\n",
    "        # We also guard ourselves against somebody having forgotten to mark the category.\n",
    "        try:\n",
    "            mainc = trial_soup.find('interp',type='offenceCategory').get('value')\n",
    "            data['offense'].append(mainc.strip())\n",
    "        except AttributeError:\n",
    "            data['offense'].append('uncategorized')\n",
    "        try:\n",
    "            subc = trial_soup.find('interp',type='offenceSubcategory').get('value')\n",
    "            data['offense_subcategory'].append(subc.strip())\n",
    "        except AttributeError:\n",
    "            data['offense_subcategory'].append('none') \n",
    "            \n",
    "                # Get verdict category/subcategory (first verdict only)\n",
    "        try:\n",
    "            mainc = trial_soup.find('interp',type='verdictCategory').get('value')\n",
    "            data['verdict'].append(mainc.strip())\n",
    "        except AttributeError:\n",
    "            data['verdict'].append('uncategorized')\n",
    "        try:\n",
    "            subc = trial_soup.find('interp',type='verdictSubcategory').get('value')\n",
    "            data['verdict_subcategory'].append(subc.strip())\n",
    "        except AttributeError:\n",
    "            data['verdict_subcategory'].append('none') \n",
    "        \n",
    "        # get the punishment and sub-punishment (first only)\n",
    "        try:\n",
    "            mainc = trial_soup.find('interp',type='punishmentCategory').get('value')\n",
    "            data['punishment'].append(mainc.strip())\n",
    "        except AttributeError:\n",
    "            data['punishment'].append('uncategorized')\n",
    "        try:\n",
    "            subc = trial_soup.find('interp',type='punishmentSubcategory').get('value')\n",
    "            data['punishment_subcategory'].append(subc.strip())\n",
    "        except AttributeError:\n",
    "            data['punishment_subcategory'].append('none') \n",
    "                 \n",
    "        # get the trial id\n",
    "        data['trial_id'].append(trial_soup.find('div1').get('id'))\n",
    "        data['session'].append(session_id)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% processed\n",
      "5.399568034557236% processed\n",
      "10.799136069114471% processed\n",
      "16.198704103671705% processed\n",
      "21.598272138228943% processed\n",
      "26.997840172786177% processed\n",
      "32.39740820734341% processed\n",
      "37.79697624190065% processed\n",
      "43.196544276457885% processed\n",
      "48.59611231101512% processed\n",
      "53.99568034557235% processed\n",
      "59.39524838012959% processed\n",
      "64.79481641468682% processed\n",
      "OBO_XML_7-2/sessionsPapers/18370130.xml couldn't be read\n",
      "OBO_XML_7-2/sessionsPapers/18371211.xml couldn't be read\n",
      "70.19438444924405% processed\n",
      "75.5939524838013% processed\n",
      "80.99352051835854% processed\n",
      "86.39308855291577% processed\n",
      "91.792656587473% processed\n",
      "OBO_XML_7-2/sessionsPapers/18461123.xml couldn't be read\n",
      "97.19222462203024% processed\n",
      "OBO_XML_7-2/sessionsPapers/18490820.xml couldn't be read\n"
     ]
    }
   ],
   "source": [
    "# range of sessions from about 25 yrs before and after the Bloody Code repeal\n",
    "bloody_span = xmls[np.all([xmls > '1800', xmls < '1850'], axis=0)]\n",
    "data = defaultdict(list)\n",
    "count = 0\n",
    "\n",
    "for session in bloody_span:\n",
    "    if count % 25 == 0:\n",
    "        print('{}% processed'.format(count * 100 / len(bloody_span)))\n",
    "    data = session_to_df('OBO_XML_7-2/sessionsPapers/' + session, data)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85756, 8)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloody_data = pd.DataFrame(data)\n",
    "bloody_data.set_index('trial_id', inplace=True)\n",
    "bloody_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloody_data.to_csv('obc_1800_1850.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
